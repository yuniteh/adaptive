{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpu import set_gpu\n",
    "import numpy as np\n",
    "import os\n",
    "import adapt.utils.data_utils as prd\n",
    "import adapt.loop as lp\n",
    "import adapt.ml.lda as dlda\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import adapt.ml.dl_subclass as dl\n",
    "import copy as cp\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TR37' 'TR56' 'TR58' 'TR59' 'TR62' 'TR64' 'WRTR53']\n"
     ]
    }
   ],
   "source": [
    "sub = 3\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "path += subs[sub] + '/DATA/MAT/'\n",
    "all_files = os.listdir(path)\n",
    "if 'skip' in all_files:\n",
    "    all_files = np.delete(all_files,all_files.index('skip'))\n",
    "print(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = np.delete(all_files,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR59\n",
      "training nn\n",
      "Set: 20170424_081251, Test: 20170424_082640, Accuracy: 46.36 , Val: 86.96 , Prev: 0.00 , Train: 86.96\n",
      "skip bad set: 20170424_082640, accuracy: 56.46\n",
      "Set: 20170424_082640, Test: 20170424_085656, Accuracy: 78.97 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170424_082640, Test: 20170424_091501, Accuracy: 71.34 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170424_082640, Test: 20170424_094328, Accuracy: 72.70 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170424_082640, Test: 20170424_104554, Accuracy: 52.14 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170424_104554, accuracy: 62.28\n",
      "Set: 20170424_104554, Test: 20170424_114643, Accuracy: 48.40 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170424_114643, accuracy: 49.66\n",
      "Set: 20170424_114643, Test: 20170424_142122, Accuracy: 69.50 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20170424_142122\n",
      "training nn\n",
      "Set: 20170424_142122, Test: 20170427_161648, Accuracy: 41.25 , Val: 77.34 , Prev: 85.60 , Train: 86.89\n",
      "recal: 2 20170427_161648\n",
      "training nn\n",
      "Set: 20170427_161648, Test: 20170427_191711, Accuracy: 74.00 , Val: 81.09 , Prev: 64.59 , Train: 89.33\n",
      "Set: 20170427_161648, Test: 20170428_151357, Accuracy: 45.75 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170428_151357, accuracy: 39.46\n",
      "Set: 20170428_151357, Test: 20170428_151659, Accuracy: 61.06 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 3 20170428_151659\n",
      "training nn\n",
      "Set: 20170428_151659, Test: 20170430_144524, Accuracy: 60.11 , Val: 73.20 , Prev: 76.33 , Train: 81.84\n",
      "recal: 4 20170430_144524\n",
      "training nn\n",
      "Set: 20170430_144524, Test: 20170430_144857, Accuracy: 60.99 , Val: 81.90 , Prev: 78.91 , Train: 90.84\n",
      "skip bad set: 20170430_144857, accuracy: 54.83\n",
      "Set: 20170430_144857, Test: 20170430_151517, Accuracy: 42.61 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 5 20170430_151517\n",
      "training nn\n",
      "Set: 20170430_151517, Test: 20170501_140917, Accuracy: 72.63 , Val: 77.04 , Prev: 78.50 , Train: 92.76\n",
      "Set: 20170430_151517, Test: 20170501_141731, Accuracy: 76.86 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170430_151517, Test: 20170501_142024, Accuracy: 53.03 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170501_142024, accuracy: 57.34\n",
      "Set: 20170501_142024, Test: 20170502_071722, Accuracy: 62.83 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170502_071722, accuracy: 51.84\n",
      "Set: 20170502_071722, Test: 20170502_072300, Accuracy: 54.05 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170502_072300, accuracy: 33.47\n",
      "Set: 20170502_072300, Test: 20170502_073440, Accuracy: 67.32 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170502_073440, accuracy: 57.82\n",
      "Set: 20170502_073440, Test: 20170502_075048, Accuracy: 77.40 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170502_073440, Test: 20170502_092225, Accuracy: 75.36 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170502_073440, Test: 20170502_092652, Accuracy: 57.32 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170502_092652, accuracy: 66.53\n",
      "Set: 20170502_092652, Test: 20170502_170413, Accuracy: 27.16 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 6 20170502_170413\n",
      "training nn\n",
      "Set: 20170502_170413, Test: 20170503_160818, Accuracy: 56.16 , Val: 32.65 , Prev: 76.49 , Train: 92.48\n",
      "recal: 7 20170503_160818\n",
      "training nn\n",
      "Set: 20170503_160818, Test: 20170503_161240, Accuracy: 84.96 , Val: 89.52 , Prev: 35.51 , Train: 91.01\n",
      "Set: 20170503_160818, Test: 20170503_165907, Accuracy: 62.56 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170503_165907, accuracy: 63.95\n",
      "Set: 20170503_165907, Test: 20170504_105348, Accuracy: 47.45 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 8 20170504_105348\n",
      "training nn\n",
      "Set: 20170504_105348, Test: 20170504_105753, Accuracy: 65.90 , Val: 82.59 , Prev: 87.89 , Train: 90.66\n",
      "recal: 9 20170504_105753\n",
      "training nn\n",
      "Set: 20170504_105753, Test: 20170504_111151, Accuracy: 77.54 , Val: 80.30 , Prev: 78.10 , Train: 92.42\n",
      "Set: 20170504_105753, Test: 20170506_131247, Accuracy: 82.37 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170504_105753, Test: 20170531_164633, Accuracy: 81.35 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170504_105753, Test: 20170531_170707, Accuracy: 55.41 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170531_170707, accuracy: 57.41\n",
      "Set: 20170531_170707, Test: 20170531_174743, Accuracy: 80.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170531_170707, Test: 20170602_095356, Accuracy: 59.36 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170602_095356, accuracy: 52.24\n",
      "Set: 20170602_095356, Test: 20170602_095749, Accuracy: 86.11 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Missing classes\n",
      "Set: 20170602_095356, Test: 20170602_141330, Accuracy: 63.85 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170602_141330, accuracy: 60.33\n",
      "Set: 20170602_141330, Test: 20170602_141548, Accuracy: 76.79 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Missing classes\n",
      "Set: 20170602_141330, Test: 20170604_133359, Accuracy: 90.54 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170602_141330, Test: 20170604_133553, Accuracy: 69.98 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170604_133553, accuracy: 60.05\n",
      "Set: 20170604_133553, Test: 20170604_134046, Accuracy: 85.36 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170604_133553, Test: 20170604_134326, Accuracy: 51.19 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 10 20170604_134326\n",
      "training nn\n",
      "Set: 20170604_134326, Test: 20170605_110918, Accuracy: 76.31 , Val: 73.91 , Prev: 83.29 , Train: 88.59\n",
      "Set: 20170604_134326, Test: 20170605_111438, Accuracy: 80.94 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170604_134326, Test: 20170605_112816, Accuracy: 64.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170605_112816, accuracy: 60.54\n",
      "Set: 20170605_112816, Test: 20170605_171921, Accuracy: 77.47 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170605_112816, Test: 20170605_172143, Accuracy: 58.34 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170605_172143, accuracy: 53.20\n",
      "Set: 20170605_172143, Test: 20170605_172358, Accuracy: 51.12 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170605_172358, accuracy: 66.80\n",
      "Set: 20170605_172358, Test: 20170605_172605, Accuracy: 59.50 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170605_172605, accuracy: 69.52\n",
      "Set: 20170605_172605, Test: 20170605_172838, Accuracy: 78.15 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170605_172605, Test: 20170606_172325, Accuracy: 53.98 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170606_172325, accuracy: 26.26\n",
      "Set: 20170606_172325, Test: 20170606_172621, Accuracy: 34.92 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170606_172621, accuracy: 44.63\n",
      "Set: 20170606_172621, Test: 20170606_172943, Accuracy: 32.54 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170606_172943, accuracy: 50.34\n",
      "Set: 20170606_172943, Test: 20170606_173231, Accuracy: 28.59 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170606_173231, accuracy: 53.74\n",
      "Set: 20170606_173231, Test: 20170606_173444, Accuracy: 40.23 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170606_173444, accuracy: 56.87\n",
      "Set: 20170606_173444, Test: 20170607_111519, Accuracy: 49.15 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 11 20170607_111519\n",
      "training nn\n",
      "Set: 20170607_111519, Test: 20170607_111831, Accuracy: 66.03 , Val: 97.01 , Prev: 72.55 , Train: 91.16\n",
      "recal: 12 20170607_111831\n",
      "training nn\n",
      "Set: 20170607_111831, Test: 20170607_132942, Accuracy: 84.00 , Val: 82.72 , Prev: 94.01 , Train: 90.93\n",
      "Set: 20170607_111831, Test: 20170607_134433, Accuracy: 67.32 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170607_134433, accuracy: 63.59\n",
      "Set: 20170607_134433, Test: 20170608_105908, Accuracy: 37.51 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 13 20170608_105908\n",
      "training nn\n",
      "Set: 20170608_105908, Test: 20170608_110127, Accuracy: 68.55 , Val: 61.50 , Prev: 81.36 , Train: 90.61\n",
      "skip bad set: 20170608_110127, accuracy: 62.36\n",
      "Set: 20170608_110127, Test: 20170610_091855, Accuracy: 60.72 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170610_091855, accuracy: 68.98\n",
      "Set: 20170610_091855, Test: 20170610_142504, Accuracy: 68.35 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 14 20170610_142504\n",
      "training nn\n",
      "Set: 20170610_142504, Test: 20170612_115630, Accuracy: 90.33 , Val: 90.62 , Prev: 66.26 , Train: 90.74\n",
      "Set: 20170610_142504, Test: 20170612_193216, Accuracy: 21.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 15 20170612_193216\n",
      "training nn\n",
      "Set: 20170612_193216, Test: 20170613_171441, Accuracy: 57.25 , Val: 51.43 , Prev: 89.54 , Train: 88.08\n",
      "recal: 16 20170613_171441\n",
      "training nn\n",
      "Set: 20170613_171441, Test: 20170613_171735, Accuracy: 63.24 , Val: 80.27 , Prev: 9.25 , Train: 87.39\n",
      "recal: 17 20170613_171735\n",
      "training nn\n",
      "Set: 20170613_171735, Test: 20170613_174657, Accuracy: 61.95 , Val: 79.05 , Prev: 84.63 , Train: 88.15\n",
      "skip bad set: 20170613_174657, accuracy: 66.26\n",
      "Set: 20170613_174657, Test: 20170613_174902, Accuracy: 61.20 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170613_174902, accuracy: 60.00\n",
      "Set: 20170613_174902, Test: 20170613_175924, Accuracy: 78.62 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170613_174902, Test: 20170616_155518, Accuracy: 55.14 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170616_155518, accuracy: 67.21\n",
      "Set: 20170616_155518, Test: 20170616_155727, Accuracy: 77.94 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170616_155518, Test: 20170618_125006, Accuracy: 57.39 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170618_125006, accuracy: 69.97\n",
      "Set: 20170618_125006, Test: 20170619_181445, Accuracy: 67.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 18 20170619_181445\n",
      "training nn\n",
      "Set: 20170619_181445, Test: 20170619_183646, Accuracy: 77.40 , Val: 84.78 , Prev: 77.41 , Train: 88.81\n",
      "Set: 20170619_181445, Test: 20170620_164658, Accuracy: 75.09 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170619_181445, Test: 20170620_165204, Accuracy: 76.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20170619_181445, Test: 20170622_142516, Accuracy: 51.40 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170622_142516, accuracy: 60.14\n",
      "Set: 20170622_142516, Test: 20170623_130233, Accuracy: 10.01 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170623_130233, accuracy: 26.26\n",
      "Set: 20170623_130233, Test: 20170623_131239, Accuracy: 21.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170623_131239, accuracy: 46.80\n",
      "Set: 20170623_131239, Test: 20170623_180256, Accuracy: 34.58 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170623_180256, accuracy: 64.35\n",
      "Set: 20170623_180256, Test: 20170623_180953, Accuracy: 36.01 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170623_180953, accuracy: 68.57\n",
      "Set: 20170623_180953, Test: 20170623_185445, Accuracy: 37.58 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170623_185445, accuracy: 59.46\n",
      "Set: 20170623_185445, Test: 20170623_185857, Accuracy: 39.01 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 19 20170623_185857\n",
      "training nn\n",
      "Set: 20170623_185857, Test: 20170625_113904, Accuracy: 65.42 , Val: 65.71 , Prev: 77.58 , Train: 83.40\n",
      "recal: 20 20170625_113904\n",
      "training nn\n",
      "Set: 20170625_113904, Test: 20170625_114119, Accuracy: 66.85 , Val: 77.82 , Prev: 79.18 , Train: 85.98\n",
      "recal: 21 20170625_114119\n",
      "training nn\n",
      "Set: 20170625_114119, Test: 20170708_130614, Accuracy: 74.00 , Val: 84.22 , Prev: 74.83 , Train: 89.39\n",
      "Set: 20170625_114119, Test: 20170710_093530, Accuracy: 67.46 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170710_093530, accuracy: 50.07\n",
      "Missing classes\n",
      "Set: 20170710_093530, Test: 20170710_093802, Accuracy: 44.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20170710_093802, accuracy: 44.49\n",
      "Set: 20170710_093802, Test: 20170710_093945, Accuracy: 60.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 22 20170710_093945\n",
      "training nn\n",
      "Set: 20170710_093945, Test: 20170710_094211, Accuracy: 87.75 , Val: 73.61 , Prev: 82.99 , Train: 87.20\n",
      "Set: 20170710_093945, Test: 20170727_160330, Accuracy: 80.05 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "crcnn 22\n"
     ]
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = False\n",
    "mod_tot = ['ld','bld','bcnn','bcnnl','cnn','acnn','acewc', 'crcnnl', 'crld','cnnl','acnnl3','acnnl5','acnnl30','crcnn','acnnlm','acewcl','acewclm']\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn','acnn03','acnn30','acewc00','acewc30','bcnnl', 'cnnl', 'crcnnl', 'acnnl03','acnnl30','acewcl','acnnlm','acewclm','xtra','xtra1','xtra2']\n",
    "ft = 'feat'\n",
    "iter = 1\n",
    "\n",
    "for sub in range(3,4):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    for it in range(0,iter):\n",
    "        # first iteration, includes LDA; others exclude LDA\n",
    "        if it == 0:\n",
    "            # mod_all = ['bld','bcnnl','ld','cnnl','acnnl3','acnnl5','acnnl30','acewcl']\n",
    "            mod_all = ['crcnn']\n",
    "        else:\n",
    "            mod_all = ['cnnl','acnnl3','acnnl5','acnnl30','acewcl']\n",
    "\n",
    "        # load or initialize cnn weights\n",
    "        if load_mod:\n",
    "            with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                all_acc, all_recal, all_val, all_prev, all_train, _, _, c_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        else:\n",
    "            c_weights = None\n",
    "            cl_wc = None\n",
    "            all_recal = np.empty((len(mod_tot),1))\n",
    "            all_recal[:] = np.nan\n",
    "            all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "        mod_i = 0\n",
    "        for mod in mod_all:\n",
    "            acc = np.zeros((len(all_files),5))\n",
    "            acc_val = np.zeros((len(all_files),5))\n",
    "            acc_prev = np.zeros((len(all_files),5))\n",
    "            acc_train = np.zeros((len(all_files),5))\n",
    "\n",
    "            if 'cnn' in mod:\n",
    "                acc_i = 2\n",
    "            elif 'cewc' in mod:\n",
    "                acc_i = 4\n",
    "            elif 'lda' in mod:\n",
    "                acc_i = 0\n",
    "\n",
    "            cnn = None\n",
    "            ewc = None\n",
    "\n",
    "            ep = 30\n",
    "            recal = 0\n",
    "            skip = False\n",
    "\n",
    "            # Loop through files\n",
    "            for i in range(1,len(all_files)-1):\n",
    "                # Check if need to recalibrate\n",
    "                if i > 1:\n",
    "                    if 'b' in mod:\n",
    "                        skip = True\n",
    "                    elif acc[i,acc_i] < 70:\n",
    "                        skip = False\n",
    "                    else:\n",
    "                        skip = True\n",
    "                    \n",
    "                    if not skip:\n",
    "                        train_file = all_files[i]\n",
    "                        train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "                        train_data = train_data[:,:8,:]\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                            \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "\n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "\n",
    "                        _, _, _, _, _, x_tr, y_tr, emg_scale_tr, _, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, ft=ft, noise=False, split=False)\n",
    "\n",
    "                        _, _, _, _, _, x_val, y_val, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale_tr, prop_b=False, ft=ft, noise=False, split=False)\n",
    "\n",
    "                        _, _, w, c, _, _ = lp.train_models(x_train_lda=x_tr, y_train_lda=y_tr)\n",
    "                        skip_test = lp.test_models(None, None, x_val, None, y_val, lda=[w,c])[0]\n",
    "\n",
    "                        if skip_test > 70:\n",
    "                            skip = False\n",
    "                            recal += 1\n",
    "                            print('recal: ' + str(recal) + ' ' + all_files[i])\n",
    "                            acc[i,acc_i] *= -1\n",
    "                        else:\n",
    "                            skip = True\n",
    "                            print('skip bad set: ' + all_files[i] + ', ' + f'accuracy: {skip_test:.2f}')\n",
    "                        \n",
    "                        del skip_test, train_data, train_params, val_data, val_params, x_tr, y_tr, x_val, y_val, emg_scale_tr, train_temp, params_temp, tr_i, te_i\n",
    "\n",
    "                if not skip:\n",
    "                    # load training file\n",
    "                    train_file = all_files[i]\n",
    "                    train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "                    train_data = train_data[:,:8,:]\n",
    "\n",
    "                    # if first train, use two train files\n",
    "                    if i == 1:\n",
    "                        train_data2, train_params2 = prd.load_caps_train(sub_path + all_files[i-1] + '/traindata.mat')\n",
    "                        train_data2 = train_data2[:,:8,:]\n",
    "                        train_data = np.vstack((train_data,train_data2))\n",
    "                        train_params = np.vstack((train_params,train_params2))\n",
    "                        del train_data2, train_params2\n",
    "\n",
    "                        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "                        val_data = train_data\n",
    "                        val_params = train_params\n",
    "                    else:\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                    \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "\n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "\n",
    "                        del train_temp, params_temp, tr_i, te_i\n",
    "\n",
    "                    if (i == 1 and mod[0] == 'a') or (mod[0] != 'a'):\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        key = np.empty(train_dof.shape)\n",
    "                        for key_i in range(len(train_dof)):\n",
    "                            key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "                    \n",
    "                    train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "                    val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key)\n",
    "\n",
    "                    # if combining, save current training data\n",
    "                    if 'cr' in mod:\n",
    "                        # combine old and new training data\n",
    "                        if i > 1:\n",
    "                            train_data = np.vstack((train_data_0,train_data))\n",
    "                            train_params = np.vstack((train_params_0,train_params))\n",
    "\n",
    "                        train_data_0 = cp.deepcopy(train_data)\n",
    "                        train_params_0 = cp.deepcopy(train_params)\n",
    "                    \n",
    "                    if i == 1:\n",
    "                        n_dof = int(np.max(key))\n",
    "\n",
    "                    if (mod[0] == 'a' and i > 1) or ('cr' in mod and i > 1):\n",
    "                        _, traincnn, y_train, _, x_train_cnn, x_train_lda, y_train_lda, _, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale, scaler=scaler, num_classes=n_dof, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False)\n",
    "                    else:\n",
    "                        _, traincnn, y_train, _, x_train_cnn, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False)\n",
    "                        if (i == 1) and (c_weights is not None):\n",
    "                            scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                    # if 'ewc' in mod:\n",
    "                    _, _, y_clean, _, x_clean_cnn, x_clean_lda, y_clean_lda, _, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "                    _, _, y_val, _, x_val_cnn, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "                    del train_data, train_params, val_data, val_params\n",
    "\n",
    "                    if 'l' in mod and 'lda' not in mod:\n",
    "                        cnnlda = True\n",
    "                    else:\n",
    "                        cnnlda = False\n",
    "                    \n",
    "                    if 'lda' not in mod:\n",
    "                        n_dof = int(np.max(key))\n",
    "                        if i == 1:\n",
    "                            if c_weights is None:\n",
    "                                _, cnn, _, _, w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                                c_weights = cp.deepcopy(cnn.get_weights())\n",
    "                                scaler_0 = cp.deepcopy(scaler)    \n",
    "                                cl_wc = cp.deepcopy([w_c,c_c])\n",
    "                            else:\n",
    "                                print('setting CNN weights')\n",
    "                                cnn = dl.CNN(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.set_weights(c_weights)\n",
    "                                if cnnlda:\n",
    "                                    w_c = cp.deepcopy(cl_wc[0].astype('float32'))\n",
    "                                    c_c = cp.deepcopy(cl_wc[1].astype('float32'))\n",
    "                                    print('setting lda weights')\n",
    "                            \n",
    "                            if 'ewc' in mod:\n",
    "                                ewc = dl.EWC(mod='CNN', n_class=n_dof)\n",
    "                                ewc(x_train_cnn[:1,...])\n",
    "                                ewc.set_weights(c_weights)\n",
    "\n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                            else:\n",
    "                                clda = None\n",
    "                        else:\n",
    "                            if mod =='acnnlm': # update CNN encoder using lda for loss\n",
    "                                ep = 5\n",
    "                                _, cnn, _, _, _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, cnn=[cnn,w_c,c_c],cnnlda=cnnlda)\n",
    "                            elif 'acnn' in mod: # update whole CNN and lda weights\n",
    "                                ep = int(mod[-2:])\n",
    "                                _, cnn, _, _, w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, cnn=cnn, cnnlda=cnnlda, lr=0.00001)\n",
    "                            elif mod == 'afcnnl': # update lda only \n",
    "                                _, _, w_c, c_c, _, _ = lp.train_models(x_train_lda=cnn.enc(x_train_cnn).numpy(), y_train_lda=np.argmax(y_train,axis=1)[...,np.newaxis],cnnlda=cnnlda)\n",
    "                            elif 'cnn' in mod: # recalibrate cnnlda\n",
    "                                _, cnn, _, _, w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                            elif mod == 'acewclm':\n",
    "                                _, _ = lp.train_task(ewc, ep, 1, x_train_cnn, y_train, [x_val_cnn], [y_val], lams=[int(mod[-2:])], bat=bat, clda=[w_c,c_c], cnnlda=cnnlda)\n",
    "                            elif 'acewc' in mod:\n",
    "                                w_c, c_c = lp.train_task(ewc, ep, 1, x_train_cnn, y_train, [prev_x, x_val_cnn],[prev_y, y_val], lams=[0,int(mod[-2:])], bat=bat, cnnlda=cnnlda)\n",
    "                            \n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                            else:\n",
    "                                clda = None\n",
    "\n",
    "                            if 'cnn' in mod:\n",
    "                                acc_prev[i,:] = lp.test_models(prev_x, None, None, prev_y, None, cnn=cnn, clda=clda)\n",
    "                            elif 'ewc' in mod: \n",
    "                                acc_prev[i,:] = lp.test_models(prev_x, None, None, prev_y, None, ewc_cnn=ewc, clda=clda)\n",
    "\n",
    "                        if 'cnn' in mod:\n",
    "                            acc_val[i,:] = lp.test_models(x_val_cnn, None, None, y_val, None, cnn=cnn, clda=clda)\n",
    "                            acc_train[i,:] = lp.test_models(x_clean_cnn, None, None, y_clean, None, cnn=cnn, clda=clda)\n",
    "                        elif 'ewc' in mod:\n",
    "                            acc_val[i,:] = lp.test_models(x_val_cnn, None, None, y_val, None, ewc_cnn=ewc, clda=clda)\n",
    "                            acc_train[i,:] = lp.test_models(x_clean_cnn, None, None, y_clean, None, ewc_cnn=ewc, clda=clda)\n",
    "                            \n",
    "                            ewc.compute_fisher(x_clean_cnn, y_clean, num_samples=200, plot_diffs=False) \n",
    "                            ewc.star()\n",
    "                    else:\n",
    "                        if mod[0] != 'a' or (i == 1 and mod[0] == 'a'):\n",
    "                            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda)\n",
    "                        else:\n",
    "                            w, c, mu_class, cov_class, N = dlda.update_lda(x_train_lda, y_train_lda, N, mu_class, cov_class)\n",
    "\n",
    "                        acc_val[i,:] = lp.test_models(None, None, x_val_lda, None, y_val_lda, lda=[w,c])\n",
    "                        acc_train[i,:] = lp.test_models(None, None, x_clean_lda, None, y_clean_lda, lda=[w,c])\n",
    "                        if i > 1:\n",
    "                            acc_prev[i,:] = lp.test_models(None, None, prev_x_lda, None, prev_y_lda, lda=[w,c])\n",
    "                        del x_train_lda, y_train_lda\n",
    "                    \n",
    "                    prev_x = cp.deepcopy(x_val_cnn)\n",
    "                    prev_y = cp.deepcopy(y_val)\n",
    "                    prev_x_lda = cp.deepcopy(x_val_lda)\n",
    "                    prev_y_lda = cp.deepcopy(y_val_lda)\n",
    "                    \n",
    "                    del x_train_cnn, y_train, traincnn, x_val_cnn, y_val, x_val_lda, y_val_lda, x_clean_cnn, y_clean, x_clean_lda, y_clean_lda\n",
    "                \n",
    "                # load data\n",
    "                test_file = all_files[i+1]\n",
    "                test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "                test_data = test_data[:,:8,:]\n",
    "                \n",
    "                # check class labels\n",
    "                test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "                test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key)\n",
    "\n",
    "                # test \n",
    "                y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "\n",
    "                # test \n",
    "                if 'cnn' in mod:\n",
    "                    if 'cnnl' in mod:\n",
    "                        acc[i+1,:] = lp.test_models(x_test_cnn, None, x_test_lda, y_test, y_test_lda, cnn=cnn, clda=[w_c,c_c])\n",
    "                    else:\n",
    "                        acc[i+1,:] = lp.test_models(x_test_cnn, None, x_test_lda, y_test, y_test_lda, cnn)\n",
    "                elif 'cewc' in mod:\n",
    "                    if 'cewcl' in mod:\n",
    "                        acc[i+1,:] = lp.test_models(x_test_cnn, None, x_test_lda, y_test, y_test_lda, ewc_cnn=ewc, clda=[w_c,c_c])\n",
    "                    else:\n",
    "                        acc[i+1,:] = lp.test_models(x_test_cnn, None, x_test_lda, y_test, y_test_lda, ewc_cnn=ewc)\n",
    "                elif 'lda' in mod:\n",
    "                    acc[i+1,:] = lp.test_models(x_test_cnn, None, x_test_lda, y_test, y_test_lda, lda=[w,c])\n",
    "\n",
    "                print ('Set: ' + train_file + ', Test: ' + test_file + ',', f'Accuracy: {acc[i+1,acc_i]:.2f}', f', Val: {acc_val[i,acc_i]:.2f}', f', Prev: {acc_prev[i,acc_i]:.2f}', f', Train: {acc_train[i,acc_i]:.2f}')\n",
    "                del y_test, x_test_cnn, x_test_lda, y_test_lda, test_data, test_params\n",
    "\n",
    "            all_acc[:,mod_tot.index(mod)] = acc[:,acc_i]\n",
    "            all_recal[mod_tot.index(mod)] = recal\n",
    "            all_val[:,mod_tot.index(mod)] = acc_val[:,acc_i]\n",
    "            all_prev[:,mod_tot.index(mod)] = acc_prev[:,acc_i]\n",
    "            all_train[:,mod_tot.index(mod)] = acc_train[:,acc_i]\n",
    "            print(mod + ' ' + str(recal))\n",
    "            mod_i += 1\n",
    "\n",
    "            if 'cr' in mod:\n",
    "                del train_data_0, train_params_0\n",
    "\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','wb') as f:\n",
    "            pickle.dump([all_acc, all_recal, all_val, all_prev, all_train, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "fig,ax = plt.subplots(1,5,figsize=(30,4))\n",
    "for sub in range(2,3):#,5):\n",
    "    with open(subs[sub] + '_0_r_accs.p','rb') as f:\n",
    "        # acc_all, recal_all = pickle.load(f)\n",
    "        acc_all, recal_all, cur_all, prev_all, val_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "    # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "    colors =  cm.get_cmap('tab20c')\n",
    "    c = np.empty((20,4))\n",
    "    for i in range(20):\n",
    "        c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "    nn_c[0,-1] = 1\n",
    "    all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "    pt_m = ['ko-','o-','o-','o-','s','s','s','s','D']\n",
    "    nn_c = np.vstack((np.array([0,0,0,1]), c[0,:],c[1,:],c[2,:],c[3,:],c[4,:],c[5,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "    # nn_c[0,-1] = 1\n",
    "\n",
    "    labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "    labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "    # labels = mod_tot\n",
    "\n",
    "    ax_ind = sub\n",
    "    it = 0\n",
    "    for v in [1,2]: \n",
    "        i = mod_tot.index(mod_all[v])\n",
    "        acc_temp = acc_all[1:-1,i]\n",
    "        if not np.isnan(acc_temp).all():\n",
    "            x = np.arange(len(acc_temp))\n",
    "            recal_i = (acc_temp < 0)\n",
    "            ax[ax_ind].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v],color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "            it+=1\n",
    "\n",
    "    for i in range(5):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        \n",
    "        ax[i].set_ylim([0,100])\n",
    "        ax[i].set_title('TR' + str(i+1))\n",
    "    ax[0].legend()\n",
    "    ax[2].set_xlabel('Calibration Set')\n",
    "    ax[0].set_ylabel('Accuracy (%)')\n",
    "    plt.rc('font', size=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "for sub in range(2,3):#,5):\n",
    "    fig,ax = plt.subplots(1,4,figsize=(20,4))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, val_all,mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [1,0,0,1,2,2,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[it]+': ' + str(int(recal_all[i,0])),color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "                it+=1\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "cv_iter = 1\n",
    "for sub in range(0,5):\n",
    "    fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','o','*','o','s','D','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','f-cnn-5','f-cnn-3','f-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in [0, 3, 5, 4, 6, 7]: #range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v]+': ' + str(int(recal_all[i,0])),color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                it+=1\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "        \n",
    "\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 5\n",
    "for sub in [0,2]:#range(1):#5):\n",
    "    fig,ax = plt.subplots(1,4,figsize=(20,4))\n",
    "    for it in range(1,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','o','o','s','s','v','v','D','D','D']\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "\n",
    "        # d0 = datetime(int(all_files[0][:4]),int(all_files[0][4:6]),int(all_files[0][6:8]),int(all_files[0][9:11]),int(all_files[0][11:13]),int(all_files[0][13:]))\n",
    "        # delta = np.empty((len(acc_all)-1,1))\n",
    "        # for i in range(1,len(acc_all)-1):\n",
    "        #     d1 = datetime(int(all_files[i][:4]),int(all_files[i][4:6]),int(all_files[i][6:8]),int(all_files[i][9:11]),int(all_files[i][11:13]),int(all_files[i][13:]))\n",
    "        #     delta[i] = (d1 - d0).total_seconds()\n",
    "            \n",
    "        ax_ind = [0,0,0,1,1,2,2,3,3,3]\n",
    "        for i in [0,2,4,6,8,9]:#range(3):#acc_all.shape[-1]):\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[i]].plot(np.abs(acc_temp),'-',color=nn_c[i,:])\n",
    "                ax[ax_ind[i]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[i],label=labels[i]+': ' + str(int(recal_all[i,0])),color=nn_c[i,:])\n",
    "                ax[ax_ind[i]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[i,:])\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].axhline(65, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = all_files[0]\n",
    "train_file2 = all_files[1]\n",
    "ft= 'feat'\n",
    "\n",
    "train_data, train_params = prd.load_caps_train(path + train_file + '/traindata.mat')\n",
    "train_data = train_data[:,:8,:]\n",
    "\n",
    "train_dof = np.unique(train_params[:,2])\n",
    "key = np.empty(train_dof.shape)\n",
    "for i in range(len(train_dof)):\n",
    "    key[i] = train_params[np.argmax(train_params[:,2] == train_dof[i]),0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2, train_params2 = prd.load_caps_train(path + train_file2 + '/traindata.mat')\n",
    "train_data2 = train_data2[:,:8,:]\n",
    "train_data = np.vstack((train_data,train_data2))\n",
    "train_params = np.vstack((train_params,train_params2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep initial training data\n",
    "ep = 30\n",
    "n_dof = len(train_dof)\n",
    "\n",
    "# train_data, train_params = prd.threshold(train_data, train_params)\n",
    "trainmlp_0, traincnn_0, y_train_0, x_train_mlp_0, x_train_cnn_0, x_train_lda_0, y_train_lda_0, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b = False, batch_size=32,ft=ft, noise=True, split=False)\n",
    "y_test_0, x_test_mlp_0, x_test_cnn_0, x_lda_0, y_lda_0 = prd.prep_test_caps(train_data, train_params, scaler, emg_scale, num_classes=len(train_dof),ft=ft, split=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial training\n",
    "mlp, cnn, w, c = lp.train_models(traincnn_0, trainmlp_0, x_train_lda_0, y_train_lda_0, n_dof, ep=3)\n",
    "mlp_0 = mlp.get_weights()\n",
    "cnn_0 = cnn.get_weights()\n",
    "w_0 = cp.deepcopy(w)\n",
    "c_0 = cp.deepcopy(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA recalibration threshold\n",
    "acc = np.empty((len(all_files),5))\n",
    "\n",
    "for i in range(0,len(all_files)-1):\n",
    "    # load data\n",
    "    train_file = all_files[i]\n",
    "    train_data, train_params = prd.load_caps_train(path + train_file + '/traindata.mat')\n",
    "    train_data = train_data[:,:8,:]\n",
    "    # train_data, train_params = prd.threshold(train_data, train_params)\n",
    "\n",
    "    train_dof = np.unique(train_params[:,2])\n",
    "    key = np.empty(train_dof.shape)\n",
    "    for ki in range(len(train_dof)):\n",
    "        key[ki] = train_params[np.argmax(train_params[:,2] == train_dof[ki]),0]\n",
    "\n",
    "    _, _, _, _, _, x_train_lda_0, y_train_lda_0, _, _, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=32, ft=ft, noise=False, split=False, emg_scale=np.ones((train_data.shape[1],1)))\n",
    "\n",
    "    _, _, w, c = lp.train_models(x_train_lda=x_train_lda_0, y_train_lda=y_train_lda_0)\n",
    "\n",
    "    # load data\n",
    "    test_file = all_files[i+1]\n",
    "    test_data, test_params = prd.load_caps_train(path + test_file + '/traindata.mat')\n",
    "    test_data = test_data[:,:8,:]\n",
    "\n",
    "    # test_data, test_params = prd.threshold(test_data, test_params)\n",
    "    \n",
    "    # check class labels\n",
    "    test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key)\n",
    "\n",
    "    # test \n",
    "    y_test, x_test_mlp, x_test_cnn, x_lda, y_lda = prd.prep_test_caps(test_data, test_params, ft=ft, split=False)\n",
    "    acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, lda = [w, c])\n",
    "    print ('Set: ' + test_file, f'LDA Accuracy: {acc[i,0]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test on current dataset\n",
    "acc = np.empty((len(all_files),5))\n",
    "# old_train_mlp, old_train_cnn, old_x_train_lda, old_y_train_lda, old_y_train = [], [], [], [], []\n",
    "\n",
    "for i in range(0,len(all_files)):\n",
    "    train_file = all_files[i]\n",
    "    train_data, train_params = prd.load_caps_train(path + train_file + '/traindata.mat')\n",
    "    train_data = train_data[:,:8,:]\n",
    "\n",
    "    orig_train_dof = np.unique(train_params[:,2])\n",
    "    train_data, train_params = prd.threshold(train_data, train_params)\n",
    "    train_dof = np.unique(train_params[:,2])\n",
    "\n",
    "    if len(orig_train_dof) == len(train_dof):\n",
    "        n_dof = len(train_dof)\n",
    "        key = np.empty(train_dof.shape)\n",
    "        for key_i in range(len(train_dof)):\n",
    "            key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "\n",
    "        trainmlp_0, traincnn_0, y_train_0, x_train_mlp_0, x_train_cnn_0, x_train_lda_0, y_train_lda_0, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b = False, batch_size=32, ft=ft, noise=False, split=True)\n",
    "\n",
    "        if i > 0:\n",
    "            x_train_mlp_0, x_train_cnn_0, x_train_lda_0, y_train_lda_0, y_train_0 = np.vstack((old_train_mlp, x_train_mlp_0)), np.vstack((old_train_cnn, x_train_cnn_0)), np.vstack((old_x_train_lda, x_train_lda_0)), np.vstack((old_y_train_lda, y_train_lda_0)), np.vstack((old_y_train, y_train_0))\n",
    "\n",
    "            trainmlp_0 = tf.data.Dataset.from_tensor_slices((x_train_mlp_0, y_train_0, y_train_0)).shuffle(x_train_mlp_0.shape[0],reshuffle_each_iteration=True).batch(32)\n",
    "            traincnn_0 = tf.data.Dataset.from_tensor_slices((x_train_cnn_0, y_train_0, y_train_0)).shuffle(x_train_cnn_0.shape[0],reshuffle_each_iteration=True).batch(32)\n",
    "\n",
    "        mlp, cnn, w, c = lp.train_models(traincnn_0, trainmlp_0, x_train_lda_0, y_train_lda_0, n_dof, ep=30)\n",
    "\n",
    "        # test \n",
    "        y_test, x_test_mlp, x_test_cnn, x_lda, y_lda = prd.prep_test_caps(train_data, train_params, scaler, emg_scale, num_classes=len(train_dof),ft=ft, split=True)\n",
    "        acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, cnn, mlp, [w, c])\n",
    "        print ('Set: ' + train_file, f'CNN Accuracy: {acc[i,2]:.2f},', f'MLP Accuracy: {acc[i,1]:.2f},', f'LDA Accuracy: {acc[i,0]:.2f}')\n",
    "        old_train_mlp, old_train_cnn, old_x_train_lda, old_y_train_lda, old_y_train = x_train_mlp_0, x_train_cnn_0, x_train_lda_0, y_train_lda_0, y_train_0\n",
    "    else:\n",
    "        print('Skipping ' + train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on initial, test without recalibration or adaptation\n",
    "acc = np.empty((len(all_files),5))\n",
    "\n",
    "for i in range(0,len(all_files)):\n",
    "    # load data\n",
    "    test_file = all_files[i]\n",
    "    test_data, test_params = prd.load_caps_train(path + test_file + '/traindata.mat')\n",
    "    test_data = test_data[:,:8,:]\n",
    "\n",
    "    # test_data, test_params = prd.threshold(test_data, test_params)\n",
    "    \n",
    "    # check class labels\n",
    "    test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key)\n",
    "\n",
    "    # test \n",
    "    y_test, x_test_mlp, x_test_cnn, x_lda, y_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=len(train_dof),ft=ft, split=False)\n",
    "    acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, cnn, mlp, [w, c])\n",
    "    print ('Set: ' + test_file, f'CNN Accuracy: {acc[i,2]:.2f},', f'MLP Accuracy: {acc[i,1]:.2f},', f'LDA Accuracy: {acc[i,0]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewc_acc = np.empty((len(all_files),5))\n",
    "ewc_acc[:] = np.nan\n",
    "rec_acc = np.empty((len(all_files),5))\n",
    "rec_acc[:] = np.nan\n",
    "\n",
    "n_dof = len(train_dof)\n",
    "\n",
    "cnn.set_weights(cnn_0)\n",
    "mlp.set_weights(mlp_0)\n",
    "\n",
    "ewc_cnn = dl.EWC(mod='CNN')\n",
    "ewc_cnn(x_train_cnn_0[:1,...])\n",
    "loss, fish_loss = lp.train_task(ewc_cnn, ep, 1, x_train_cnn_0,y_train_0, [x_test_cnn_0],[y_test_0], lams=[0])\n",
    "x_test_ewc_cnn_0 = cp.deepcopy(x_test_cnn_0)\n",
    "y_test_ewc_0 = cp.deepcopy(y_test_0)\n",
    "x_val_ewc_cnn = cp.deepcopy(x_test_cnn_0)\n",
    "y_val_ewc = cp.deepcopy(y_test_0)\n",
    "\n",
    "ewc = dl.EWC()\n",
    "ewc(x_train_mlp_0[:1,...])\n",
    "loss, fish_loss = lp.train_task(ewc, ep, 1, x_train_mlp_0,y_train_0, [x_test_mlp_0],[y_test_0], lams=[0])\n",
    "x_test_ewc_0 = cp.deepcopy(x_test_mlp_0)\n",
    "x_val_ewc = cp.deepcopy(x_test_mlp_0)\n",
    "\n",
    "ewc_acc[0,:] = lp.test_models(x_test_cnn_0, x_test_mlp_0, x_lda_0, y_test_0, y_lda_0, cnn, mlp, [w, c], ewc, ewc_cnn)\n",
    "print(ewc_acc[0,:])\n",
    "\n",
    "for i in range(1,len(all_files)-1,1):\n",
    "    # load recalibration data\n",
    "    ewc_file = all_files[i]\n",
    "    ewc_data, ewc_params = prd.load_caps_train(path + ewc_file + '/traindata.mat')\n",
    "    ewc_data = ewc_data[:,:8,:]\n",
    "\n",
    "    ewc_cnn.compute_fisher(x_val_ewc_cnn, y_val_ewc, num_samples=200, plot_diffs=False) # use validation set for Fisher computation\n",
    "    ewc_cnn.star()\n",
    "\n",
    "    ewc.compute_fisher(x_val_ewc, y_val_ewc, num_samples=200, plot_diffs=False) # use validation set for Fisher computation\n",
    "    ewc.star()\n",
    "\n",
    "    # check class labels\n",
    "    orig_train_dof = np.unique(ewc_params[:,2])\n",
    "    ewc_data, ewc_params = prd.threshold(ewc_data, ewc_params)\n",
    "    train_dof = np.unique(ewc_params[:,2])\n",
    "\n",
    "    if len(orig_train_dof) == len(train_dof):\n",
    "        # n_dof = len(train_dof)\n",
    "        \n",
    "        # training data\n",
    "        ewcmlp, ewccnn, y_train_ewc, x_train_ewc, x_train_ewc_cnn, x_train_lda, y_train_lda, _, _, _, _, _ = prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, num_classes=n_dof, batch_size=32, noise=True, scaler=scaler, emg_scale=emg_scale,ft=ft,split=True)\n",
    "        # training data for recalibration\n",
    "        rmlp, rcnn, y_train_r, x_train_r, x_train_r_cnn, x_train_r_lda, y_train_r_lda, r_emg_scale, r_scaler, _, _, _ = prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, batch_size=32, noise=True, ft=ft, split=True)\n",
    "        # validation data for fisher\n",
    "        _, _, y_val_ewc, x_val_ewc, x_val_ewc_cnn, _, _, _, _, _, _, _ = prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, num_classes=n_dof, batch_size=32, noise=False, scaler=scaler, emg_scale=emg_scale,ft=ft,split=True)\n",
    "        # test data\n",
    "        y_test_ewc, x_test_ewc, x_test_ewc_cnn, x_lda, y_lda = prd.prep_test_caps(ewc_data, ewc_params, scaler, emg_scale, num_classes=n_dof,ft=ft,split=True)\n",
    "        # test data for recalibration\n",
    "        y_test_r, x_test_r, x_test_r_cnn, x_r_lda, y_r_lda = prd.prep_test_caps(ewc_data, ewc_params, r_scaler, r_emg_scale, ft=ft,split=True)\n",
    "\n",
    "        loss, fish_loss = lp.train_task(ewc_cnn, 30, 1, x_train_ewc_cnn, y_train_ewc, [x_test_ewc_cnn_0, x_test_ewc_cnn], [y_test_ewc_0, y_test_ewc], lams=[15])#,15,50])\n",
    "        loss, fish_loss = lp.train_task(ewc, 30, 1, x_train_ewc, y_train_ewc, [x_test_ewc_0, x_test_ewc], [y_test_ewc_0, y_test_ewc], lams=[15])#,15,50])\n",
    "        mlp, cnn, _, _ = lp.train_models(ewccnn, ewcmlp, n_dof=n_dof, ep=5, mlp=mlp, cnn=cnn)\n",
    "\n",
    "        mlp_r, cnn_r, w, c = lp.train_models(rcnn, rmlp, x_train_r_lda, y_train_r_lda, n_dof=n_dof, ep=30)\n",
    "\n",
    "        # test\n",
    "        ewc_acc[i,:] = lp.test_models(x_test_ewc_cnn, x_test_ewc, x_lda, y_test_ewc, y_lda, cnn, mlp, ewc=ewc, ewc_cnn=ewc_cnn)\n",
    "        rec_acc[i,:] = lp.test_models(x_test_r_cnn, x_test_r, x_r_lda, y_test_r, y_r_lda, cnn_r, mlp_r, lda=[w,c])\n",
    "\n",
    "        x_test_ewc_cnn_0 = cp.deepcopy(x_test_ewc_cnn)\n",
    "        x_test_ewc_0 = cp.deepcopy(x_test_ewc)\n",
    "        y_test_ewc_0 = cp.deepcopy(y_test_ewc)\n",
    "\n",
    "        print ('EWC: ' + ewc_file + ',', f'EWC CNN Accuracy: {ewc_acc[i,4]:.2f},', f'EWC Accuracy: {ewc_acc[i,3]:.2f},', f'a-CNN Accuracy: {ewc_acc[i,2]:.2f},', f'a-MLP Accuracy: {ewc_acc[i,1]:.2f},',  f'r-CNN Accuracy: {rec_acc[i,2]:.2f},', f'r-MLP Accuracy: {rec_acc[i,1]:.2f},', f'r-LDA Accuracy: {rec_acc[i,0]:.2f}')\n",
    "    else:\n",
    "        print('Skipping ' + ewc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all controllers with ewc\n",
    "ep = 30\n",
    "# Remove extra file if odd number of files\n",
    "if len(all_files)%2 == 0:\n",
    "    all_files = np.delete(all_files,-1)\n",
    "ewc_acc = np.empty((len(all_files),5))\n",
    "ewc_acc[:] = np.nan\n",
    "rec_acc = np.empty((len(all_files),5))\n",
    "rec_acc[:] = np.nan\n",
    "\n",
    "n_dof = len(train_dof)\n",
    "\n",
    "cnn.set_weights(cnn_0)\n",
    "mlp.set_weights(mlp_0)\n",
    "\n",
    "ewc_cnn = dl.EWC(mod='CNN')\n",
    "ewc_cnn(x_train_cnn_0[:1,...])\n",
    "loss, fish_loss = lp.train_task(ewc_cnn, ep, 1, x_train_cnn_0, y_train_0, [x_test_cnn_0],[y_test_0], lams=[0])\n",
    "x_test_ewc_cnn_0 = cp.deepcopy(x_test_cnn_0)\n",
    "y_test_ewc_0 = cp.deepcopy(y_test_0)\n",
    "x_val_ewc_cnn = cp.deepcopy(x_test_cnn_0)\n",
    "y_val_ewc = cp.deepcopy(y_test_0)\n",
    "\n",
    "ewc = dl.EWC()\n",
    "ewc(x_train_mlp_0[:1,...])\n",
    "loss, fish_loss = lp.train_task(ewc, ep, 1, x_train_mlp_0, y_train_0, [x_test_mlp_0],[y_test_0], lams=[0])\n",
    "x_test_ewc_0 = cp.deepcopy(x_test_mlp_0)\n",
    "x_val_ewc = cp.deepcopy(x_test_mlp_0)\n",
    "\n",
    "ewc_acc[0,:] = lp.test_models(x_test_cnn_0, x_test_mlp_0, x_lda_0, y_test_0, y_lda_0, cnn, mlp, [w_0, c_0], ewc, ewc_cnn)\n",
    "\n",
    "for i in range(7,len(all_files)-1,2):\n",
    "    # load recalibration data\n",
    "    ewc_file = all_files[i]\n",
    "    ewc_data, ewc_params = prd.load_caps_train(path + ewc_file + '/traindata.mat')\n",
    "    ewc_data = ewc_data[:,:8,:]\n",
    "\n",
    "    ewc_cnn.compute_fisher(x_val_ewc_cnn, y_val_ewc, num_samples=200, plot_diffs=False) # use validation set for Fisher computation\n",
    "    ewc_cnn.star()\n",
    "\n",
    "    ewc.compute_fisher(x_val_ewc, y_val_ewc, num_samples=200, plot_diffs=False) # use validation set for Fisher computation\n",
    "    ewc.star()\n",
    "\n",
    "    orig_train_dof = np.unique(ewc_params[:,2])\n",
    "    ewc_data, ewc_params = prd.threshold(ewc_data, ewc_params)\n",
    "    train_dof = np.unique(ewc_params[:,2])\n",
    "\n",
    "    if len(orig_train_dof) == len(train_dof):\n",
    "\n",
    "        # check class labels\n",
    "        ewc_data, ewc_params = lp.check_labels(ewc_data, ewc_params, train_dof, key)\n",
    "        \n",
    "        ewcmlp, ewccnn, y_train_ewc, x_train_ewc, x_train_ewc_cnn, x_train_lda, y_train_lda, _, _, _, _, _ = prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, num_classes=n_dof, batch_size=64, noise=True, scaler=scaler, emg_scale=emg_scale,ft=ft, split=True)\n",
    "        _, _, y_val_ewc, x_val_ewc, x_val_ewc_cnn, _, _, _, _, _, _, _= prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, num_classes=n_dof, batch_size=64, noise=False, scaler=scaler, emg_scale=emg_scale,ft=ft, split=True)\n",
    "\n",
    "        y_test_ewc, x_test_ewc, x_test_ewc_cnn, x_lda, y_lda = prd.prep_test_caps(ewc_data, ewc_params, scaler, emg_scale, num_classes=n_dof,ft=ft, split=True)\n",
    "\n",
    "        loss, fish_loss = lp.train_task(ewc_cnn, 30, 1, x_train_ewc_cnn, y_train_ewc, [x_test_ewc_cnn_0, x_test_ewc_cnn], [y_test_ewc_0, y_test_ewc], lams=[15])#,15,50])\n",
    "        loss, fish_loss = lp.train_task(ewc, 30, 1, x_train_ewc, y_train_ewc, [x_test_ewc_0, x_test_ewc], [y_test_ewc_0, y_test_ewc], lams=[15])#,15,50])\n",
    "        mlp, cnn, _, _ = lp.train_models(ewccnn, ewcmlp, x_train_lda, y_train_lda, n_dof, 30, mlp, cnn)\n",
    "        \n",
    "        # training data for recalibration\n",
    "        rmlp, rcnn, y_train_r, x_train_r, x_train_r_cnn, x_train_r_lda, y_train_r_lda, r_emg_scale, r_scaler, _, _, _ = prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, batch_size=32, noise=True, ft=ft, split=True)\n",
    "        y_test_r, x_test_r, x_test_r_cnn, x_r_lda, y_r_lda = prd.prep_test_caps(ewc_data, ewc_params, r_scaler, r_emg_scale, ft=ft, split=True)\n",
    "\n",
    "        mlp_r, cnn_r, w, c = lp.train_models(rcnn, rmlp, x_train_r_lda, y_train_r_lda, ep=30)\n",
    "\n",
    "        # load test data\n",
    "        test_file = all_files[i+1]\n",
    "        test_data, test_params = prd.load_caps_train(path + test_file + '/traindata.mat')\n",
    "        test_data = test_data[:,:8,:]\n",
    "        \n",
    "        # check class labels\n",
    "        test_data, test_params = prd.threshold(test_data, test_params)\n",
    "        test_data, test_params = lp.check_labels(test_data, test_params, train_dof, key)\n",
    "\n",
    "        ewc_acc[i,:] = lp.test_models(x_test_ewc_cnn, x_test_ewc, x_lda, y_test_ewc, y_lda, cnn, mlp, ewc=ewc, ewc_cnn=ewc_cnn)\n",
    "        rec_acc[i,:] = lp.test_models(x_test_r_cnn, x_test_r, x_r_lda, y_test_r, y_r_lda, cnn_r, mlp_r, lda=[w,c])\n",
    "\n",
    "        # test \n",
    "        y_test, x_test_mlp, x_test_cnn, x_lda, y_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft)\n",
    "        y_test_r, x_test_r, x_test_r_cnn, x_r_lda, y_r_lda = prd.prep_test_caps(test_data, test_params, r_scaler, r_emg_scale, num_classes=n_dof,ft=ft)\n",
    "        ewc_acc[i+1,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, cnn, mlp, ewc=ewc, ewc_cnn=ewc_cnn)\n",
    "        rec_acc[i+1,:] = lp.test_models(x_test_r_cnn, x_test_r, x_r_lda, y_test_r, y_r_lda, cnn_r, mlp_r, lda=[w,c])\n",
    "\n",
    "        x_test_ewc_cnn_0 = cp.deepcopy(x_test_ewc_cnn)\n",
    "        x_test_ewc_0 = cp.deepcopy(x_test_ewc)\n",
    "        y_test_ewc_0 = cp.deepcopy(y_test_ewc)\n",
    "\n",
    "        print ('EWC: ' + ewc_file + ', Test: ' + test_file + ',', f'EWC CNN Accuracy: {ewc_acc[i+1,4]:.2f},', f'EWC Accuracy: {ewc_acc[i+1,3]:.2f},', f'a-CNN Accuracy: {ewc_acc[i+1,2]:.2f},', f'a-MLP Accuracy: {ewc_acc[i+1,1]:.2f},',  f'r-CNN Accuracy: {rec_acc[i+1,2]:.2f},', f'r-MLP Accuracy: {rec_acc[i+1,1]:.2f},', f'r-LDA Accuracy: {rec_acc[i+1,0]:.2f}')\n",
    "    else:\n",
    "        print('Skipping: ' + ewc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewc_acc[0,...] = np.nan\n",
    "rec_acc[0,...] = np.nan\n",
    "rlda_acc = np.empty((len(all_files),15))\n",
    "rlda_acc[:] = np.nan\n",
    "lda_acc = np.empty((len(all_files),15))\n",
    "lda_acc[:] = np.nan\n",
    "acc_all = np.hstack((acc[:,...],ewc_acc,rec_acc))\n",
    "for i in range(acc.shape[1]):\n",
    "    rlda_acc[:,i] = acc[:,i] - ewc_acc[:,0]\n",
    "    rlda_acc[:,i+5] = ewc_acc[:,i] - ewc_acc[:,0]\n",
    "    rlda_acc[:,i+10] = rec_acc[:,i] - ewc_acc[:,0]\n",
    "\n",
    "    lda_acc[:,i] = acc[:,i] - acc[:,0]\n",
    "    lda_acc[:,i+5] = ewc_acc[:,i] - acc[:,0]\n",
    "    lda_acc[:,i+10] = rec_acc[:,i] - acc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_i = [0, 5]\n",
    "cnn_i = [2, 12, 7, 9]\n",
    "mlp_i = [1, 11, 6, 8]\n",
    "nn_i = np.vstack((mlp_i,cnn_i))\n",
    "\n",
    "colors =  cm.get_cmap('tab20c')\n",
    "c = np.empty((20,4))\n",
    "for i in range(20):\n",
    "    c[i,:] = colors(i*1/20)\n",
    "\n",
    "mlp_c = np.vstack((c[8,:],c[9,:],c[10,:],c[11,:]))\n",
    "cnn_c = np.vstack((c[0,:],c[1,:],c[2,:],c[3,:]))\n",
    "nn_c = np.stack((mlp_c,cnn_c))\n",
    "lda_c = ['ko-','ko--']\n",
    "nn_m = ['o-','s-','v-','x-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(rlda_acc)\n",
    "\n",
    "for mod in range(nn_i.shape[0]):\n",
    "    i_ = nn_i[mod,...]\n",
    "    fig,ax = plt.subplots(1,3,figsize=(16,4))\n",
    "\n",
    "    # for i in range(len(lda_i)):\n",
    "        # ax[0].plot(acc_all[mask[:,lda_i[i]],lda_i[i]][1::2], lda_c[i])\n",
    "    for i in range(len(i_)):\n",
    "        ax[0].plot(acc_all[mask[:,i_[i]],i_[i]][1::2], nn_m[i], color=nn_c[mod,i,:])\n",
    "        ax[1].plot(rlda_acc[mask[:,i_[i]],i_[i]][1::2], nn_m[i], color=nn_c[mod,i,:])\n",
    "        ax[2].plot(lda_acc[mask[:,i_[i]],i_[i]][1::2], nn_m[i], color=nn_c[mod,i,:])\n",
    "\n",
    "    if mod == 0:\n",
    "        ax[0].legend(['LDA', 'r-LDA', 'MLP', 'r-MLP', 'a-MLP', 'ewc-MLP'])\n",
    "        ax[1].legend(['MLP vs. r-LDA', 'r-MLP vs. r-LDA', 'a-MLP vs. r-LDA', 'ewc-MLP vs. r-LDA'])\n",
    "        ax[2].legend(['MLP vs. LDA', 'r-MLP vs. LDA', 'a-MLP vs. LDA', 'ewc-MLP vs. LDA'])\n",
    "    else:\n",
    "        ax[0].legend(['LDA', 'r-LDA', 'CNN', 'r-CNN', 'a-CNN', 'ewc-CNN'])\n",
    "        ax[1].legend(['CNN vs. r-LDA', 'r-CNN vs. r-LDA', 'a-CNN vs. r-LDA', 'ewc-CNN vs. r-LDA'])\n",
    "        ax[2].legend(['CNN vs. LDA', 'r-CNN vs. LDA', 'a-CNN vs. LDA', 'ewc-CNN vs. LDA'])\n",
    "\n",
    "    ax[0].set_ylim([0,100])\n",
    "    ax[1].axhline(0, ls = '--',color='black')\n",
    "    ax[2].axhline(0, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,4))\n",
    "x = range(0,acc.shape[0]-2,1)\n",
    "x_skip = range(0,acc.shape[0]-2,2)\n",
    "\n",
    "for i in range(3):\n",
    "    ax[0].plot(x_skip,acc[2::2,i],'o', label='_nolegend_')\n",
    "    ax[0].plot(x,acc[2:,i],'-')\n",
    "\n",
    "    if i == 0:\n",
    "        ls = 's--'\n",
    "    else:\n",
    "        ls = 's-'\n",
    "    ax[1].plot(x_skip,recal_acc[mask,i], ls)\n",
    "\n",
    "    if i > 0:\n",
    "        ax[2].plot(x_skip,adapt_acc[mask,i], 'v-')\n",
    "\n",
    "    ax[i].set_ylim([0,100])\n",
    "\n",
    "ax[0].legend(['LDA','MLP','CNN'])\n",
    "ax[1].legend(['r-LDA','r-MLP','r-CNN'])\n",
    "ax[2].legend(['a-MLP','a-CNN'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,4))\n",
    "x = range(0,acc.shape[0]-2,1)\n",
    "x_skip = range(0,acc.shape[0]-2,2)\n",
    "\n",
    "for i in range(3):\n",
    "    ax[0].plot(x_skip,acc[2::2,i],'o',color=col[i,:], label='_nolegend_')\n",
    "    ax[0].plot(x,acc[2:,i],'-',color=col[i,:])\n",
    "\n",
    "    if i == 0:\n",
    "        ls = 's--'\n",
    "    else:\n",
    "        ls = 's-'\n",
    "    ax[1].plot(x_skip,recal_acc[mask,i], ls,color=col[i,:])\n",
    "\n",
    "    if i > 0:\n",
    "        ax[2].plot(x_skip,adapt_acc[mask,i], 'v-',color=col[i,:])\n",
    "\n",
    "    ax[i].set_ylim([0,100])\n",
    "\n",
    "ax[0].legend(['LDA','MLP','CNN'])\n",
    "ax[1].legend(['r-LDA','r-MLP','r-CNN'])\n",
    "ax[2].legend(['a-MLP','a-CNN'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all controllers with adaptation\n",
    "ep = 30\n",
    "# Remove extra file if odd number of files\n",
    "# if len(all_files)%2 == 0:\n",
    "#     all_files = np.delete(all_files,-1)\n",
    "align_acc = np.empty((len(all_files),5))\n",
    "align_acc[:] = np.nan\n",
    "n_dof = len(train_dof)\n",
    "\n",
    "for i in range(2,len(all_files)-1):\n",
    "    # load recalibration data\n",
    "    align_file = all_files[i]\n",
    "    align_data, align_params = prd.load_caps_train(path + align_file + '/traindata.mat')\n",
    "    align_data = align_data[:,:8,:]\n",
    "\n",
    "    # check class labels\n",
    "    align_data, align_params = prd.threshold(align_data, align_params)\n",
    "    align_data, align_params = lp.check_labels(align_data, align_params, train_dof, key)\n",
    "    \n",
    "    alignmlp, aligncnn, _, _, _, x_train_lda, y_train_lda, _, _, _, _, _ = prd.prep_train_caps(align_data, align_params, prop_b = False, num_classes=n_dof, batch_size=32, noise=False, scaler=scaler, emg_scale=emg_scale,ft=ft,split=True)\n",
    "    mlp, cnn, mlp_ali, cnn_ali, w, c = lp.train_models(aligncnn, alignmlp, x_train_lda, y_train_lda, n_dof, ep, mlp, cnn, align=True)\n",
    "\n",
    "    # test \n",
    "    y_test, x_test_mlp, x_test_cnn, x_lda, y_lda = prd.prep_test_caps(align_data, align_params, scaler, emg_scale, num_classes=n_dof,ft=ft, split=True)\n",
    "    align_acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, cnn, mlp, [w, c], cnn_align=cnn_ali, mlp_align=mlp_ali)\n",
    "\n",
    "    print ('Align: ' + align_file + ',', f'CNN Accuracy: {align_acc[i,2]:.2f},', f'MLP Accuracy: {align_acc[i,1]:.2f},', f'LDA Accuracy: {align_acc[i,0]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e4d54467b05e62951c9fd7929782b99429e3b62c1a3b146d4f3dbf79f907e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('adapt_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
