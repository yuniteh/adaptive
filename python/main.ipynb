{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpu import set_gpu\n",
    "import numpy as np\n",
    "import os\n",
    "import adapt.utils.data_utils as prd\n",
    "import adapt.loop as lp\n",
    "import adapt.ml.lda as dlda\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import adapt.ml.dl_subclass as dl\n",
    "import copy as cp\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython import display\n",
    "import gc as gc\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mu_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR62\n",
      "train dof: [ 1  6 16 19 48 90], key: [0 1 2 3 4 5]\n",
      "setting CNN weights\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180524_161811, Accuracy: 99.74 , Val: 99.32 , Prev: 0.00 , Train: 98.93\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180525_084201, Accuracy: 82.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180531_073149, Accuracy: 89.48 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180602_105936, Accuracy: 80.53 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180604_090437, Accuracy: 74.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20180604_090437\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.1324222087860107\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180604_214053, Accuracy: 81.78 , Val: 79.50 , Prev: 80.79 , Train: 100.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180605_080547, Accuracy: 81.42 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180606_174322, Accuracy: 61.43 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20180606_174322\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.1730751991271973\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180606_174322, Test: 20180610_081839, Accuracy: 67.83 , Val: 84.91 , Prev: 72.53 , Train: 100.00\n",
      "recal: 3 20180610_081839\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.831636667251587\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180612_085047, Accuracy: 78.40 , Val: 87.83 , Prev: 83.77 , Train: 100.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180614_081624, Accuracy: 81.21 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180616_182930, Accuracy: 71.06 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 4 20180616_182930\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.9031143188476562\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180616_182930, Test: 20180617_184248, Accuracy: 61.43 , Val: 88.24 , Prev: 79.50 , Train: 99.90\n",
      "recal: 5 20180617_184248\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.1531028747558594\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180619_193541, Accuracy: 81.52 , Val: 51.14 , Prev: 83.56 , Train: 99.37\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073139, Accuracy: 77.93 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073838, Accuracy: 66.37 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 6 20180620_073838\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.4240286350250244\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_073838, Test: 20180620_083903, Accuracy: 69.50 , Val: 86.06 , Prev: 69.09 , Train: 100.00\n",
      "recal: 7 20180620_083903\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.161031484603882\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180621_094013, Accuracy: 81.62 , Val: 86.47 , Prev: 83.87 , Train: 98.33\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180623_200107, Accuracy: 68.82 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 8 20180623_200107\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.0940957069396973\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180703_072425, Accuracy: 78.34 , Val: 80.54 , Prev: 76.07 , Train: 99.90\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180711_074144, Accuracy: 77.30 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180711_113445, Accuracy: 55.96 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 9 20180711_113445\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.1898834705352783\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180713_102029, Accuracy: 75.27 , Val: 83.35 , Prev: 65.76 , Train: 99.27\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180717_073851, Accuracy: 71.53 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 10 20180717_073851\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.11932373046875\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180717_073851, Test: 20180717_112511, Accuracy: 82.67 , Val: 86.17 , Prev: 53.69 , Train: 99.48\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180717_073851, Test: 20190321_073718, Accuracy: 46.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 11 20190321_073718\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.192763090133667\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_073718, Test: 20190321_095657, Accuracy: 63.48 , Val: 77.65 , Prev: 75.78 , Train: 99.25\n",
      "recal: 12 20190321_095657\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2971694469451904\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_095657, Test: 20190325_084612, Accuracy: 56.78 , Val: 69.74 , Prev: 74.02 , Train: 99.81\n",
      "recal: 13 20190325_084612\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2474164962768555\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190325_084612, Test: 20190326_073702, Accuracy: 60.76 , Val: 74.58 , Prev: 59.50 , Train: 99.81\n",
      "recal: 14 20190326_073702\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.315352201461792\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190328_125619, Accuracy: 73.29 , Val: 83.46 , Prev: 81.75 , Train: 98.79\n",
      "recal: 15 20190328_125619\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.231074810028076\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190328_125619, Test: 20190331_112350, Accuracy: 81.74 , Val: 75.77 , Prev: 76.86 , Train: 99.35\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190328_125619, Test: 20190402_073436, Accuracy: 66.37 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 16 20190402_073436\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.365314245223999\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_073436, Test: 20190402_142249, Accuracy: 64.51 , Val: 79.89 , Prev: 53.30 , Train: 99.91\n",
      "recal: 17 20190402_142249\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.1635921001434326\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190403_050414, Accuracy: 74.15 , Val: 83.71 , Prev: 71.97 , Train: 98.88\n",
      "recal: 18 20190403_050414\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2542593479156494\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190403_050414, Test: 20190403_052115, Accuracy: 81.37 , Val: 81.38 , Prev: 77.28 , Train: 99.72\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190403_050414, Test: 20190404_075938, Accuracy: 69.49 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 19 20190404_075938\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.153610944747925\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190404_085916, Accuracy: 76.15 , Val: 73.65 , Prev: 83.33 , Train: 100.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190406_091945, Accuracy: 72.05 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 20 20190406_091945\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.320906639099121\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190410_102236, Accuracy: 82.63 , Val: 85.23 , Prev: 82.77 , Train: 99.64\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190410_102641, Accuracy: 80.48 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190415_073441, Accuracy: 88.12 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190415_102702, Accuracy: 78.45 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190423_063337, Accuracy: 79.46 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190424_045927, Accuracy: 82.21 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190502_102725, Accuracy: 83.52 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190505_082637, Accuracy: 80.76 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190505_083227, Accuracy: 72.75 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 21 20190505_083227\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.6537365913391113\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190505_083227, Test: 20190518_142954, Accuracy: 69.73 , Val: 76.72 , Prev: 82.61 , Train: 99.44\n",
      "------------------------acnn05 17 - 4 -- 0-------------\n",
      "train dof: [ 1  6 16 19 48 90], key: [0 1 2 3 4 5]\n",
      "setting CNN weights\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180524_161811, Accuracy: 99.74 , Val: 99.32 , Prev: 0.00 , Train: 98.33\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180525_084201, Accuracy: 82.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180531_073149, Accuracy: 89.48 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180602_105936, Accuracy: 80.53 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180604_090437, Accuracy: 74.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20180604_090437\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.9941434860229492\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180604_214053, Accuracy: 82.56 , Val: 83.56 , Prev: 80.87 , Train: 99.69\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180605_080547, Accuracy: 81.42 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180606_174322, Accuracy: 62.31 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20180606_174322\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2705020904541016\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180606_174322, Test: 20180610_081839, Accuracy: 69.13 , Val: 84.91 , Prev: 71.38 , Train: 98.75\n",
      "recal: 3 20180610_081839\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.3423190116882324\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180612_085047, Accuracy: 77.88 , Val: 85.33 , Prev: 85.74 , Train: 99.79\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180614_081624, Accuracy: 74.91 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 4 20180614_081624\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2083804607391357\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180614_081624, Test: 20180616_182930, Accuracy: 68.92 , Val: 87.20 , Prev: 75.86 , Train: 99.79\n",
      "recal: 5 20180616_182930\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.1284117698669434\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180616_182930, Test: 20180617_184248, Accuracy: 61.11 , Val: 88.45 , Prev: 75.86 , Train: 100.00\n",
      "recal: 6 20180617_184248\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.6943001747131348\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180619_193541, Accuracy: 82.20 , Val: 52.70 , Prev: 87.30 , Train: 98.54\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073139, Accuracy: 76.73 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073838, Accuracy: 70.17 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 7 20180620_073838\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.9870328903198242\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_073838, Test: 20180620_083903, Accuracy: 63.87 , Val: 81.69 , Prev: 63.58 , Train: 100.00\n",
      "recal: 8 20180620_083903\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.02575421333313\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180621_094013, Accuracy: 79.75 , Val: 82.93 , Prev: 88.03 , Train: 95.94\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180623_200107, Accuracy: 66.06 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 9 20180623_200107\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.815749168395996\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180703_072425, Accuracy: 78.24 , Val: 80.75 , Prev: 75.96 , Train: 98.33\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180711_074144, Accuracy: 75.64 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180711_113445, Accuracy: 56.17 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 10 20180711_113445\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.8467154502868652\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180713_102029, Accuracy: 78.50 , Val: 86.06 , Prev: 65.66 , Train: 98.85\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180717_073851, Accuracy: 72.15 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 11 20180717_073851\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.4236249923706055\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180717_073851, Test: 20180717_112511, Accuracy: 88.13 , Val: 86.59 , Prev: 48.60 , Train: 99.06\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180717_073851, Test: 20190321_073718, Accuracy: 51.56 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 12 20190321_073718\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.9860787391662598\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_073718, Test: 20190321_095657, Accuracy: 70.28 , Val: 79.33 , Prev: 77.34 , Train: 97.48\n",
      "recal: 13 20190321_095657\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.189354419708252\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_095657, Test: 20190325_084612, Accuracy: 56.68 , Val: 65.08 , Prev: 74.12 , Train: 98.60\n",
      "recal: 14 20190325_084612\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.620628833770752\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190325_084612, Test: 20190326_073702, Accuracy: 59.04 , Val: 75.05 , Prev: 59.22 , Train: 98.42\n",
      "recal: 15 20190326_073702\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.629474639892578\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190328_125619, Accuracy: 78.73 , Val: 82.62 , Prev: 74.39 , Train: 97.86\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190331_112350, Accuracy: 88.12 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190402_073436, Accuracy: 79.32 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190402_142249, Accuracy: 73.22 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 16 20190402_142249\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 4.125767946243286\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190403_050414, Accuracy: 64.88 , Val: 80.82 , Prev: 81.51 , Train: 97.48\n",
      "recal: 17 20190403_050414\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.147894859313965\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190403_050414, Test: 20190403_052115, Accuracy: 78.44 , Val: 81.10 , Prev: 77.93 , Train: 100.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190403_050414, Test: 20190404_075938, Accuracy: 66.60 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 18 20190404_075938\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.568018913269043\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190404_085916, Accuracy: 79.02 , Val: 77.19 , Prev: 85.01 , Train: 100.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190406_091945, Accuracy: 72.81 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 19 20190406_091945\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 4.051378488540649\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190410_102236, Accuracy: 81.88 , Val: 86.13 , Prev: 80.17 , Train: 99.19\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190410_102641, Accuracy: 80.30 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190415_073441, Accuracy: 87.75 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190415_102702, Accuracy: 78.81 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190423_063337, Accuracy: 79.37 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190424_045927, Accuracy: 82.49 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190502_102725, Accuracy: 82.68 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190505_082637, Accuracy: 81.23 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190505_083227, Accuracy: 71.59 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 20 20190505_083227\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.9047677516937256\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190505_083227, Test: 20190518_142954, Accuracy: 70.61 , Val: 74.49 , Prev: 85.68 , Train: 97.48\n",
      "------------------------avcnn 17 - 3 -- 0-------------\n"
     ]
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = True\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn2','acnn05','acnn30','acewc30','acewc15', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn', 'avcnn15', 'acnnl03','crvcnn','acewclm','crcnn','acewc00','xtra2']\n",
    "ft = 'tdar'\n",
    "iter = 10\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_mod = 0\n",
    "\n",
    "for it in range(1):#iter):\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    if it == 0:\n",
    "        mod_all = ['blda','lda','alda','cnn','acnn05','avcnn']#,'crcnn']\n",
    "        mod_all = ['lda','alda','acnn05','avcnn']\n",
    "        mod_all = ['acnn05','avcnn']\n",
    "    else:\n",
    "        mod_all = ['bcnn','cnn','acnn05','avcnn']\n",
    "\n",
    "    for sub in range(4,5):\n",
    "        print(subs[sub])\n",
    "        sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "        all_files = os.listdir(sub_path)\n",
    "        if 'skip' in all_files:\n",
    "            all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "        # load or initialize cnn weights\n",
    "        if load_mod:\n",
    "            with open(subs[sub] + '_' + str(it) + '_r_accs_y.p','rb') as f:\n",
    "                all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof,_, _, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        else:\n",
    "            c_weights = None\n",
    "            v_weights = None\n",
    "            v_wc = None\n",
    "            cl_wc = None\n",
    "            scaler_0 = None\n",
    "            all_recal = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "            rows, cols = (len(all_files), len(mod_tot))\n",
    "            all_dof = [[0]*cols]*rows\n",
    "\n",
    "        mod_i = 0\n",
    "        for mod in mod_all:\n",
    "            acc = np.zeros((len(all_files),2))\n",
    "            acc_val = np.zeros((len(all_files),2))\n",
    "            acc_prev = np.zeros((len(all_files),2))\n",
    "            acc_train = np.zeros((len(all_files),2))\n",
    "            mod_recal = np.zeros((len(all_files),))\n",
    "\n",
    "            if 'lda' in mod:\n",
    "                acc_i = 0\n",
    "            else:\n",
    "                acc_i = 1\n",
    "\n",
    "            cnn = None\n",
    "            ewc = None\n",
    "            clda = None\n",
    "\n",
    "            ep = 30\n",
    "            recal = 0\n",
    "            skip_recal = 0\n",
    "            skip = False\n",
    "\n",
    "            # Loop through files\n",
    "            for i in range(0,len(all_files)-1):\n",
    "                # Check if need to recalibrate\n",
    "                if i > 0:\n",
    "                    if acc[i,acc_i] < 75:\n",
    "                        skip = False\n",
    "                        recal += 1\n",
    "                        print('recal: ' + str(recal) + ' ' + all_files[i])\n",
    "                        acc[i,acc_i] *= -1\n",
    "                        mod_recal[i] = 1\n",
    "                    else:\n",
    "                        skip = True\n",
    "                    \n",
    "                    if 'b' in mod:\n",
    "                        skip = True\n",
    "                        \n",
    "                if not skip:\n",
    "                    # load training file\n",
    "                    train_file = all_files[i]\n",
    "                    train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "                    # if first train, use two train files\n",
    "                    if i == 0:\n",
    "                        # load training file\n",
    "                        train_file2 = all_files[i+1]\n",
    "                        train_data2, train_params2 = prd.load_caps_train(sub_path + train_file2 + '/traindata.mat')\n",
    "                        train_data = np.vstack((train_data,train_data2))\n",
    "                        train_params = np.vstack((train_params,train_params2))\n",
    "                        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "                        val_data, val_params = train_data, train_params\n",
    "                    else:\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                    \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "                        \n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "                        del train_temp, params_temp, tr_i, te_i\n",
    "                        \n",
    "                    # if combining, save current training data\n",
    "                    # if 'cr' in mod:\n",
    "                    #     # combine old and new training data\n",
    "                    #     if i > 1:\n",
    "                    #         train_data = np.vstack((train_data_0,train_data))\n",
    "                    #         train_params = np.vstack((train_params_0,train_params))\n",
    "\n",
    "                    #     train_data_0 = cp.deepcopy(train_data)\n",
    "                    #     train_params_0 = cp.deepcopy(train_params)\n",
    "\n",
    "                    # if (i == 1 and mod[0] == 'a') or (i == 1 and mod[:2] == 'cr') or (mod[0] != 'a' and mod[:2] != 'cr'):\n",
    "                        \n",
    "                    if i > 0:\n",
    "                        # get previous dofs\n",
    "                        prev_ndof = [n_dof, key, train_dof]\n",
    "                        if mod[0] == 'a':\n",
    "                            print('prev: ' + str(train_dof))\n",
    "                            # get current dofs and create key\n",
    "                            train_dof = np.unique(train_params[:,-1])\n",
    "                            print('cur: ' + str(train_dof))\n",
    "                            key = np.zeros((len(train_dof),))\n",
    "                            # check if current dofs are all in old dof list\n",
    "                            dof_ovlp = np.isin(train_dof,prev_ndof[2],assume_unique=True)\n",
    "                            temp_dof = cp.deepcopy(train_dof)\n",
    "                            # loop through dofs that are in previous dofs, set the keys\n",
    "                            for dof in train_dof[dof_ovlp]:\n",
    "                                key[train_dof==dof] = prev_ndof[1][prev_ndof[2]==dof]\n",
    "                                temp_dof[train_dof==dof] = prev_ndof[2][prev_ndof[2]==dof]\n",
    "\n",
    "                            # check if previous dofs has classes not in this set\n",
    "                            dof_xtra = ~np.isin(prev_ndof[2],temp_dof,assume_unique=True)\n",
    "                            temp_dof = np.hstack((temp_dof,prev_ndof[2][dof_xtra]))\n",
    "                            key = np.hstack((key,prev_ndof[1][dof_xtra]))\n",
    "\n",
    "                            # loop through dofs that are not in previous dofs (ie new classes), add keys\n",
    "                            key_i = 1\n",
    "                            xtra = False\n",
    "                            for dof in train_dof[~dof_ovlp]:\n",
    "                                # if 'lda' in mod:\n",
    "                                # xtra = True\n",
    "                                # key[temp_dof==dof] = np.max(key) + 1\n",
    "                                    # key_i += 1\n",
    "                                # else:\n",
    "                                    # remove extras\n",
    "                                print('removing train ' + str(dof))\n",
    "                                ind = train_params[:,-1] == dof\n",
    "                                train_params = train_params[~ind,...]\n",
    "                                train_data = train_data[~ind,...]\n",
    "                                ind = val_params[:,-1] == dof\n",
    "                                val_params = val_params[~ind,...]\n",
    "                                val_data = val_data[~ind,...]\n",
    "                                key = np.delete(key,temp_dof==dof)\n",
    "                                temp_dof = np.delete(temp_dof,temp_dof==dof)\n",
    "\n",
    "                            train_dof = cp.deepcopy(temp_dof)\n",
    "                        else:\n",
    "                            # get current dofs and create key\n",
    "                            train_dof = np.unique(train_params[:,-1])\n",
    "                            key = np.arange(len(train_dof))\n",
    "                    else:\n",
    "                        # get current dofs and create key\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        key = np.arange(len(train_dof))\n",
    "\n",
    "                    n_dof = len(train_dof)\n",
    "                    all_dof[i][mod_tot.index(mod)] = train_dof\n",
    "\n",
    "                    train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key,False)\n",
    "                    val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key,False)\n",
    "\n",
    "                    print('train dof: ' + str(train_dof) + ', key: ' + str(key))\n",
    "\n",
    "                    if (mod[0] == 'a' and i > 0) or ('cr' in mod and i > 0) or (mod == 'vcnn' and i > 0):\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale, scaler=scaler, num_classes=n_dof, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False)\n",
    "                    else:\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "                        if ((i == 0) and (c_weights is not None)) or ((i == 0) and (v_weights is not None)):\n",
    "                            scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                    _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "                    if 'cr' in mod:\n",
    "                        # combine old and new training data\n",
    "                        if i > 0:\n",
    "                            x_clean_cnn = np.vstack((clean_data_0,x_clean_cnn))\n",
    "                            y_clean = np.vstack((clean_params_0,y_clean))\n",
    "                            x_train_cnn = np.vstack((x_clean_cnn,x_train_cnn))\n",
    "                            y_train = np.vstack((y_clean,y_train))\n",
    "\n",
    "                    del train_data, train_params, val_data, val_params\n",
    "\n",
    "                    if 'lda' not in mod:\n",
    "                        cnnlda = 'l' in mod\n",
    "                        if i == 0:\n",
    "                            if c_weights is None:\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda, bn_training=True, prog_train=True)\n",
    "                                c_weights = cp.deepcopy([cnn.enc.get_weights(),cnn.clf.get_weights()])\n",
    "                                scaler_0 = cp.deepcopy(scaler)    \n",
    "                            else:\n",
    "                                print('setting CNN weights')\n",
    "                                cnn = dl.CNN(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                                if cnnlda:\n",
    "                                    print('setting LDA weights')\n",
    "                                    w_c = cp.deepcopy(cl_wc[0].astype('float32'))\n",
    "                                    c_c = cp.deepcopy(cl_wc[1].astype('float32'))\n",
    "                            if 'ewc' in mod:\n",
    "                                cnn = dl.EWC(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                            if 'ad' in mod:\n",
    "                                cnn = dl.CNN(n_class=n_dof,adapt=True)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                            if 'av' in mod:\n",
    "                                mu_class, std_class, N = prd.set_mean(x_clean_cnn,y_clean)\n",
    "\n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                            \n",
    "                            if scaler_0 is None:\n",
    "                                scaler_0 = cp.deepcopy(scaler)\n",
    "                            else:\n",
    "                                scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                        else:\n",
    "                            prev_w = cnn.get_weights()\n",
    "                            if xtra and mod[0] == 'a':\n",
    "                                print('add neuron')\n",
    "                                cnn = dl.CNN(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                w_temp = cnn.get_weights()\n",
    "                                w_temp[:-2] = prev_w[:-2]\n",
    "                                w_temp[-2][:,:-1] = prev_w[-2]\n",
    "                                w_temp[-1][:-1] = prev_w[-1]\n",
    "                                cnn.set_weights(w_temp)\n",
    "                            if 'adcnn' in mod: # adapt first layer only\n",
    "                                # cnn.base.trainable=False\n",
    "                                cnn.clf.trainable=False\n",
    "                                ep = int(mod[-2:])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], adapt=True, cnnlda=cnnlda, lr=0.00001)\n",
    "                            elif 'avcnn' in mod:\n",
    "                                # prev_w = cnn.get_weights()\n",
    "                                # prev_w = cnn.get_weights()\n",
    "                                prev_mu = [mu_class, std_class, N]\n",
    "                                # generate old training data, same size as clean data\n",
    "                                # x_out = cp.deepcopy(x_clean_cnn)\n",
    "                                # num_y = x_out.shape[0]//prev_ndof[0]\n",
    "                                # y_gen = np.zeros((x_out.shape[0],n_dof))\n",
    "                                # for cl in prev_ndof[1]:\n",
    "                                #     cl_i = prev_ndof[1]==cl\n",
    "                                #     y_gen[int(cl*num_y):int((cl+1)*num_y),np.where(prev_ndof[1]==cl)] = 1\n",
    "                                #     x_out[int(cl*num_y):int((cl+1)*num_y),...] = np.random.normal(mu_class[cl_i], std_class[cl_i],x_clean_cnn[:num_y,...].shape)\n",
    "                                \n",
    "                                x_out = cp.deepcopy(x_clean_cnn)\n",
    "                                num_y = x_out.shape[0]//prev_ndof[0]\n",
    "                                y_gen = np.zeros((x_out.shape[0],n_dof))\n",
    "                                y_xtra = 0\n",
    "                                x_xtra = 0\n",
    "                                for cl in prev_ndof[1]:\n",
    "                                    cl_i = np.where(prev_ndof[1]==cl)\n",
    "                                    x_ind = np.squeeze(y_clean[:,cl_i]==1)\n",
    "                                    if np.sum(x_ind) > 0:\n",
    "                                        y_gen[x_ind,np.where(prev_ndof[1]==cl)] = 1\n",
    "                                        x_out[x_ind,...] = np.random.normal(mu_class[cl_i], std_class[cl_i],x_clean_cnn[x_ind,...].shape)\n",
    "                                    else:\n",
    "                                        temp = np.zeros((num_y,n_dof))\n",
    "                                        temp[:,cl_i] = 1\n",
    "                                        if isinstance(y_xtra,np.ndarray):\n",
    "                                            y_xtra = np.vstack((y_xtra,temp))\n",
    "                                            x_xtra = np.vstack((x_xtra,np.random.normal(mu_class[cl_i], std_class[cl_i],x_clean_cnn[:num_y,...].shape)))\n",
    "                                        else:\n",
    "                                            y_xtra = cp.deepcopy(temp)\n",
    "                                            x_xtra = np.random.normal(mu_class[cl_i], std_class[cl_i],x_clean_cnn[:num_y,...].shape)\n",
    "                                \n",
    "                                # for cl in prev_ndof[1].astype(int):\n",
    "                                #     cl_i = np.where(prev_ndof[1]==cl)\n",
    "                                #     x_ind = np.squeeze(y_clean[:,cl]==1)\n",
    "                                #     if np.sum(x_ind) > 0:\n",
    "                                #         y_gen[x_ind,np.where(prev_ndof[1]==cl)] = 1\n",
    "                                #         x_out[x_ind,...] = np.random.normal(mu_class[cl], std_class[cl],x_clean_cnn[x_ind,...].shape)\n",
    "                                #     else:\n",
    "                                #         temp = np.zeros((num_y,n_dof))\n",
    "                                #         temp[:,cl] = 1\n",
    "                                #         if isinstance(y_xtra,np.ndarray):\n",
    "                                #             y_xtra = np.vstack((y_xtra,temp))\n",
    "                                #             x_xtra = np.vstack((x_xtra,np.random.normal(mu_class[cl], std_class[cl],x_clean_cnn[:num_y,...].shape)))\n",
    "                                #         else:\n",
    "                                #             y_xtra = cp.deepcopy(temp)\n",
    "                                #             x_xtra = np.random.normal(mu_class[cl_i], std_class[cl],x_clean_cnn[:num_y,...].shape)\n",
    "\n",
    "                                if isinstance(y_xtra,np.ndarray):\n",
    "                                    x_out = np.vstack((x_out,x_xtra))\n",
    "                                    y_gen = np.vstack((y_gen,y_xtra))\n",
    "\n",
    "                                x_train_aug = np.vstack((x_out,x_train_cnn))\n",
    "                                y_train_aug = np.vstack((y_gen,y_train))\n",
    "\n",
    "                                x_train_m = np.vstack((x_out,x_clean_cnn))\n",
    "                                y_train_m = np.vstack((y_gen,y_clean))\n",
    "\n",
    "                                mu_class = [None] * \n",
    "                                for cl in key.astype(int):\n",
    "                                    cl_i = np.where(key==cl)\n",
    "                                    x_ind = np.squeeze(y_train_m[:,cl_i]==1)\n",
    "                                    mu_class[cl_i] = np.nanmean(x_train_m[x_ind,...])\n",
    "                                    std_class[cl_i] = np.nanstd(x_train_m[x_ind,...])\n",
    "                                # mu_class, std_class, N = prd.set_mean(x_clean_cnn,y_clean)\n",
    "\n",
    "                                # mu_class, std_class, N = prd.update_mean(x_clean_cnn,y_clean,N,mu_class,std_class,key,prev_ndof[1])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)],_,_ = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=5, dec=False, lr=0.00001, bn_training=False, bn_trainable=False, prog_train=xtra)\n",
    "                            elif 'acnn' in mod: # update whole CNN and lda weights\n",
    "                                ep = int(mod[-2:])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], cnnlda=cnnlda, lr=0.00001, bn_training=False, bn_trainable=False, prog_train=xtra)\n",
    "                            elif 'cnn' in mod: # recalibrate cnnlda\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda, bn_training=True, prog_train=True)\n",
    "                            elif 'acewc' in mod:\n",
    "                                w_c, c_c, all_times[i,mod_tot.index(mod)] = lp.train_task(cnn, 15, 1, x_train_cnn, y_train, [prev_x, x_val_cnn],[prev_y, y_val], lams=[int(mod[-2:])], bat=bat, cnnlda=cnnlda)\n",
    "                            \n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                        del test_mod\n",
    "                        test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                        if i > 0:\n",
    "                            acc_prev[i,:] = lp.test_models(prev_x, prev_y, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_val[i,:] = lp.test_models(x_val_cnn, y_val, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_train[i,:] = lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        \n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            mod_recal[i] = -1\n",
    "                            print('bad recal')\n",
    "                            # if acc_val[i,acc_i] < 75:\n",
    "                            #     if i > 0:\n",
    "                            #         n_dof, key, train_dof = prev_ndof\n",
    "                            #         if 'vcnn' in mod:\n",
    "                            #             mu_class, std_class, N = prev_mu\n",
    "                            #         cnn = dl.CNN(n_class = n_dof)\n",
    "                            #         cnn(x_train_cnn[:1,...])\n",
    "                            #         cnn.set_weights(prev_w)\n",
    "                            #         del test_mod\n",
    "                            #         test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                            # else:\n",
    "                            #     print('keeping new model')\n",
    "                            #     mod_recal[i] = -2\n",
    "                        elif 'cr' in mod:\n",
    "                            clean_data_0 = cp.deepcopy(x_clean_cnn)\n",
    "                            clean_params_0 = cp.deepcopy(y_clean)\n",
    "                        if 'ewc' in mod: \n",
    "                            cnn.compute_fisher(x_train_cnn, y_train, num_samples=200, plot_diffs=False) \n",
    "                            cnn.star()\n",
    "                    else:\n",
    "                        if i == 0:\n",
    "                            N = np.zeros((n_dof),)\n",
    "                            cov_class = np.zeros([x_train_lda.shape[1],x_train_lda.shape[1]])\n",
    "                            mu_class = np.zeros([n_dof,x_train_lda.shape[1]])\n",
    "                        prev_lda = [mu_class,cov_class,N]\n",
    "                        start_time = time.time()\n",
    "                        if mod[0] != 'a' or (i == 0 and mod[0] == 'a'):\n",
    "                            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda, key)\n",
    "                        else:\n",
    "                            w, c, mu_class, cov_class, N = dlda.update_lda(x_train_lda, y_train_lda, N, mu_class, cov_class, key, prev_ndof[1])\n",
    "                        all_times[i,mod_tot.index(mod)] = time.time() - start_time\n",
    "\n",
    "                        acc_val[i,:],out = lp.test_models(None, None, x_val_lda, y_val_lda, lda=[w,c])\n",
    "                        acc_train[i,:],out = lp.test_models(None, None, x_train_lda, y_train_lda, lda=[w,c])\n",
    "                        if i > 0:\n",
    "                            acc_prev[i,:],out = lp.test_models(None, None, prev_x_lda, prev_y_lda, lda=[w,c])\n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            mod_recal[i] = -1\n",
    "                            print('bad recal')\n",
    "                            # if acc_val[i,acc_i] < 75:\n",
    "                            #     if i > 0:\n",
    "                            #         mu_class, cov_class, N = prev_lda\n",
    "                            #         n_dof, key, train_dof = prev_ndof\n",
    "                            # else:\n",
    "                            #     print('keeping new model')\n",
    "                            #     mod_recal[i] = -2\n",
    "                        del x_train_lda, y_train_lda\n",
    "                    \n",
    "                    if mod_recal[i] != -1 or i == 0 :\n",
    "                        prev_x = cp.deepcopy(x_val_cnn)\n",
    "                        prev_y = cp.deepcopy(y_val)\n",
    "                        prev_x_lda = cp.deepcopy(x_val_lda)\n",
    "                        prev_y_lda = cp.deepcopy(y_val_lda)\n",
    "                    \n",
    "                    del x_train_cnn, y_train, x_val_cnn, y_val, x_val_lda, y_val_lda, x_clean_cnn, y_clean\n",
    "                \n",
    "                # load data\n",
    "                test_file = all_files[i+1]\n",
    "                test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "                \n",
    "                # check class labels\n",
    "                test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "                test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key,True)\n",
    "\n",
    "                # test \n",
    "                y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "                \n",
    "                # test \n",
    "                if 'lda' in mod:\n",
    "                    acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "                else:\n",
    "                    acc[i+1,:] = lp.test_models(x_test_cnn, y_test, x_test_lda, y_test_lda, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "\n",
    "                print ('Set: ' + train_file + ', Test: ' + test_file + ',', f'Accuracy: {acc[i+1,acc_i]:.2f}', f', Val: {acc_val[i,acc_i]:.2f}', f', Prev: {acc_prev[i,acc_i]:.2f}', f', Train: {acc_train[i,acc_i]:.2f}')\n",
    "                del y_test, x_test_cnn, x_test_lda, y_test_lda#, test_data, test_params\n",
    "\n",
    "            all_acc[:,mod_tot.index(mod)] = acc[:,acc_i]\n",
    "            all_val[:,mod_tot.index(mod)] = acc_val[:,acc_i]\n",
    "            all_prev[:,mod_tot.index(mod)] = acc_prev[:,acc_i]\n",
    "            all_train[:,mod_tot.index(mod)] = acc_train[:,acc_i]\n",
    "            all_recal[:,mod_tot.index(mod)] = mod_recal\n",
    "\n",
    "            print('------------------------' + mod + ' ' + str(np.sum(mod_recal==1)) + ' - ' + str(np.sum(mod_recal==-1)) + ' -- ' + str(np.sum(mod_recal==-2)) + '-------------')\n",
    "            mod_i += 1\n",
    "\n",
    "            # if 'cr' in mod:\n",
    "            #     del train_data_0, train_params_0\n",
    "\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','wb') as f:\n",
    "            pickle.dump([all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof, mod_all, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale],f)\n",
    "        \n",
    "        gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:69: RuntimeWarning: Mean of empty slice\n",
      "  ave_acc2 = np.nanmean(np.abs(np.array(it_acc2)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:70: RuntimeWarning: Mean of empty slice\n",
      "  ave_acc = np.nanmean(np.abs(np.array(it_acc)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:71: RuntimeWarning: Mean of empty slice\n",
      "  ave_val = np.nanmean(np.abs(np.array(it_val)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:72: RuntimeWarning: Mean of empty slice\n",
      "  ave_prev = np.nanmean(np.abs(np.array(it_prev)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:73: RuntimeWarning: Mean of empty slice\n",
      "  ave_train = np.nanmean(np.abs(np.array(it_train)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:74: RuntimeWarning: Mean of empty slice\n",
      "  ave_times = np.nanmean(np.abs(np.array(it_times)),axis=0)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1670: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "ft = 'tdar'\n",
    "iter = 6\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "plot_mod = ['lda','alda','cnn','acnn03','avcnn','crcnn']\n",
    "plot_mod = ['lda','avcnn']\n",
    "plot_mod = ['acnn05','avcnn']\n",
    "for plot_i in range(1):\n",
    "    for sub in range(4,5):    \n",
    "        it_acc = []\n",
    "        it_recal = []\n",
    "        it_fail = []\n",
    "        it_val = []\n",
    "        it_prev = []\n",
    "        it_train = []\n",
    "        it_times = []\n",
    "        it_replaced = []\n",
    "        for it in range(1):#iter):\n",
    "        \n",
    "            # load or initialize cnn weights\n",
    "            if plot_i == 1:\n",
    "                with open('0323 full run pre and post/' + subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            else:\n",
    "                with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            \n",
    "            lda_ind = mod_tot.index('alda') + 1\n",
    "            all_acc[all_acc==0] = np.nan\n",
    "            all_val[all_val==0] = np.nan\n",
    "            all_prev[all_prev==0] = np.nan\n",
    "            all_train[all_train==0] = np.nan\n",
    "            all_times[all_times==0] = np.nan\n",
    "\n",
    "            it_acc.append(all_acc)\n",
    "            it_recal.append(np.sum(all_recal==1,axis=0))\n",
    "            it_fail.append(np.sum(all_recal==-1,axis=0))\n",
    "            it_replaced.append(np.sum(all_recal==-2,axis=0))\n",
    "            it_val.append(all_val)\n",
    "            it_prev.append(all_prev)\n",
    "            it_train.append(all_train)\n",
    "            it_times.append(all_times)\n",
    "\n",
    "            it_acc[it][:,:lda_ind] = it_acc[0][:,:lda_ind]\n",
    "            it_recal[it][:lda_ind] = it_recal[0][:lda_ind]\n",
    "            it_fail[it][:lda_ind] = it_fail[0][:lda_ind]\n",
    "            it_replaced[it][:lda_ind] = it_replaced[0][:lda_ind]\n",
    "            it_val[it][:,:lda_ind] = it_val[0][:,:lda_ind]\n",
    "            it_prev[it][:,:lda_ind] = it_prev[0][:,:lda_ind]\n",
    "            it_train[it][:,:lda_ind] = it_train[0][:,:lda_ind]\n",
    "            it_times[it][:,:lda_ind] = it_times[0][:,:lda_ind]\n",
    "\n",
    "\n",
    "        it_acc2 = cp.deepcopy(it_acc)\n",
    "        for i in range(len(it_acc2)):\n",
    "            x = it_val[i] < 0\n",
    "            # print(x.type)\n",
    "            # print(it_acc2[i].shape)\n",
    "            # print(ave_val.shape)\n",
    "            it_acc2[i][(it_acc[i]< 0) & (it_val[i] > 0)]= it_val[i][(it_acc[i]< 0)& (it_val[i] > 0)]\n",
    "            # it_acc2[i][(it_acc[i]< 0)]= it_val[i][(it_acc[i]< 0)]\n",
    "\n",
    "\n",
    "        ave_acc2 = np.nanmean(np.abs(np.array(it_acc2)),axis=0)\n",
    "        ave_acc = np.nanmean(np.abs(np.array(it_acc)),axis=0)\n",
    "        ave_val = np.nanmean(np.abs(np.array(it_val)),axis=0)\n",
    "        ave_prev = np.nanmean(np.abs(np.array(it_prev)),axis=0)\n",
    "        ave_train = np.nanmean(np.abs(np.array(it_train)),axis=0)\n",
    "        ave_times = np.nanmean(np.abs(np.array(it_times)),axis=0)\n",
    "        ave_recal = np.nanmean(np.array(it_recal),axis=0)\n",
    "        ave_fail = np.nanmean(np.array(it_fail),axis=0)\n",
    "        ave_replaced = np.nanmean(np.array(it_replaced),axis=0)\n",
    "\n",
    "        std_acc2 = np.nanstd(np.abs(np.array(it_acc2)),axis=0)/np.sum(~np.isnan(np.array(it_acc2)),axis=0)\n",
    "        std_acc = np.nanstd(np.abs(np.array(it_acc)),axis=0)/np.sum(~np.isnan(np.array(it_acc)),axis=0)\n",
    "        std_val = np.nanstd(np.abs(np.array(it_val)),axis=0)/np.sum(~np.isnan(np.array(it_val)),axis=0)\n",
    "        std_prev = np.nanstd(np.abs(np.array(it_prev)),axis=0)/np.sum(~np.isnan(np.array(it_prev)),axis=0)\n",
    "        std_train = np.nanstd(np.abs(np.array(it_train)),axis=0)/np.sum(~np.isnan(np.array(it_train)),axis=0)\n",
    "        std_times = np.nanstd(np.abs(np.array(it_times)),axis=0)/np.sum(~np.isnan(np.array(it_times)),axis=0)\n",
    "        std_recal = np.nanstd(np.array(it_recal),axis=0)/np.sum(~np.isnan(np.array(it_recal)),axis=0)\n",
    "        std_fail = np.nanstd(np.array(it_fail),axis=0)/np.sum(~np.isnan(np.array(it_fail)),axis=0)\n",
    "        std_replaced = np.nanstd(np.array(it_replaced),axis=0)/np.sum(~np.isnan(np.array(it_replaced)),axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "        for mod in plot_mod:\n",
    "            plot_ind = mod_tot.index(mod)\n",
    "            # ax.plot(ave_acc[2:,plot_ind],'.-',label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            x = ~np.isnan(ave_val[:,plot_ind]) \n",
    "            # ave_acc2 = cp.deepcopy(ave_acc)\n",
    "            # ave_acc[x,plot_ind] = ave_val[x,plot_ind]\n",
    "\n",
    "            ax[0].plot(ave_acc[:,plot_ind],'.-',ms=8,label= mod + ' = '+ \"{:.2f}\".format(ave_recal[plot_ind]) + ' +/- ' + \"{:.2f}\".format(std_recal[plot_ind]) + ', ' + \"{:.2f}\".format(ave_fail[plot_ind]) +' failed'+ ', ' + \"{:.2f}\".format(ave_replaced[plot_ind]) +' replaced')#str(std_recal[plot_ind,0]))\n",
    "            # ax[0].plot(np.squeeze(np.where(x)),ave_acc[x,plot_ind],'kx',ms=12)\n",
    "            ax[1].plot(ave_acc2[:,plot_ind],'.-',ms=8,label= mod + ' = '+ \"{:.2f}\".format(ave_recal[plot_ind]) + ' +/- ' + \"{:.2f}\".format(std_recal[plot_ind])+ ', ' + \"{:.2f}\".format(ave_fail[plot_ind]) + ' failed'+ ', ' + \"{:.2f}\".format(ave_replaced[plot_ind]) +' replaced')#+ str(std_recal[plot_ind,0]))\n",
    "            # ax.plot(np.squeeze(np.where(x)), ave_val[~np.isnan(ave_val[:,plot_ind]),plot_ind],'.-',ms=8,label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            # plt.fill_between(np.arange(ave_acc[2:,plot_ind].shape[0]),ave_acc[2:,plot_ind]-std_acc[2:,plot_ind],ave_acc[2:,plot_ind]+std_acc[2:,plot_ind],alpha=.3)\n",
    "        ax[1].legend()\n",
    "        for i in range(2):\n",
    "            ax[i].axhline(75, ls='--', color='grey')\n",
    "            ax[i].set_ylim([0,100])\n",
    "        # ax[1].axhline(75, ls='--', color='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\utils\\data_utils.py:638: RuntimeWarning: invalid value encountered in sqrt\n",
      "  m = np.sqrt((m1-m2).T*(cov_inv)*(m1-m2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init test dof: [ 1  6 16 17 19 48 53]\n",
      "test_dof: [ 1  6 16 17 19 48 53], key: [0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\main.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=91'>92</a>\u001b[0m test_data, test_params \u001b[39m=\u001b[39m lp\u001b[39m.\u001b[39mcheck_labels(test_data,test_params,train_dof,key,\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=93'>94</a>\u001b[0m \u001b[39m# test \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=94'>95</a>\u001b[0m y_test, _, x_test_cnn, x_test_lda, y_test_lda \u001b[39m=\u001b[39m prd\u001b[39m.\u001b[39;49mprep_test_caps(test_data, test_params, scaler, emg_scale, num_classes\u001b[39m=\u001b[39;49mn_dof, ft\u001b[39m=\u001b[39;49mft, split\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=96'>97</a>\u001b[0m \u001b[39m# test \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=97'>98</a>\u001b[0m acc[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,:] \u001b[39m=\u001b[39m lp\u001b[39m.\u001b[39mtest_models(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m,  x_test_lda, y_test_lda, lda\u001b[39m=\u001b[39m[w,c])\n",
      "File \u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\utils\\data_utils.py:157\u001b[0m, in \u001b[0;36mprep_test_caps\u001b[1;34m(x, params, scaler, emg_scale, num_classes, ft, split)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=154'>155</a>\u001b[0m \u001b[39m# LDA data\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=155'>156</a>\u001b[0m y_lda \u001b[39m=\u001b[39m params[:,[\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=156'>157</a>\u001b[0m x_lda \u001b[39m=\u001b[39m extract_feats_caps(x_orig,ft\u001b[39m=\u001b[39;49mft)\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=157'>158</a>\u001b[0m \u001b[39m# y_lda = np.argmax(y_train_noise,axis=1)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=158'>159</a>\u001b[0m \u001b[39m# x_lda = extract_feats_caps(x_train_noise,ft=ft)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=160'>161</a>\u001b[0m \u001b[39mreturn\u001b[39;00m y_test, x_test_mlp, x_test_cnn, x_lda, y_lda\n",
      "File \u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\utils\\data_utils.py:280\u001b[0m, in \u001b[0;36mextract_feats_caps\u001b[1;34m(raw, ft, uint, order)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=276'>277</a>\u001b[0m     z_th \u001b[39m=\u001b[39m \u001b[39m0.025\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=277'>278</a>\u001b[0m     s_th \u001b[39m=\u001b[39m \u001b[39m0.015\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=279'>280</a>\u001b[0m mean_mav \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile(np\u001b[39m.\u001b[39;49mmean(raw,axis\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,np\u001b[39m.\u001b[39mnewaxis],(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,N))\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=280'>281</a>\u001b[0m raw_demean \u001b[39m=\u001b[39m raw\u001b[39m-\u001b[39mmean_mav\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=282'>283</a>\u001b[0m mav\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mabs(raw_demean),axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3436'>3437</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3437'>3438</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3439'>3440</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39m_mean(a, axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3440'>3441</a>\u001b[0m                       out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\core\\_methods.py:179\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=175'>176</a>\u001b[0m         dtype \u001b[39m=\u001b[39m mu\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39mf4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=176'>177</a>\u001b[0m         is_float16_result \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=178'>179</a>\u001b[0m ret \u001b[39m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=179'>180</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, mu\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=180'>181</a>\u001b[0m     ret \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mtrue_divide(\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=181'>182</a>\u001b[0m             ret, rcount, out\u001b[39m=\u001b[39mret, casting\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m'\u001b[39m, subok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = False\n",
    "ft = 'tdar'\n",
    "iter = 10\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_mod = 0\n",
    "\n",
    "sub_b = []\n",
    "sub_s = []\n",
    "sub_sep=[]\n",
    "for sub in range(2,7):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    # load or initialize cnn weights\n",
    "    acc = np.zeros((len(all_files),2))\n",
    "    acc_val = np.zeros((len(all_files),2))\n",
    "    acc_prev = np.zeros((len(all_files),2))\n",
    "    acc_train = np.zeros((len(all_files),2))\n",
    "    \n",
    "\n",
    "    acc_i = 0\n",
    "\n",
    "    # Loop through files\n",
    "    for i in range(len(all_files)-1):              \n",
    "        # load training file\n",
    "        train_file = all_files[i]\n",
    "        train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "        # load training file\n",
    "        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "        val_data, val_params = train_data, train_params\n",
    "            \n",
    "        # get current dofs and create key\n",
    "        if i == 0:\n",
    "            train_dof = np.unique(train_params[:,-1])\n",
    "            key = np.arange(len(train_dof))\n",
    "\n",
    "            n_dof = len(train_dof)\n",
    "\n",
    "            h = np.ones((len(all_files),n_dof))\n",
    "            b = np.ones((len(all_files),n_dof))\n",
    "            sep = np.zeros((len(all_files),n_dof))\n",
    "            tot_b = np.ones((len(all_files,)))\n",
    "            h[:] = np.nan\n",
    "            b[:] = np.nan\n",
    "\n",
    "            train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key,False)\n",
    "            val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key,False)\n",
    "\n",
    "            _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=False, split=False,num_classes=n_dof)\n",
    "\n",
    "            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda, key)\n",
    "            \n",
    "            cl_s = np.zeros((n_dof,))\n",
    "            # cl_s[:] = np.nan\n",
    "            m_cl = np.zeros((n_dof,x_train_lda.shape[1]))\n",
    "            s_cl = np.zeros((n_dof,x_train_lda.shape[1],x_train_lda.shape[1]))\n",
    "            for cl in train_dof:\n",
    "                train_ind = np.squeeze(y_train_lda == key[train_dof==cl])\n",
    "                temp_x = x_train_lda[train_ind,...]\n",
    "                m1 = np.nanmean(temp_x,axis=0)\n",
    "                s1 = np.cov(temp_x.T)\n",
    "                m_cl[key[train_dof==cl],...] = np.nanmean(temp_x,axis=0)\n",
    "                s_cl[key[train_dof==cl],...] = np.cov(temp_x.T)\n",
    "                ct = 0\n",
    "                for cl_i in train_dof:\n",
    "                    temp_ind = np.squeeze(y_train_lda == key[train_dof==cl_i])\n",
    "                    temp_x2 = x_train_lda[temp_ind,...]\n",
    "                    m2 = np.nanmean(temp_x2,axis=0)\n",
    "                    s2 = np.cov(temp_x2.T)\n",
    "                    cl_s[key[train_dof==cl]] += np.nanmean(prd.mahal(m1,s1,m2,s2))\n",
    "                    ct+=1\n",
    "                cl_s[key[train_dof==cl]] /= ct\n",
    "\n",
    "        #del x_train_lda, y_train_lda, x_train_cnn, y_train, x_clean_cnn, y_clean\n",
    "        \n",
    "        # load data\n",
    "        test_file = all_files[i+1]\n",
    "        test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "        \n",
    "        # check class labels\n",
    "        test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "        test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key,True)\n",
    "\n",
    "        # test \n",
    "        y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "        \n",
    "        # test \n",
    "        acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "\n",
    "        for cl in train_dof:\n",
    "            test_ind = np.squeeze(y_test_lda == key[train_dof==cl])\n",
    "            if i == 0:\n",
    "                train_ind = np.squeeze(y_train_lda == key[train_dof==cl])\n",
    "                m1 = np.nanmean(x_train_lda[train_ind,:],axis=0)\n",
    "                s1 = np.cov(x_train_lda[train_ind,:].T)\n",
    "            if np.sum(test_ind) > 50:\n",
    "                # print(np.sum(test_ind))\n",
    "                m2 = np.nanmean(x_test_lda[test_ind,:],axis=0)\n",
    "                s2 = np.cov(x_test_lda[test_ind,:].T)\n",
    "                h[i+1,key[train_dof==cl]] = prd.hellinger(m1,s1,m2,s2)\n",
    "                b[i+1,key[train_dof==cl]] = np.nanmean(prd.mahal(m1,s1,m2,s2))\n",
    "                ct = 0\n",
    "                for cl_i in train_dof:\n",
    "                    cl_sep = np.nanmean(prd.mahal(np.squeeze(m_cl[key[train_dof==cl_i],...]),np.squeeze(s_cl[key[train_dof==cl_i],...]),m2,s2))\n",
    "                    if np.isnan(cl_sep):\n",
    "                        print('oops')\n",
    "                    else:\n",
    "                        ct += 1\n",
    "                        sep[i+1,key[train_dof==cl]] += cl_sep\n",
    "                sep[i+1,key[train_dof==cl]] /= ct\n",
    "                # b[i+1,key[train_dof==cl]] = prd.bhatta(m1,s1,m2,s2)\n",
    "        # print(h[i+1,:])\n",
    "        # m1 = np.nanmean(x_train_lda,axis=0)\n",
    "        # s1 = np.cov(x_train_lda.T)\n",
    "        # m2 = np.nanmean(x_test_lda,axis=0)\n",
    "        # s2 = np.cov(x_test_lda.T)\n",
    "        # b[i+1,0] = np.min(h[i+1,:])\n",
    "\n",
    "        print(\"{:.2f}\".format(acc[i+1,0]))\n",
    "        del y_test, x_test_cnn#, x_test_lda, y_test_lda, test_data, test_params\n",
    "\n",
    "    print(np.median(acc,axis=0))\n",
    "    with open(subs[sub] + '_hell.p','wb') as f:\n",
    "        pickle.dump([acc,h,b,cl_s,sep],f)\n",
    "    sub_b.append(b)\n",
    "    sub_s.append(cl_s)\n",
    "    sub_sep.append(sep)\n",
    "    \n",
    "    gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 4\n",
    "h[h==0] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "for sub in range(7):\n",
    "    with open(subs[sub] + '_hell.p','rb') as f:\n",
    "        acc,h,b,cl_s,sep = pickle.load(f)\n",
    "    min_h = np.nanmax(h[1:,:],axis=1)\n",
    "    norm_m = b/np.nanmean(cl_s)\n",
    "    acc_p = acc[1:,0]\n",
    "    fig,ax = plt.subplots()\n",
    "    fig1,ax1 = plt.subplots(1,2,figsize=(15,3))\n",
    "    # ax.plot(-np.nanmean(b[1:,:],axis=1),acc_p, 'x')\n",
    "    ax.plot(np.nanmax(b[1:,:],axis=1),acc_p, 'x')\n",
    "    ax1[0].plot(np.nanmax(b[1:,:],axis=1), 'x')\n",
    "    ax1[1].plot(acc_p, 'x')\n",
    "    ax1[1].set_ylim([0,100])\n",
    "    # ax1[0].set_ylim([0,20])\n",
    "    # ax.set_xlim([0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for sub in range(7):\n",
    "    with open(subs[sub] + '_lda_accs.p','rb') as f:\n",
    "        acc, _ = pickle.load(f)\n",
    "    temp.append(acc[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 128\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn','acnn03','acnn30','acewc00','acewc30', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn03', 'avcnn15', 'acnnl03','crvcnn','acewclm','xtra','xtra1','xtra2']\n",
    "ft = 'feat'\n",
    "iter = 1\n",
    "\n",
    "for sub in range(4,5):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    mod_all = ['vcnn']\n",
    "\n",
    "    # load or initialize cnn weights\n",
    "    if load_mod:\n",
    "        with open(subs[sub] + '_' + str(0) + '_r_accs.p','rb') as f:\n",
    "            all_acc, all_recal, all_val, all_prev, all_train, all_times, _, _, c_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "    else:\n",
    "        c_weights = None\n",
    "        v_weights = None\n",
    "        v_wc = None\n",
    "        cl_wc = None\n",
    "        all_recal = np.empty((len(mod_tot),1))\n",
    "        all_recal[:] = np.nan\n",
    "        all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "    mod_i = 0\n",
    "    for mod in mod_all:\n",
    "        acc = np.zeros((len(all_files),5))\n",
    "        acc_val = np.zeros((len(all_files),5))\n",
    "        acc_prev = np.zeros((len(all_files),5))\n",
    "        acc_train = np.zeros((len(all_files),5))\n",
    "\n",
    "        if 'cnn' in mod:\n",
    "            acc_i = 2\n",
    "        elif 'cewc' in mod:\n",
    "            acc_i = 4\n",
    "        elif 'lda' in mod:\n",
    "            acc_i = 0\n",
    "\n",
    "        cnn = None\n",
    "        ewc = None\n",
    "\n",
    "        ep = 50\n",
    "        recal = 0\n",
    "        skip = False\n",
    "\n",
    "        # Loop through files\n",
    "        for i in range(1,2):#len(all_files)-1):\n",
    "            # load training file\n",
    "            train_file = all_files[i]\n",
    "            train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "            train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "            val_data = train_data\n",
    "            val_params = train_params\n",
    "\n",
    "            train_dof = np.unique(train_params[:,-1])\n",
    "            key = np.empty(train_dof.shape)\n",
    "            for key_i in range(len(train_dof)):\n",
    "                key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "            n_dof = int(np.max(key))\n",
    "            \n",
    "            train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "            val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key)\n",
    "\n",
    "            _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "\n",
    "            _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "            del train_data, train_params, val_data, val_params\n",
    "\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, print_b=True)\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=[cnn], n_dof=n_dof, ep=30, dec=False,print_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_clean_cnn)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    x_out[y_clean[:,cl]==1,...] = np.random.normal(np.mean(x_clean_cnn[y_clean[:,cl]==1,...],axis=0), np.std(x_clean_cnn[y_clean[:,cl]==1,...],axis=0),x_clean_cnn[y_clean[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_train_cnn)\n",
    "for cl in range(y_train.shape[1]):\n",
    "    x_out[y_train[:,cl]==1,...] = np.random.normal(np.mean(x_train_cnn[y_train[:,cl]==1,...],axis=0), np.std(x_train_cnn[y_train[:,cl]==1,...],axis=0),x_train_cnn[y_train[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = np.ones((1*y_clean.shape[0],8))\n",
    "x_out,_,_ = cnn.dec(samp1,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),samp=True)\n",
    "# _,x_out,_,_ = cnn(x_clean_cnn,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),dec=True)\n",
    "x_out = x_out.numpy()\n",
    "# for i in range(y_clean.shape[1]):\n",
    "#     adjust1 = np.std(x_clean_cnn[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     rescale = np.mean(adjust1)/np.mean(np.std(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0))\n",
    "#     gmean = np.mean(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     x_out[np.argmax(y_clean,axis=1)==i,...] = (x_out[np.argmax(y_clean,axis=1)==i,...] - gmean)*rescale + gmean\n",
    "# x_out = np.maximum(np.minimum(x_out,1),0)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    ind = np.tile(np.argmax(y_clean,axis=1)==cl,[1])\n",
    "    ind2 = np.argmax(y_clean,axis=1)==cl\n",
    "    x_temp = x_out[ind,...].reshape((np.sum(ind),-1))\n",
    "    x_true = x_clean_cnn[ind2,...].reshape((np.sum(ind2),-1))\n",
    "    # for i in range()\n",
    "    plt.figure()\n",
    "    # for i in range(x_true.shape[0]):\n",
    "    #     plt.plot(x_true[i,...],'k-')\n",
    "        \n",
    "    for i in range(x_temp.shape[0]):\n",
    "        plt.plot(x_temp[i,...],'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lda = x_clean_cnn.reshape(x_clean_cnn.shape[0],-1)\n",
    "y_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "y_train_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_lda,y_lda)\n",
    "y_out = dlda.predict(x_lda, w, c)\n",
    "print(dlda.eval_lda(w, c, x_lda, y_lda))\n",
    "x_out_lda = x_out.reshape(x_out.shape[0],-1)\n",
    "print(dlda.eval_lda(w,c, x_out_lda,np.tile(y_train_lda,[1,1])))\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_out_lda,np.tile(y_train_lda,[1,1]))\n",
    "print(dlda.eval_lda(w, c, x_out_lda,y_train_lda))\n",
    "print(dlda.eval_lda(w, c, x_lda, np.argmax(y_clean,axis=1)[...,np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn1,test_accuracy)\n",
    "print(lp.test_models(x_out, y_clean, None, None, cnn=cnn1, test_mod=test_mod, test_accuracy=test_accuracy))\n",
    "\n",
    "cnn2, _ = lp.train_models(traincnn=x_out,y_train=y_clean, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn2,test_accuracy)\n",
    "print(lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn2, test_mod=test_mod, test_accuracy=test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "fig,ax = plt.subplots(1,5,figsize=(30,4))\n",
    "for sub in range(2,3):#,5):\n",
    "    with open(subs[sub] + '_0_r_accs.p','rb') as f:\n",
    "        acc_all, recal_all, cur_all, prev_all, val_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "    # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "    colors =  cm.get_cmap('tab20c')\n",
    "    c = np.empty((20,4))\n",
    "    for i in range(20):\n",
    "        c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "    nn_c[0,-1] = 1\n",
    "    all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "    pt_m = ['ko-','o-','o-','o-','s','s','s','s','D']\n",
    "    nn_c = np.vstack((np.array([0,0,0,1]), c[0,:],c[1,:],c[2,:],c[3,:],c[4,:],c[5,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "    # nn_c[0,-1] = 1\n",
    "\n",
    "    labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "    labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "    # labels = mod_tot\n",
    "\n",
    "    ax_ind = sub\n",
    "    it = 0\n",
    "    for v in [1,2]: \n",
    "        i = mod_tot.index(mod_all[v])\n",
    "        acc_temp = acc_all[1:-1,i]\n",
    "        if not np.isnan(acc_temp).all():\n",
    "            x = np.arange(len(acc_temp))\n",
    "            recal_i = (acc_temp < 0)\n",
    "            ax[ax_ind].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v],color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "            it+=1\n",
    "\n",
    "    for i in range(5):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        \n",
    "        ax[i].set_ylim([0,100])\n",
    "        ax[i].set_title('TR' + str(i+1))\n",
    "    ax[0].legend()\n",
    "    ax[2].set_xlabel('Calibration Set')\n",
    "    ax[0].set_ylabel('Accuracy (%)')\n",
    "    plt.rc('font', size=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "for sub in range(2,3):#,5):\n",
    "    fig,ax = plt.subplots(1,4,figsize=(20,4))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, val_all,mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [1,0,0,1,2,2,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[it]+': ' + str(int(recal_all[i,0])),color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "                it+=1\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "cv_iter = 1\n",
    "for sub in range(0,5):\n",
    "    fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','o','*','o','s','D','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','f-cnn-5','f-cnn-3','f-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in [0, 3, 5, 4, 6, 7]: #range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v]+': ' + str(int(recal_all[i,0])),color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                it+=1\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "        \n",
    "\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e4d54467b05e62951c9fd7929782b99429e3b62c1a3b146d4f3dbf79f907e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('adapt_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
