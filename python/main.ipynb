{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpu import set_gpu\n",
    "import numpy as np\n",
    "import os\n",
    "import adapt.utils.data_utils as prd\n",
    "import adapt.loop as lp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import adapt.ml.dl_subclass as dl\n",
    "import copy as cp\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 3\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "path += subs[sub] + '/DATA/MAT/'\n",
    "all_files = os.listdir(path)\n",
    "if 'skip' in all_files:\n",
    "    all_files = np.delete(all_files,all_files.index('skip'))\n",
    "print(subs[sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = np.delete(all_files,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "for sub in range(5):\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "    print(subs[sub])\n",
    "\n",
    "    mod_tot = ['ld','mlp','cnn','amlp','acnn','amewc','acewc', 'crmlp', 'crcnn', 'crld']\n",
    "\n",
    "    iter = 5\n",
    "    ft = 'feat'\n",
    "    recal_all = np.empty((iter,len(mod_tot),1))\n",
    "    recal_all[:] = np.nan\n",
    "    acc_all = np.empty((iter,len(all_files),len(mod_tot)))\n",
    "    acc_all[:] = np.nan\n",
    "\n",
    "    for it in range(iter):\n",
    "        if it == 0:\n",
    "            mod_all = ['ld','cnn','acnn','acewc','crld','crcnn']\n",
    "        else:\n",
    "            mod_all = ['cnn','acnn','acewc','crcnn']\n",
    "        m_weights = None\n",
    "        c_weights = None\n",
    "        for mod in mod_all:\n",
    "            acc = np.empty((len(all_files),5))\n",
    "            acc[:] == np.nan\n",
    "\n",
    "            if 'mlp' in mod:\n",
    "                acc_i = 1\n",
    "            elif 'cnn' in mod:\n",
    "                acc_i = 2\n",
    "            elif 'mewc' in mod:\n",
    "                acc_i = 3\n",
    "            elif 'cewc' in mod:\n",
    "                acc_i = 4\n",
    "            elif 'ld' in mod:\n",
    "                acc_i = 0\n",
    "\n",
    "            mlp = None\n",
    "            cnn = None\n",
    "\n",
    "            ep = 30\n",
    "            recal = 0\n",
    "            skip = False\n",
    "\n",
    "            for i in range(1,len(all_files)-1):\n",
    "                if i > 1:\n",
    "                    if acc[i-1,acc_i] < 65:\n",
    "                        skip = False\n",
    "                        recal += 1\n",
    "                        print('recal: ' + str(recal) + ' ' + all_files[i])\n",
    "                        acc[i-1,acc_i] *= -1\n",
    "                    else:\n",
    "                        skip = True\n",
    "\n",
    "                if not skip:\n",
    "                    train_file = all_files[i]\n",
    "                    train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "                    train_data = train_data[:,:8,:]\n",
    "\n",
    "                    if i == 1:\n",
    "                        train_data2, train_params2 = prd.load_caps_train(sub_path + all_files[i-1] + '/traindata.mat')\n",
    "                        train_data2 = train_data2[:,:8,:]\n",
    "                        train_data = np.vstack((train_data,train_data2))\n",
    "                        train_params = np.vstack((train_params,train_params2))\n",
    "                        del train_data2, train_params2\n",
    "                    \n",
    "                    if 'cr' in mod and i > 1:\n",
    "                        train_data = np.vstack((train_data_0,train_data))\n",
    "                        train_params = np.vstack((train_params_0,train_params))\n",
    "                    \n",
    "                    if i == 1:\n",
    "                        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "                    else:\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "\n",
    "                    if (i == 1 and 'a' in mod) or ('a' not in mod):\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        key = np.empty(train_dof.shape)\n",
    "                        for key_i in range(len(train_dof)):\n",
    "                            key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "                    \n",
    "                    train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "                    \n",
    "                    if i == 1:\n",
    "                        n_dof = int(np.max(key))\n",
    "\n",
    "                    if ('a' in mod and i > 1):\n",
    "                        trainmlp, traincnn, y_train, x_train_mlp, x_train_cnn, _, _, _, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale, scaler=scaler, num_classes=n_dof, prop_b=False, batch_size=32, ft=ft, noise=True, split=False)\n",
    "                    else:\n",
    "                        trainmlp, traincnn, y_train, x_train_mlp, x_train_cnn, x_lda, y_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=128, ft=ft, noise=True, split=False)\n",
    "                        if (i == 1) and (c_weights is not None or m_weights is not None):\n",
    "                            scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                    if 'ewc' in mod:\n",
    "                        _, _, y_val, x_val_mlp, x_val_cnn, _, _, _, _, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=32, ft=ft, noise=False, split=False)\n",
    "                    \n",
    "                    if 'cr' in mod:\n",
    "                        train_data_0 = cp.deepcopy(train_data)\n",
    "                        train_params_0 = cp.deepcopy(train_params)\n",
    "\n",
    "                    if 'mlp' in mod:\n",
    "                        n_dof = int(np.max(key))\n",
    "                        if i == 1:\n",
    "                            if m_weights is None:\n",
    "                                mlp, _, _, _ = lp.train_models(traincnn=None, trainmlp=trainmlp, n_dof=n_dof, ep=ep)\n",
    "                                m_weights = cp.deepcopy(mlp.get_weights())\n",
    "                                scaler_0 = cp.deepcopy(scaler)\n",
    "                            else:\n",
    "                                mlp = dl.MLP(n_class=n_dof)\n",
    "                                mlp(x_train_mlp[:1,...])\n",
    "                                mlp.set_weights(m_weights)\n",
    "                        else:\n",
    "                            if 'a' in mod:\n",
    "                                mlp, _, _, _ = lp.train_models(traincnn=None, trainmlp=trainmlp, n_dof=n_dof, ep=ep, mlp=mlp)\n",
    "                                ep = 5\n",
    "                            else:\n",
    "                                mlp, _, _, _ = lp.train_models(traincnn=None, trainmlp=trainmlp, n_dof=n_dof, ep=ep)\n",
    "                    elif 'cnn' in mod:\n",
    "                        n_dof = int(np.max(key))\n",
    "                        if i == 1:\n",
    "                            if c_weights is None:\n",
    "                                _, cnn, _, _ = lp.train_models(traincnn, n_dof=n_dof, ep=ep)\n",
    "                                c_weights = cp.deepcopy(cnn.get_weights())\n",
    "                                scaler_0 = cp.deepcopy(scaler)\n",
    "                            else:\n",
    "                                cnn = dl.CNN(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.set_weights(c_weights)\n",
    "                                print('setting weights')\n",
    "                        else:\n",
    "                            if 'a' in mod:\n",
    "                                _, cnn, _, _ = lp.train_models(traincnn, n_dof=n_dof, ep=ep, cnn=cnn)\n",
    "                                ep = 5\n",
    "                            else:\n",
    "                                _, cnn, _, _ = lp.train_models(traincnn, n_dof=n_dof, ep=ep)\n",
    "                    elif 'ewc' in mod:\n",
    "                        if mod == 'acewc':\n",
    "                            ewc_params = ['CNN', x_train_cnn, c_weights, x_val_cnn]\n",
    "                        elif mod == 'amewc':\n",
    "                            ewc_params = ['MLP', x_train_mlp, m_weights, x_val_mlp]\n",
    "                        if i == 1:\n",
    "                            lam = 0\n",
    "                            ewc = dl.EWC(mod=ewc_params[0], n_class=n_dof)\n",
    "                            ewc(ewc_params[1][:1,...])\n",
    "                            if ewc_params[2] is not None:\n",
    "                                ewc.set_weights(ewc_params[2])\n",
    "                                print('setting weights')\n",
    "                            else:\n",
    "                                _, _ = lp.train_task(ewc, ep, 1, ewc_params[1], y_train, [ewc_params[1]],[y_train], lams=[lam])\n",
    "                                if mod == 'acewc':\n",
    "                                    c_weights = cp.deepcopy(ewc.get_weights())\n",
    "                                    scaler_0 = cp.deepcopy(scaler)\n",
    "                                else:\n",
    "                                    m_weights = cp.deepcopy(ewc.get_weights())\n",
    "                                    scaler_0 = cp.deepcopy(scaler)\n",
    "                        else:\n",
    "                            lam = 15\n",
    "                            _, _ = lp.train_task(ewc, ep, 1, ewc_params[1], y_train, [ewc_params[1]],[y_train], lams=[lam])\n",
    "                                \n",
    "                        ewc.compute_fisher(ewc_params[3], y_val, num_samples=200, plot_diffs=False) \n",
    "                        ewc.star()\n",
    "                        del ewc_params\n",
    "                    elif 'ld' in mod:\n",
    "                        _, _, w, c = lp.train_models(x_train_lda=x_lda, y_train_lda=y_lda)\n",
    "                        del x_lda, y_lda\n",
    "                    \n",
    "                    del x_train_mlp, x_train_cnn, y_train, traincnn, trainmlp, train_params, train_data\n",
    "                \n",
    "                # load data\n",
    "                test_file = all_files[i+1]\n",
    "                test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "                test_data = test_data[:,:8,:]\n",
    "                \n",
    "                # check class labels\n",
    "                test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "                test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key)\n",
    "\n",
    "                # test \n",
    "                y_test, x_test_mlp, x_test_cnn, x_lda, y_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "\n",
    "                # test \n",
    "                if 'mlp' in mod:\n",
    "                    acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, mlp=mlp)\n",
    "                elif 'cnn' in mod:\n",
    "                    acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, cnn)\n",
    "                elif 'mewc' in mod:\n",
    "                    acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, ewc=ewc)\n",
    "                elif 'cewc' in mod:\n",
    "                    acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, ewc_cnn=ewc)\n",
    "                elif 'ld' in mod:\n",
    "                    acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, lda=[w,c])\n",
    "\n",
    "                print ('Set: ' + train_file + ', Test: ' + test_file + ',', f'Accuracy: {acc[i,acc_i]:.2f}')\n",
    "                del y_test, x_test_mlp, x_test_cnn, x_lda, y_lda, test_data, test_params\n",
    "\n",
    "            if 'cr' in mod:\n",
    "                del train_data_0, train_params_0\n",
    "            acc_all[it,:,mod_tot.index(mod)] = acc[:,acc_i]\n",
    "            recal_all[it,mod_tot.index(mod)] = recal\n",
    "            print(mod + ' ' + str(recal))\n",
    "        \n",
    "        it_acc = acc_all[it,...]\n",
    "        it_recal = recal_all[it,...]\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','wb') as f:\n",
    "            pickle.dump([it_acc, it_recal],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-e661cc39036c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'lda'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mlp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cnn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a-mlp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'a-cnn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ewc-mlp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ewc-cnn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'c-mlp'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'c-cnn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'c-ld'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0md0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc_all\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2016x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_iter = 5\n",
    "sub = 0\n",
    "fig,ax = plt.subplots(1,4,figsize=(28,6))\n",
    "for it in range(cv_iter):\n",
    "    with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "        acc_all, recal_all = pickle.load(f)\n",
    "\n",
    "    colors =  cm.get_cmap('tab20c')\n",
    "    c = np.empty((20,4))\n",
    "    for i in range(20):\n",
    "        c[i,:] = colors(i*1/20)\n",
    "\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "    nn_c[0,-1] = 1\n",
    "    all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "    pt_m = ['ko','o','o','s','s','v','v','D','D','D']\n",
    "\n",
    "    labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "\n",
    "    d0 = datetime(int(all_files[0][:4]),int(all_files[0][4:6]),int(all_files[0][6:8]),int(all_files[0][9:11]),int(all_files[0][11:13]),int(all_files[0][13:]))\n",
    "    delta = np.empty((len(acc_all)-1,1))\n",
    "    for i in range(1,len(acc_all)-1):\n",
    "        d1 = datetime(int(all_files[i][:4]),int(all_files[i][4:6]),int(all_files[i][6:8]),int(all_files[i][9:11]),int(all_files[i][11:13]),int(all_files[i][13:]))\n",
    "        delta[i] = (d1 - d0).total_seconds()\n",
    "        \n",
    "    ax_ind = [0,0,0,1,1,2,2,3,3,3]\n",
    "    for i in [0,2,4,6,8,9]:#range(3):#acc_all.shape[-1]):\n",
    "        acc_temp = acc_all[1:-1,i]\n",
    "        if not np.isnan(acc_temp).all():\n",
    "            x = np.arange(len(acc_temp))\n",
    "            recal_i = (acc_temp < 0)\n",
    "            ax[ax_ind[i]].plot(np.abs(acc_temp),'-',color=nn_c[i,:])\n",
    "            ax[ax_ind[i]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[i],label=labels[i]+': ' + str(int(recal_all[i,0])),color=nn_c[i,:])\n",
    "            ax[ax_ind[i]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[i,:])\n",
    "\n",
    "for i in range(4):\n",
    "    ax[i].axhline(65, ls='--', color='grey')\n",
    "    ax[i].legend()\n",
    "    ax[i].set_ylim([0,100])\n",
    "ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = all_files[0]\n",
    "train_file2 = all_files[1]\n",
    "ft= 'feat'\n",
    "\n",
    "train_data, train_params = prd.load_caps_train(path + train_file + '/traindata.mat')\n",
    "train_data = train_data[:,:8,:]\n",
    "\n",
    "train_dof = np.unique(train_params[:,2])\n",
    "key = np.empty(train_dof.shape)\n",
    "for i in range(len(train_dof)):\n",
    "    key[i] = train_params[np.argmax(train_params[:,2] == train_dof[i]),0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2, train_params2 = prd.load_caps_train(path + train_file2 + '/traindata.mat')\n",
    "train_data2 = train_data2[:,:8,:]\n",
    "train_data = np.vstack((train_data,train_data2))\n",
    "train_params = np.vstack((train_params,train_params2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep initial training data\n",
    "ep = 30\n",
    "n_dof = len(train_dof)\n",
    "\n",
    "# train_data, train_params = prd.threshold(train_data, train_params)\n",
    "trainmlp_0, traincnn_0, y_train_0, x_train_mlp_0, x_train_cnn_0, x_train_lda_0, y_train_lda_0, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b = False, batch_size=32,ft=ft, noise=True, split=False)\n",
    "y_test_0, x_test_mlp_0, x_test_cnn_0, x_lda_0, y_lda_0 = prd.prep_test_caps(train_data, train_params, scaler, emg_scale, num_classes=len(train_dof),ft=ft, split=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial training\n",
    "mlp, cnn, w, c = lp.train_models(traincnn_0, trainmlp_0, x_train_lda_0, y_train_lda_0, n_dof, ep=3)\n",
    "mlp_0 = mlp.get_weights()\n",
    "cnn_0 = cnn.get_weights()\n",
    "w_0 = cp.deepcopy(w)\n",
    "c_0 = cp.deepcopy(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA recalibration threshold\n",
    "acc = np.empty((len(all_files),5))\n",
    "\n",
    "for i in range(0,len(all_files)-1):\n",
    "    # load data\n",
    "    train_file = all_files[i]\n",
    "    train_data, train_params = prd.load_caps_train(path + train_file + '/traindata.mat')\n",
    "    train_data = train_data[:,:8,:]\n",
    "    # train_data, train_params = prd.threshold(train_data, train_params)\n",
    "\n",
    "    train_dof = np.unique(train_params[:,2])\n",
    "    key = np.empty(train_dof.shape)\n",
    "    for ki in range(len(train_dof)):\n",
    "        key[ki] = train_params[np.argmax(train_params[:,2] == train_dof[ki]),0]\n",
    "\n",
    "    _, _, _, _, _, x_train_lda_0, y_train_lda_0, _, _, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=32, ft=ft, noise=False, split=False, emg_scale=np.ones((train_data.shape[1],1)))\n",
    "\n",
    "    _, _, w, c = lp.train_models(x_train_lda=x_train_lda_0, y_train_lda=y_train_lda_0)\n",
    "\n",
    "    # load data\n",
    "    test_file = all_files[i+1]\n",
    "    test_data, test_params = prd.load_caps_train(path + test_file + '/traindata.mat')\n",
    "    test_data = test_data[:,:8,:]\n",
    "\n",
    "    # test_data, test_params = prd.threshold(test_data, test_params)\n",
    "    \n",
    "    # check class labels\n",
    "    test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key)\n",
    "\n",
    "    # test \n",
    "    y_test, x_test_mlp, x_test_cnn, x_lda, y_lda = prd.prep_test_caps(test_data, test_params, ft=ft, split=False)\n",
    "    acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, lda = [w, c])\n",
    "    print ('Set: ' + test_file, f'LDA Accuracy: {acc[i,0]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test on current dataset\n",
    "acc = np.empty((len(all_files),5))\n",
    "# old_train_mlp, old_train_cnn, old_x_train_lda, old_y_train_lda, old_y_train = [], [], [], [], []\n",
    "\n",
    "for i in range(0,len(all_files)):\n",
    "    train_file = all_files[i]\n",
    "    train_data, train_params = prd.load_caps_train(path + train_file + '/traindata.mat')\n",
    "    train_data = train_data[:,:8,:]\n",
    "\n",
    "    orig_train_dof = np.unique(train_params[:,2])\n",
    "    train_data, train_params = prd.threshold(train_data, train_params)\n",
    "    train_dof = np.unique(train_params[:,2])\n",
    "\n",
    "    if len(orig_train_dof) == len(train_dof):\n",
    "        n_dof = len(train_dof)\n",
    "        key = np.empty(train_dof.shape)\n",
    "        for key_i in range(len(train_dof)):\n",
    "            key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "\n",
    "        trainmlp_0, traincnn_0, y_train_0, x_train_mlp_0, x_train_cnn_0, x_train_lda_0, y_train_lda_0, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b = False, batch_size=32, ft=ft, noise=False, split=True)\n",
    "\n",
    "        if i > 0:\n",
    "            x_train_mlp_0, x_train_cnn_0, x_train_lda_0, y_train_lda_0, y_train_0 = np.vstack((old_train_mlp, x_train_mlp_0)), np.vstack((old_train_cnn, x_train_cnn_0)), np.vstack((old_x_train_lda, x_train_lda_0)), np.vstack((old_y_train_lda, y_train_lda_0)), np.vstack((old_y_train, y_train_0))\n",
    "\n",
    "            trainmlp_0 = tf.data.Dataset.from_tensor_slices((x_train_mlp_0, y_train_0, y_train_0)).shuffle(x_train_mlp_0.shape[0],reshuffle_each_iteration=True).batch(32)\n",
    "            traincnn_0 = tf.data.Dataset.from_tensor_slices((x_train_cnn_0, y_train_0, y_train_0)).shuffle(x_train_cnn_0.shape[0],reshuffle_each_iteration=True).batch(32)\n",
    "\n",
    "        mlp, cnn, w, c = lp.train_models(traincnn_0, trainmlp_0, x_train_lda_0, y_train_lda_0, n_dof, ep=30)\n",
    "\n",
    "        # test \n",
    "        y_test, x_test_mlp, x_test_cnn, x_lda, y_lda = prd.prep_test_caps(train_data, train_params, scaler, emg_scale, num_classes=len(train_dof),ft=ft, split=True)\n",
    "        acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, cnn, mlp, [w, c])\n",
    "        print ('Set: ' + train_file, f'CNN Accuracy: {acc[i,2]:.2f},', f'MLP Accuracy: {acc[i,1]:.2f},', f'LDA Accuracy: {acc[i,0]:.2f}')\n",
    "        old_train_mlp, old_train_cnn, old_x_train_lda, old_y_train_lda, old_y_train = x_train_mlp_0, x_train_cnn_0, x_train_lda_0, y_train_lda_0, y_train_0\n",
    "    else:\n",
    "        print('Skipping ' + train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on initial, test without recalibration or adaptation\n",
    "acc = np.empty((len(all_files),5))\n",
    "\n",
    "for i in range(0,len(all_files)):\n",
    "    # load data\n",
    "    test_file = all_files[i]\n",
    "    test_data, test_params = prd.load_caps_train(path + test_file + '/traindata.mat')\n",
    "    test_data = test_data[:,:8,:]\n",
    "\n",
    "    # test_data, test_params = prd.threshold(test_data, test_params)\n",
    "    \n",
    "    # check class labels\n",
    "    test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key)\n",
    "\n",
    "    # test \n",
    "    y_test, x_test_mlp, x_test_cnn, x_lda, y_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=len(train_dof),ft=ft, split=False)\n",
    "    acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, cnn, mlp, [w, c])\n",
    "    print ('Set: ' + test_file, f'CNN Accuracy: {acc[i,2]:.2f},', f'MLP Accuracy: {acc[i,1]:.2f},', f'LDA Accuracy: {acc[i,0]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewc_acc = np.empty((len(all_files),5))\n",
    "ewc_acc[:] = np.nan\n",
    "rec_acc = np.empty((len(all_files),5))\n",
    "rec_acc[:] = np.nan\n",
    "\n",
    "n_dof = len(train_dof)\n",
    "\n",
    "cnn.set_weights(cnn_0)\n",
    "mlp.set_weights(mlp_0)\n",
    "\n",
    "ewc_cnn = dl.EWC(mod='CNN')\n",
    "ewc_cnn(x_train_cnn_0[:1,...])\n",
    "loss, fish_loss = lp.train_task(ewc_cnn, ep, 1, x_train_cnn_0,y_train_0, [x_test_cnn_0],[y_test_0], lams=[0])\n",
    "x_test_ewc_cnn_0 = cp.deepcopy(x_test_cnn_0)\n",
    "y_test_ewc_0 = cp.deepcopy(y_test_0)\n",
    "x_val_ewc_cnn = cp.deepcopy(x_test_cnn_0)\n",
    "y_val_ewc = cp.deepcopy(y_test_0)\n",
    "\n",
    "ewc = dl.EWC()\n",
    "ewc(x_train_mlp_0[:1,...])\n",
    "loss, fish_loss = lp.train_task(ewc, ep, 1, x_train_mlp_0,y_train_0, [x_test_mlp_0],[y_test_0], lams=[0])\n",
    "x_test_ewc_0 = cp.deepcopy(x_test_mlp_0)\n",
    "x_val_ewc = cp.deepcopy(x_test_mlp_0)\n",
    "\n",
    "ewc_acc[0,:] = lp.test_models(x_test_cnn_0, x_test_mlp_0, x_lda_0, y_test_0, y_lda_0, cnn, mlp, [w, c], ewc, ewc_cnn)\n",
    "print(ewc_acc[0,:])\n",
    "\n",
    "for i in range(1,len(all_files)-1,1):\n",
    "    # load recalibration data\n",
    "    ewc_file = all_files[i]\n",
    "    ewc_data, ewc_params = prd.load_caps_train(path + ewc_file + '/traindata.mat')\n",
    "    ewc_data = ewc_data[:,:8,:]\n",
    "\n",
    "    ewc_cnn.compute_fisher(x_val_ewc_cnn, y_val_ewc, num_samples=200, plot_diffs=False) # use validation set for Fisher computation\n",
    "    ewc_cnn.star()\n",
    "\n",
    "    ewc.compute_fisher(x_val_ewc, y_val_ewc, num_samples=200, plot_diffs=False) # use validation set for Fisher computation\n",
    "    ewc.star()\n",
    "\n",
    "    # check class labels\n",
    "    orig_train_dof = np.unique(ewc_params[:,2])\n",
    "    ewc_data, ewc_params = prd.threshold(ewc_data, ewc_params)\n",
    "    train_dof = np.unique(ewc_params[:,2])\n",
    "\n",
    "    if len(orig_train_dof) == len(train_dof):\n",
    "        # n_dof = len(train_dof)\n",
    "        \n",
    "        # training data\n",
    "        ewcmlp, ewccnn, y_train_ewc, x_train_ewc, x_train_ewc_cnn, x_train_lda, y_train_lda, _, _, _, _, _ = prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, num_classes=n_dof, batch_size=32, noise=True, scaler=scaler, emg_scale=emg_scale,ft=ft,split=True)\n",
    "        # training data for recalibration\n",
    "        rmlp, rcnn, y_train_r, x_train_r, x_train_r_cnn, x_train_r_lda, y_train_r_lda, r_emg_scale, r_scaler, _, _, _ = prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, batch_size=32, noise=True, ft=ft, split=True)\n",
    "        # validation data for fisher\n",
    "        _, _, y_val_ewc, x_val_ewc, x_val_ewc_cnn, _, _, _, _, _, _, _ = prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, num_classes=n_dof, batch_size=32, noise=False, scaler=scaler, emg_scale=emg_scale,ft=ft,split=True)\n",
    "        # test data\n",
    "        y_test_ewc, x_test_ewc, x_test_ewc_cnn, x_lda, y_lda = prd.prep_test_caps(ewc_data, ewc_params, scaler, emg_scale, num_classes=n_dof,ft=ft,split=True)\n",
    "        # test data for recalibration\n",
    "        y_test_r, x_test_r, x_test_r_cnn, x_r_lda, y_r_lda = prd.prep_test_caps(ewc_data, ewc_params, r_scaler, r_emg_scale, ft=ft,split=True)\n",
    "\n",
    "        loss, fish_loss = lp.train_task(ewc_cnn, 30, 1, x_train_ewc_cnn, y_train_ewc, [x_test_ewc_cnn_0, x_test_ewc_cnn], [y_test_ewc_0, y_test_ewc], lams=[15])#,15,50])\n",
    "        loss, fish_loss = lp.train_task(ewc, 30, 1, x_train_ewc, y_train_ewc, [x_test_ewc_0, x_test_ewc], [y_test_ewc_0, y_test_ewc], lams=[15])#,15,50])\n",
    "        mlp, cnn, _, _ = lp.train_models(ewccnn, ewcmlp, n_dof=n_dof, ep=5, mlp=mlp, cnn=cnn)\n",
    "\n",
    "        mlp_r, cnn_r, w, c = lp.train_models(rcnn, rmlp, x_train_r_lda, y_train_r_lda, n_dof=n_dof, ep=30)\n",
    "\n",
    "        # test\n",
    "        ewc_acc[i,:] = lp.test_models(x_test_ewc_cnn, x_test_ewc, x_lda, y_test_ewc, y_lda, cnn, mlp, ewc=ewc, ewc_cnn=ewc_cnn)\n",
    "        rec_acc[i,:] = lp.test_models(x_test_r_cnn, x_test_r, x_r_lda, y_test_r, y_r_lda, cnn_r, mlp_r, lda=[w,c])\n",
    "\n",
    "        x_test_ewc_cnn_0 = cp.deepcopy(x_test_ewc_cnn)\n",
    "        x_test_ewc_0 = cp.deepcopy(x_test_ewc)\n",
    "        y_test_ewc_0 = cp.deepcopy(y_test_ewc)\n",
    "\n",
    "        print ('EWC: ' + ewc_file + ',', f'EWC CNN Accuracy: {ewc_acc[i,4]:.2f},', f'EWC Accuracy: {ewc_acc[i,3]:.2f},', f'a-CNN Accuracy: {ewc_acc[i,2]:.2f},', f'a-MLP Accuracy: {ewc_acc[i,1]:.2f},',  f'r-CNN Accuracy: {rec_acc[i,2]:.2f},', f'r-MLP Accuracy: {rec_acc[i,1]:.2f},', f'r-LDA Accuracy: {rec_acc[i,0]:.2f}')\n",
    "    else:\n",
    "        print('Skipping ' + ewc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all controllers with ewc\n",
    "ep = 30\n",
    "# Remove extra file if odd number of files\n",
    "if len(all_files)%2 == 0:\n",
    "    all_files = np.delete(all_files,-1)\n",
    "ewc_acc = np.empty((len(all_files),5))\n",
    "ewc_acc[:] = np.nan\n",
    "rec_acc = np.empty((len(all_files),5))\n",
    "rec_acc[:] = np.nan\n",
    "\n",
    "n_dof = len(train_dof)\n",
    "\n",
    "cnn.set_weights(cnn_0)\n",
    "mlp.set_weights(mlp_0)\n",
    "\n",
    "ewc_cnn = dl.EWC(mod='CNN')\n",
    "ewc_cnn(x_train_cnn_0[:1,...])\n",
    "loss, fish_loss = lp.train_task(ewc_cnn, ep, 1, x_train_cnn_0, y_train_0, [x_test_cnn_0],[y_test_0], lams=[0])\n",
    "x_test_ewc_cnn_0 = cp.deepcopy(x_test_cnn_0)\n",
    "y_test_ewc_0 = cp.deepcopy(y_test_0)\n",
    "x_val_ewc_cnn = cp.deepcopy(x_test_cnn_0)\n",
    "y_val_ewc = cp.deepcopy(y_test_0)\n",
    "\n",
    "ewc = dl.EWC()\n",
    "ewc(x_train_mlp_0[:1,...])\n",
    "loss, fish_loss = lp.train_task(ewc, ep, 1, x_train_mlp_0, y_train_0, [x_test_mlp_0],[y_test_0], lams=[0])\n",
    "x_test_ewc_0 = cp.deepcopy(x_test_mlp_0)\n",
    "x_val_ewc = cp.deepcopy(x_test_mlp_0)\n",
    "\n",
    "ewc_acc[0,:] = lp.test_models(x_test_cnn_0, x_test_mlp_0, x_lda_0, y_test_0, y_lda_0, cnn, mlp, [w_0, c_0], ewc, ewc_cnn)\n",
    "\n",
    "for i in range(7,len(all_files)-1,2):\n",
    "    # load recalibration data\n",
    "    ewc_file = all_files[i]\n",
    "    ewc_data, ewc_params = prd.load_caps_train(path + ewc_file + '/traindata.mat')\n",
    "    ewc_data = ewc_data[:,:8,:]\n",
    "\n",
    "    ewc_cnn.compute_fisher(x_val_ewc_cnn, y_val_ewc, num_samples=200, plot_diffs=False) # use validation set for Fisher computation\n",
    "    ewc_cnn.star()\n",
    "\n",
    "    ewc.compute_fisher(x_val_ewc, y_val_ewc, num_samples=200, plot_diffs=False) # use validation set for Fisher computation\n",
    "    ewc.star()\n",
    "\n",
    "    orig_train_dof = np.unique(ewc_params[:,2])\n",
    "    ewc_data, ewc_params = prd.threshold(ewc_data, ewc_params)\n",
    "    train_dof = np.unique(ewc_params[:,2])\n",
    "\n",
    "    if len(orig_train_dof) == len(train_dof):\n",
    "\n",
    "        # check class labels\n",
    "        ewc_data, ewc_params = lp.check_labels(ewc_data, ewc_params, train_dof, key)\n",
    "        \n",
    "        ewcmlp, ewccnn, y_train_ewc, x_train_ewc, x_train_ewc_cnn, x_train_lda, y_train_lda, _, _, _, _, _ = prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, num_classes=n_dof, batch_size=64, noise=True, scaler=scaler, emg_scale=emg_scale,ft=ft, split=True)\n",
    "        _, _, y_val_ewc, x_val_ewc, x_val_ewc_cnn, _, _, _, _, _, _, _= prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, num_classes=n_dof, batch_size=64, noise=False, scaler=scaler, emg_scale=emg_scale,ft=ft, split=True)\n",
    "\n",
    "        y_test_ewc, x_test_ewc, x_test_ewc_cnn, x_lda, y_lda = prd.prep_test_caps(ewc_data, ewc_params, scaler, emg_scale, num_classes=n_dof,ft=ft, split=True)\n",
    "\n",
    "        loss, fish_loss = lp.train_task(ewc_cnn, 30, 1, x_train_ewc_cnn, y_train_ewc, [x_test_ewc_cnn_0, x_test_ewc_cnn], [y_test_ewc_0, y_test_ewc], lams=[15])#,15,50])\n",
    "        loss, fish_loss = lp.train_task(ewc, 30, 1, x_train_ewc, y_train_ewc, [x_test_ewc_0, x_test_ewc], [y_test_ewc_0, y_test_ewc], lams=[15])#,15,50])\n",
    "        mlp, cnn, _, _ = lp.train_models(ewccnn, ewcmlp, x_train_lda, y_train_lda, n_dof, 30, mlp, cnn)\n",
    "        \n",
    "        # training data for recalibration\n",
    "        rmlp, rcnn, y_train_r, x_train_r, x_train_r_cnn, x_train_r_lda, y_train_r_lda, r_emg_scale, r_scaler, _, _, _ = prd.prep_train_caps(ewc_data, ewc_params, prop_b = False, batch_size=32, noise=True, ft=ft, split=True)\n",
    "        y_test_r, x_test_r, x_test_r_cnn, x_r_lda, y_r_lda = prd.prep_test_caps(ewc_data, ewc_params, r_scaler, r_emg_scale, ft=ft, split=True)\n",
    "\n",
    "        mlp_r, cnn_r, w, c = lp.train_models(rcnn, rmlp, x_train_r_lda, y_train_r_lda, ep=30)\n",
    "\n",
    "        # load test data\n",
    "        test_file = all_files[i+1]\n",
    "        test_data, test_params = prd.load_caps_train(path + test_file + '/traindata.mat')\n",
    "        test_data = test_data[:,:8,:]\n",
    "        \n",
    "        # check class labels\n",
    "        test_data, test_params = prd.threshold(test_data, test_params)\n",
    "        test_data, test_params = lp.check_labels(test_data, test_params, train_dof, key)\n",
    "\n",
    "        ewc_acc[i,:] = lp.test_models(x_test_ewc_cnn, x_test_ewc, x_lda, y_test_ewc, y_lda, cnn, mlp, ewc=ewc, ewc_cnn=ewc_cnn)\n",
    "        rec_acc[i,:] = lp.test_models(x_test_r_cnn, x_test_r, x_r_lda, y_test_r, y_r_lda, cnn_r, mlp_r, lda=[w,c])\n",
    "\n",
    "        # test \n",
    "        y_test, x_test_mlp, x_test_cnn, x_lda, y_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft)\n",
    "        y_test_r, x_test_r, x_test_r_cnn, x_r_lda, y_r_lda = prd.prep_test_caps(test_data, test_params, r_scaler, r_emg_scale, num_classes=n_dof,ft=ft)\n",
    "        ewc_acc[i+1,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, cnn, mlp, ewc=ewc, ewc_cnn=ewc_cnn)\n",
    "        rec_acc[i+1,:] = lp.test_models(x_test_r_cnn, x_test_r, x_r_lda, y_test_r, y_r_lda, cnn_r, mlp_r, lda=[w,c])\n",
    "\n",
    "        x_test_ewc_cnn_0 = cp.deepcopy(x_test_ewc_cnn)\n",
    "        x_test_ewc_0 = cp.deepcopy(x_test_ewc)\n",
    "        y_test_ewc_0 = cp.deepcopy(y_test_ewc)\n",
    "\n",
    "        print ('EWC: ' + ewc_file + ', Test: ' + test_file + ',', f'EWC CNN Accuracy: {ewc_acc[i+1,4]:.2f},', f'EWC Accuracy: {ewc_acc[i+1,3]:.2f},', f'a-CNN Accuracy: {ewc_acc[i+1,2]:.2f},', f'a-MLP Accuracy: {ewc_acc[i+1,1]:.2f},',  f'r-CNN Accuracy: {rec_acc[i+1,2]:.2f},', f'r-MLP Accuracy: {rec_acc[i+1,1]:.2f},', f'r-LDA Accuracy: {rec_acc[i+1,0]:.2f}')\n",
    "    else:\n",
    "        print('Skipping: ' + ewc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ewc_acc[0,...] = np.nan\n",
    "rec_acc[0,...] = np.nan\n",
    "rlda_acc = np.empty((len(all_files),15))\n",
    "rlda_acc[:] = np.nan\n",
    "lda_acc = np.empty((len(all_files),15))\n",
    "lda_acc[:] = np.nan\n",
    "acc_all = np.hstack((acc[:,...],ewc_acc,rec_acc))\n",
    "for i in range(acc.shape[1]):\n",
    "    rlda_acc[:,i] = acc[:,i] - ewc_acc[:,0]\n",
    "    rlda_acc[:,i+5] = ewc_acc[:,i] - ewc_acc[:,0]\n",
    "    rlda_acc[:,i+10] = rec_acc[:,i] - ewc_acc[:,0]\n",
    "\n",
    "    lda_acc[:,i] = acc[:,i] - acc[:,0]\n",
    "    lda_acc[:,i+5] = ewc_acc[:,i] - acc[:,0]\n",
    "    lda_acc[:,i+10] = rec_acc[:,i] - acc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_i = [0, 5]\n",
    "cnn_i = [2, 12, 7, 9]\n",
    "mlp_i = [1, 11, 6, 8]\n",
    "nn_i = np.vstack((mlp_i,cnn_i))\n",
    "\n",
    "colors =  cm.get_cmap('tab20c')\n",
    "c = np.empty((20,4))\n",
    "for i in range(20):\n",
    "    c[i,:] = colors(i*1/20)\n",
    "\n",
    "mlp_c = np.vstack((c[8,:],c[9,:],c[10,:],c[11,:]))\n",
    "cnn_c = np.vstack((c[0,:],c[1,:],c[2,:],c[3,:]))\n",
    "nn_c = np.stack((mlp_c,cnn_c))\n",
    "lda_c = ['ko-','ko--']\n",
    "nn_m = ['o-','s-','v-','x-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~np.isnan(rlda_acc)\n",
    "\n",
    "for mod in range(nn_i.shape[0]):\n",
    "    i_ = nn_i[mod,...]\n",
    "    fig,ax = plt.subplots(1,3,figsize=(16,4))\n",
    "\n",
    "    # for i in range(len(lda_i)):\n",
    "        # ax[0].plot(acc_all[mask[:,lda_i[i]],lda_i[i]][1::2], lda_c[i])\n",
    "    for i in range(len(i_)):\n",
    "        ax[0].plot(acc_all[mask[:,i_[i]],i_[i]][1::2], nn_m[i], color=nn_c[mod,i,:])\n",
    "        ax[1].plot(rlda_acc[mask[:,i_[i]],i_[i]][1::2], nn_m[i], color=nn_c[mod,i,:])\n",
    "        ax[2].plot(lda_acc[mask[:,i_[i]],i_[i]][1::2], nn_m[i], color=nn_c[mod,i,:])\n",
    "\n",
    "    if mod == 0:\n",
    "        ax[0].legend(['LDA', 'r-LDA', 'MLP', 'r-MLP', 'a-MLP', 'ewc-MLP'])\n",
    "        ax[1].legend(['MLP vs. r-LDA', 'r-MLP vs. r-LDA', 'a-MLP vs. r-LDA', 'ewc-MLP vs. r-LDA'])\n",
    "        ax[2].legend(['MLP vs. LDA', 'r-MLP vs. LDA', 'a-MLP vs. LDA', 'ewc-MLP vs. LDA'])\n",
    "    else:\n",
    "        ax[0].legend(['LDA', 'r-LDA', 'CNN', 'r-CNN', 'a-CNN', 'ewc-CNN'])\n",
    "        ax[1].legend(['CNN vs. r-LDA', 'r-CNN vs. r-LDA', 'a-CNN vs. r-LDA', 'ewc-CNN vs. r-LDA'])\n",
    "        ax[2].legend(['CNN vs. LDA', 'r-CNN vs. LDA', 'a-CNN vs. LDA', 'ewc-CNN vs. LDA'])\n",
    "\n",
    "    ax[0].set_ylim([0,100])\n",
    "    ax[1].axhline(0, ls = '--',color='black')\n",
    "    ax[2].axhline(0, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,4))\n",
    "x = range(0,acc.shape[0]-2,1)\n",
    "x_skip = range(0,acc.shape[0]-2,2)\n",
    "\n",
    "for i in range(3):\n",
    "    ax[0].plot(x_skip,acc[2::2,i],'o', label='_nolegend_')\n",
    "    ax[0].plot(x,acc[2:,i],'-')\n",
    "\n",
    "    if i == 0:\n",
    "        ls = 's--'\n",
    "    else:\n",
    "        ls = 's-'\n",
    "    ax[1].plot(x_skip,recal_acc[mask,i], ls)\n",
    "\n",
    "    if i > 0:\n",
    "        ax[2].plot(x_skip,adapt_acc[mask,i], 'v-')\n",
    "\n",
    "    ax[i].set_ylim([0,100])\n",
    "\n",
    "ax[0].legend(['LDA','MLP','CNN'])\n",
    "ax[1].legend(['r-LDA','r-MLP','r-CNN'])\n",
    "ax[2].legend(['a-MLP','a-CNN'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(16,4))\n",
    "x = range(0,acc.shape[0]-2,1)\n",
    "x_skip = range(0,acc.shape[0]-2,2)\n",
    "\n",
    "for i in range(3):\n",
    "    ax[0].plot(x_skip,acc[2::2,i],'o',color=col[i,:], label='_nolegend_')\n",
    "    ax[0].plot(x,acc[2:,i],'-',color=col[i,:])\n",
    "\n",
    "    if i == 0:\n",
    "        ls = 's--'\n",
    "    else:\n",
    "        ls = 's-'\n",
    "    ax[1].plot(x_skip,recal_acc[mask,i], ls,color=col[i,:])\n",
    "\n",
    "    if i > 0:\n",
    "        ax[2].plot(x_skip,adapt_acc[mask,i], 'v-',color=col[i,:])\n",
    "\n",
    "    ax[i].set_ylim([0,100])\n",
    "\n",
    "ax[0].legend(['LDA','MLP','CNN'])\n",
    "ax[1].legend(['r-LDA','r-MLP','r-CNN'])\n",
    "ax[2].legend(['a-MLP','a-CNN'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all controllers with adaptation\n",
    "ep = 30\n",
    "# Remove extra file if odd number of files\n",
    "# if len(all_files)%2 == 0:\n",
    "#     all_files = np.delete(all_files,-1)\n",
    "align_acc = np.empty((len(all_files),5))\n",
    "align_acc[:] = np.nan\n",
    "n_dof = len(train_dof)\n",
    "\n",
    "for i in range(2,len(all_files)-1):\n",
    "    # load recalibration data\n",
    "    align_file = all_files[i]\n",
    "    align_data, align_params = prd.load_caps_train(path + align_file + '/traindata.mat')\n",
    "    align_data = align_data[:,:8,:]\n",
    "\n",
    "    # check class labels\n",
    "    align_data, align_params = prd.threshold(align_data, align_params)\n",
    "    align_data, align_params = lp.check_labels(align_data, align_params, train_dof, key)\n",
    "    \n",
    "    alignmlp, aligncnn, _, _, _, x_train_lda, y_train_lda, _, _, _, _, _ = prd.prep_train_caps(align_data, align_params, prop_b = False, num_classes=n_dof, batch_size=32, noise=False, scaler=scaler, emg_scale=emg_scale,ft=ft,split=True)\n",
    "    mlp, cnn, mlp_ali, cnn_ali, w, c = lp.train_models(aligncnn, alignmlp, x_train_lda, y_train_lda, n_dof, ep, mlp, cnn, align=True)\n",
    "\n",
    "    # test \n",
    "    y_test, x_test_mlp, x_test_cnn, x_lda, y_lda = prd.prep_test_caps(align_data, align_params, scaler, emg_scale, num_classes=n_dof,ft=ft, split=True)\n",
    "    align_acc[i,:] = lp.test_models(x_test_cnn, x_test_mlp, x_lda, y_test, y_lda, cnn, mlp, [w, c], cnn_align=cnn_ali, mlp_align=mlp_ali)\n",
    "\n",
    "    print ('Align: ' + align_file + ',', f'CNN Accuracy: {align_acc[i,2]:.2f},', f'MLP Accuracy: {align_acc[i,1]:.2f},', f'LDA Accuracy: {align_acc[i,0]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e4d54467b05e62951c9fd7929782b99429e3b62c1a3b146d4f3dbf79f907e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('adapt_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
