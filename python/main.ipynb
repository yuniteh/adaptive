{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpu import set_gpu\n",
    "import numpy as np\n",
    "import os\n",
    "import adapt.utils.data_utils as prd\n",
    "import adapt.loop as lp\n",
    "import adapt.ml.lda as dlda\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import adapt.ml.dl_subclass as dl\n",
    "import copy as cp\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython import display\n",
    "import gc as gc\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR62\n",
      "train dof: [ 1  6 16 19 48 90], key: [0 1 2 3 4 5]\n",
      "setting CNN weights\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180524_161811, Accuracy: 99.74 , Val: 99.32 , Prev: 0.00 , Train: 99.12\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180525_084201, Accuracy: 82.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180531_073149, Accuracy: 89.48 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180602_105936, Accuracy: 80.53 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180604_090437, Accuracy: 74.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20180604_090437\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.0336380004882812\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180604_214053, Accuracy: 81.94 , Val: 80.75 , Prev: 81.08 , Train: 100.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180605_080547, Accuracy: 80.64 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180606_174322, Accuracy: 63.35 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20180606_174322\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.509254217147827\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180606_174322, Test: 20180610_081839, Accuracy: 72.20 , Val: 86.78 , Prev: 74.09 , Train: 100.00\n",
      "recal: 3 20180610_081839\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.392927408218384\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180612_085047, Accuracy: 79.80 , Val: 87.51 , Prev: 84.60 , Train: 100.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180614_081624, Accuracy: 79.18 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180616_182930, Accuracy: 70.69 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 4 20180616_182930\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.5511889457702637\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180616_182930, Test: 20180617_184248, Accuracy: 61.48 , Val: 91.05 , Prev: 78.04 , Train: 99.38\n",
      "recal: 5 20180617_184248\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.6984853744506836\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180619_193541, Accuracy: 80.95 , Val: 53.22 , Prev: 82.62 , Train: 99.37\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073139, Accuracy: 79.23 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073838, Accuracy: 72.88 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 6 20180620_073838\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.408903121948242\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_073838, Test: 20180620_083903, Accuracy: 68.30 , Val: 85.64 , Prev: 71.38 , Train: 100.00\n",
      "recal: 7 20180620_083903\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.317410707473755\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180621_094013, Accuracy: 82.98 , Val: 86.58 , Prev: 88.14 , Train: 98.02\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180623_200107, Accuracy: 68.30 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 8 20180623_200107\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.0195441246032715\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180703_072425, Accuracy: 80.43 , Val: 79.40 , Prev: 75.96 , Train: 99.06\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180711_074144, Accuracy: 77.04 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180711_113445, Accuracy: 57.83 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 9 20180711_113445\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.0014891624450684\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180713_102029, Accuracy: 77.77 , Val: 84.50 , Prev: 65.04 , Train: 99.17\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180717_073851, Accuracy: 70.64 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 10 20180717_073851\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.046198844909668\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180717_073851, Test: 20180717_112511, Accuracy: 85.11 , Val: 86.49 , Prev: 43.39 , Train: 99.48\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180717_073851, Test: 20190321_073718, Accuracy: 46.16 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 11 20190321_073718\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.550952196121216\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_073718, Test: 20190321_095657, Accuracy: 68.10 , Val: 77.37 , Prev: 74.74 , Train: 98.97\n",
      "recal: 12 20190321_095657\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.165386438369751\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_095657, Test: 20190325_084612, Accuracy: 57.06 , Val: 71.14 , Prev: 74.58 , Train: 99.63\n",
      "recal: 13 20190325_084612\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.178786277770996\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190325_084612, Test: 20190326_073702, Accuracy: 45.89 , Val: 67.97 , Prev: 45.62 , Train: 99.81\n",
      "recal: 14 20190326_073702\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.3005895614624023\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190328_125619, Accuracy: 71.20 , Val: 82.34 , Prev: 83.52 , Train: 98.70\n",
      "recal: 15 20190328_125619\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.719963788986206\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190328_125619, Test: 20190331_112350, Accuracy: 81.65 , Val: 77.81 , Prev: 76.67 , Train: 99.72\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190328_125619, Test: 20190402_073436, Accuracy: 72.43 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 16 20190402_073436\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.155453681945801\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_073436, Test: 20190402_142249, Accuracy: 66.23 , Val: 79.24 , Prev: 58.68 , Train: 99.72\n",
      "recal: 17 20190402_142249\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2414095401763916\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190403_050414, Accuracy: 75.08 , Val: 82.96 , Prev: 73.65 , Train: 98.60\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190403_052115, Accuracy: 85.42 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190404_075938, Accuracy: 70.70 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 18 20190404_075938\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.1391475200653076\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190404_085916, Accuracy: 75.41 , Val: 76.16 , Prev: 77.56 , Train: 99.91\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190406_091945, Accuracy: 67.76 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 19 20190406_091945\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.536764144897461\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190410_102236, Accuracy: 80.30 , Val: 84.23 , Prev: 76.44 , Train: 99.19\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190410_102641, Accuracy: 80.76 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190415_073441, Accuracy: 87.24 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190415_102702, Accuracy: 81.53 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190423_063337, Accuracy: 77.78 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190424_045927, Accuracy: 79.55 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190502_102725, Accuracy: 84.68 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190505_082637, Accuracy: 80.81 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190505_083227, Accuracy: 69.26 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 20 20190505_083227\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2206008434295654\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190505_083227, Test: 20190518_142954, Accuracy: 69.26 , Val: 74.67 , Prev: 81.80 , Train: 99.07\n",
      "------------------------acnn05 16 - 4 -- 0-------------\n"
     ]
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = True\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn2','acnn05','acnn30','acewc30','acewc15', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn', 'avcnn15', 'acnnl03','crvcnn','acewclm','crcnn','acewc00','xtra2']\n",
    "ft = 'tdar'\n",
    "iter = 10\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_mod = 0\n",
    "\n",
    "for it in range(1):#iter):\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    if it == 0:\n",
    "        mod_all = ['blda','lda','alda','cnn','acnn05','avcnn']#,'crcnn']\n",
    "        mod_all = ['lda','alda','acnn05','avcnn']\n",
    "        mod_all = ['acnn05']\n",
    "    else:\n",
    "        mod_all = ['bcnn','cnn','acnn05','avcnn']\n",
    "\n",
    "    for sub in range(4,5):\n",
    "        print(subs[sub])\n",
    "        sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "        all_files = os.listdir(sub_path)\n",
    "        if 'skip' in all_files:\n",
    "            all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "        # load or initialize cnn weights\n",
    "        if load_mod:\n",
    "            with open(subs[sub] + '_' + str(it) + '_r_accs_y.p','rb') as f:\n",
    "                all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof,_, _, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        else:\n",
    "            c_weights = None\n",
    "            v_weights = None\n",
    "            v_wc = None\n",
    "            cl_wc = None\n",
    "            scaler_0 = None\n",
    "            all_recal = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "            rows, cols = (len(all_files), len(mod_tot))\n",
    "            all_dof = [[0]*cols]*rows\n",
    "\n",
    "        mod_i = 0\n",
    "        for mod in mod_all:\n",
    "            acc = np.zeros((len(all_files),2))\n",
    "            acc_val = np.zeros((len(all_files),2))\n",
    "            acc_prev = np.zeros((len(all_files),2))\n",
    "            acc_train = np.zeros((len(all_files),2))\n",
    "            mod_recal = np.zeros((len(all_files),))\n",
    "\n",
    "            if 'lda' in mod:\n",
    "                acc_i = 0\n",
    "            else:\n",
    "                acc_i = 1\n",
    "\n",
    "            cnn = None\n",
    "            ewc = None\n",
    "            clda = None\n",
    "\n",
    "            ep = 30\n",
    "            recal = 0\n",
    "            skip_recal = 0\n",
    "            skip = False\n",
    "\n",
    "            # Loop through files\n",
    "            for i in range(0,len(all_files)-1):\n",
    "                # Check if need to recalibrate\n",
    "                if i > 0:\n",
    "                    if acc[i,acc_i] < 75:\n",
    "                        skip = False\n",
    "                        recal += 1\n",
    "                        print('recal: ' + str(recal) + ' ' + all_files[i])\n",
    "                        acc[i,acc_i] *= -1\n",
    "                        mod_recal[i] = 1\n",
    "                    else:\n",
    "                        skip = True\n",
    "                    \n",
    "                    if 'b' in mod:\n",
    "                        skip = True\n",
    "                        \n",
    "                if not skip:\n",
    "                    # load training file\n",
    "                    train_file = all_files[i]\n",
    "                    train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "                    # if first train, use two train files\n",
    "                    if i == 0:\n",
    "                        # load training file\n",
    "                        train_file2 = all_files[i+1]\n",
    "                        train_data2, train_params2 = prd.load_caps_train(sub_path + train_file2 + '/traindata.mat')\n",
    "                        train_data = np.vstack((train_data,train_data2))\n",
    "                        train_params = np.vstack((train_params,train_params2))\n",
    "                        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "                        val_data, val_params = train_data, train_params\n",
    "                    else:\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                    \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "                        \n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "                        del train_temp, params_temp, tr_i, te_i\n",
    "                        \n",
    "                    # if combining, save current training data\n",
    "                    # if 'cr' in mod:\n",
    "                    #     # combine old and new training data\n",
    "                    #     if i > 1:\n",
    "                    #         train_data = np.vstack((train_data_0,train_data))\n",
    "                    #         train_params = np.vstack((train_params_0,train_params))\n",
    "\n",
    "                    #     train_data_0 = cp.deepcopy(train_data)\n",
    "                    #     train_params_0 = cp.deepcopy(train_params)\n",
    "\n",
    "                    # if (i == 1 and mod[0] == 'a') or (i == 1 and mod[:2] == 'cr') or (mod[0] != 'a' and mod[:2] != 'cr'):\n",
    "                        \n",
    "                    if i > 0:\n",
    "                        # get previous dofs\n",
    "                        prev_ndof = [n_dof, key, train_dof]\n",
    "                        if mod[0] == 'a':\n",
    "                            print('prev: ' + str(train_dof))\n",
    "                            # get current dofs and create key\n",
    "                            train_dof = np.unique(train_params[:,-1])\n",
    "                            print('cur: ' + str(train_dof))\n",
    "                            key = np.zeros((len(train_dof),))\n",
    "                            # check if current dofs are all in old dof list\n",
    "                            dof_ovlp = np.isin(train_dof,prev_ndof[2],assume_unique=True)\n",
    "                            temp_dof = cp.deepcopy(train_dof)\n",
    "                            # loop through dofs that are in previous dofs, set the keys\n",
    "                            for dof in train_dof[dof_ovlp]:\n",
    "                                key[train_dof==dof] = prev_ndof[1][prev_ndof[2]==dof]\n",
    "                                temp_dof[train_dof==dof] = prev_ndof[2][prev_ndof[2]==dof]\n",
    "\n",
    "                            # check if previous dofs has classes not in this set\n",
    "                            dof_xtra = ~np.isin(prev_ndof[2],temp_dof,assume_unique=True)\n",
    "                            temp_dof = np.hstack((temp_dof,prev_ndof[2][dof_xtra]))\n",
    "                            key = np.hstack((key,prev_ndof[1][dof_xtra]))\n",
    "\n",
    "                            # loop through dofs that are not in previous dofs (ie new classes), add keys\n",
    "                            key_i = 1\n",
    "                            xtra = False\n",
    "                            for dof in train_dof[~dof_ovlp]:\n",
    "                                # if 'lda' in mod:\n",
    "                                # xtra = True\n",
    "                                # key[temp_dof==dof] = np.max(key) + 1\n",
    "                                    # key_i += 1\n",
    "                                # else:\n",
    "                                    # remove extras\n",
    "                                print('removing train ' + str(dof))\n",
    "                                ind = train_params[:,-1] == dof\n",
    "                                train_params = train_params[~ind,...]\n",
    "                                train_data = train_data[~ind,...]\n",
    "                                ind = val_params[:,-1] == dof\n",
    "                                val_params = val_params[~ind,...]\n",
    "                                val_data = val_data[~ind,...]\n",
    "                                key = np.delete(key,temp_dof==dof)\n",
    "                                temp_dof = np.delete(temp_dof,temp_dof==dof)\n",
    "\n",
    "                            train_dof = cp.deepcopy(temp_dof)\n",
    "                        else:\n",
    "                            # get current dofs and create key\n",
    "                            train_dof = np.unique(train_params[:,-1])\n",
    "                            key = np.arange(len(train_dof))\n",
    "                    else:\n",
    "                        # get current dofs and create key\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        key = np.arange(len(train_dof))\n",
    "\n",
    "                    n_dof = len(train_dof)\n",
    "                    all_dof[i][mod_tot.index(mod)] = train_dof\n",
    "\n",
    "                    train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key,False)\n",
    "                    val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key,False)\n",
    "\n",
    "                    print('train dof: ' + str(train_dof) + ', key: ' + str(key))\n",
    "\n",
    "                    if (mod[0] == 'a' and i > 0) or ('cr' in mod and i > 0) or (mod == 'vcnn' and i > 0):\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale, scaler=scaler, num_classes=n_dof, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False)\n",
    "                    else:\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "                        if ((i == 0) and (c_weights is not None)) or ((i == 0) and (v_weights is not None)):\n",
    "                            scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                    _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "                    if 'cr' in mod:\n",
    "                        # combine old and new training data\n",
    "                        if i > 0:\n",
    "                            x_clean_cnn = np.vstack((clean_data_0,x_clean_cnn))\n",
    "                            y_clean = np.vstack((clean_params_0,y_clean))\n",
    "                            x_train_cnn = np.vstack((x_clean_cnn,x_train_cnn))\n",
    "                            y_train = np.vstack((y_clean,y_train))\n",
    "\n",
    "                    del train_data, train_params, val_data, val_params\n",
    "\n",
    "                    if 'lda' not in mod:\n",
    "                        cnnlda = 'l' in mod\n",
    "                        if i == 0:\n",
    "                            if c_weights is None:\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda, bn_training=True, prog_train=True)\n",
    "                                c_weights = cp.deepcopy([cnn.enc.get_weights(),cnn.clf.get_weights()])\n",
    "                                scaler_0 = cp.deepcopy(scaler)    \n",
    "                            else:\n",
    "                                print('setting CNN weights')\n",
    "                                cnn = dl.CNN(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                                if cnnlda:\n",
    "                                    print('setting LDA weights')\n",
    "                                    w_c = cp.deepcopy(cl_wc[0].astype('float32'))\n",
    "                                    c_c = cp.deepcopy(cl_wc[1].astype('float32'))\n",
    "                            if 'ewc' in mod:\n",
    "                                cnn = dl.EWC(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                            if 'ad' in mod:\n",
    "                                cnn = dl.CNN(n_class=n_dof,adapt=True)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                            if 'av' in mod:\n",
    "                                mu_class, std_class, N = prd.set_mean(x_clean_cnn,y_clean)\n",
    "\n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                            \n",
    "                            if scaler_0 is None:\n",
    "                                scaler_0 = cp.deepcopy(scaler)\n",
    "                            else:\n",
    "                                scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                        else:\n",
    "                            prev_w = cnn.get_weights()\n",
    "                            if xtra and mod[0] == 'a':\n",
    "                                print('add neuron')\n",
    "                                cnn = dl.CNN(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                w_temp = cnn.get_weights()\n",
    "                                w_temp[:-2] = prev_w[:-2]\n",
    "                                w_temp[-2][:,:-1] = prev_w[-2]\n",
    "                                w_temp[-1][:-1] = prev_w[-1]\n",
    "                                cnn.set_weights(w_temp)\n",
    "                            if 'adcnn' in mod: # adapt first layer only\n",
    "                                # cnn.base.trainable=False\n",
    "                                cnn.clf.trainable=False\n",
    "                                ep = int(mod[-2:])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], adapt=True, cnnlda=cnnlda, lr=0.00001)\n",
    "                            elif 'avcnn' in mod:\n",
    "                                # prev_w = cnn.get_weights()\n",
    "                                # prev_w = cnn.get_weights()\n",
    "                                prev_mu = [mu_class, std_class, N]\n",
    "                                # generate old training data, same size as clean data\n",
    "                                # x_out = cp.deepcopy(x_clean_cnn)\n",
    "                                # num_y = x_out.shape[0]//prev_ndof[0]\n",
    "                                # y_gen = np.zeros((x_out.shape[0],n_dof))\n",
    "                                # for cl in prev_ndof[1]:\n",
    "                                #     cl_i = prev_ndof[1]==cl\n",
    "                                #     y_gen[int(cl*num_y):int((cl+1)*num_y),np.where(prev_ndof[1]==cl)] = 1\n",
    "                                #     x_out[int(cl*num_y):int((cl+1)*num_y),...] = np.random.normal(mu_class[cl_i], std_class[cl_i],x_clean_cnn[:num_y,...].shape)\n",
    "                                \n",
    "                                x_out = cp.deepcopy(x_clean_cnn)\n",
    "                                num_y = x_out.shape[0]//prev_ndof[0]\n",
    "                                y_gen = np.zeros((x_out.shape[0],n_dof))\n",
    "                                y_xtra = 0\n",
    "                                x_xtra = 0\n",
    "                                for cl in prev_ndof[1]:\n",
    "                                    cl_i = np.where(prev_ndof[1]==cl)\n",
    "                                    x_ind = np.squeeze(y_clean[:,cl_i]==1)\n",
    "                                    if np.sum(x_ind) > 0:\n",
    "                                        y_gen[x_ind,np.where(prev_ndof[1]==cl)] = 1\n",
    "                                        x_out[x_ind,...] = np.random.normal(mu_class[cl_i[0],...], std_class[cl_i[0],...],x_clean_cnn[x_ind,...].shape)\n",
    "                                    else:\n",
    "                                        temp = np.zeros((num_y,n_dof))\n",
    "                                        temp[:,cl_i] = 1\n",
    "                                        if isinstance(y_xtra,np.ndarray):\n",
    "                                            y_xtra = np.vstack((y_xtra,temp))\n",
    "                                            x_xtra = np.vstack((x_xtra,np.random.normal(mu_class[cl_i[0],...], std_class[cl_i[0],...],x_clean_cnn[:num_y,...].shape)))\n",
    "                                        else:\n",
    "                                            y_xtra = cp.deepcopy(temp)\n",
    "                                            x_xtra = np.random.normal(mu_class[cl_i[0],...], std_class[cl_i[0],...],x_clean_cnn[:num_y,...].shape)\n",
    "                                \n",
    "                                # for cl in prev_ndof[1].astype(int):\n",
    "                                #     cl_i = np.where(prev_ndof[1]==cl)\n",
    "                                #     x_ind = np.squeeze(y_clean[:,cl]==1)\n",
    "                                #     if np.sum(x_ind) > 0:\n",
    "                                #         y_gen[x_ind,np.where(prev_ndof[1]==cl)] = 1\n",
    "                                #         x_out[x_ind,...] = np.random.normal(mu_class[cl], std_class[cl],x_clean_cnn[x_ind,...].shape)\n",
    "                                #     else:\n",
    "                                #         temp = np.zeros((num_y,n_dof))\n",
    "                                #         temp[:,cl] = 1\n",
    "                                #         if isinstance(y_xtra,np.ndarray):\n",
    "                                #             y_xtra = np.vstack((y_xtra,temp))\n",
    "                                #             x_xtra = np.vstack((x_xtra,np.random.normal(mu_class[cl], std_class[cl],x_clean_cnn[:num_y,...].shape)))\n",
    "                                #         else:\n",
    "                                #             y_xtra = cp.deepcopy(temp)\n",
    "                                #             x_xtra = np.random.normal(mu_class[cl_i], std_class[cl],x_clean_cnn[:num_y,...].shape)\n",
    "\n",
    "                                if isinstance(y_xtra,np.ndarray):\n",
    "                                    x_out = np.vstack((x_out,x_xtra))\n",
    "                                    y_gen = np.vstack((y_gen,y_xtra))\n",
    "\n",
    "                                x_train_aug = np.vstack((x_out,x_train_cnn))\n",
    "                                y_train_aug = np.vstack((y_gen,y_train))\n",
    "\n",
    "                                x_train_m = np.vstack((x_out,x_clean_cnn))\n",
    "                                y_train_m = np.vstack((y_gen,y_clean))\n",
    "\n",
    "                                # mu_class = np.zeros((n_dof))\n",
    "                                for cl in key.astype(int):\n",
    "                                    cl_i = np.where(key==cl)\n",
    "                                    x_ind = np.squeeze(y_train_m[:,cl_i]==1)\n",
    "                                    mu_class[cl_i[0],...] = np.nanmean(x_train_m[x_ind,...])\n",
    "                                    std_class[cl_i[0],...] = np.nanstd(x_train_m[x_ind,...])\n",
    "                                # mu_class, std_class, N = prd.set_mean(x_clean_cnn,y_clean)\n",
    "\n",
    "                                # mu_class, std_class, N = prd.update_mean(x_clean_cnn,y_clean,N,mu_class,std_class,key,prev_ndof[1])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)],_,_ = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=5, dec=False, lr=0.00001, bn_training=True, bn_trainable=False, prog_train=True)\n",
    "                            elif 'acnn' in mod: # update whole CNN and lda weights\n",
    "                                ep = int(mod[-2:])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], cnnlda=cnnlda, lr=0.00001, bn_training=True, bn_trainable=False, prog_train=True)\n",
    "                            elif 'cnn' in mod: # recalibrate cnnlda\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda, bn_training=True, prog_train=True)\n",
    "                            elif 'acewc' in mod:\n",
    "                                w_c, c_c, all_times[i,mod_tot.index(mod)] = lp.train_task(cnn, 15, 1, x_train_cnn, y_train, [prev_x, x_val_cnn],[prev_y, y_val], lams=[int(mod[-2:])], bat=bat, cnnlda=cnnlda)\n",
    "                            \n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                        del test_mod\n",
    "                        test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                        if i > 0:\n",
    "                            acc_prev[i,:] = lp.test_models(prev_x, prev_y, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_val[i,:] = lp.test_models(x_val_cnn, y_val, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_train[i,:] = lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        \n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            mod_recal[i] = -1\n",
    "                            print('bad recal')\n",
    "                            # if acc_val[i,acc_i] < 75:\n",
    "                            #     if i > 0:\n",
    "                            #         n_dof, key, train_dof = prev_ndof\n",
    "                            #         if 'vcnn' in mod:\n",
    "                            #             mu_class, std_class, N = prev_mu\n",
    "                            #         cnn = dl.CNN(n_class = n_dof)\n",
    "                            #         cnn(x_train_cnn[:1,...])\n",
    "                            #         cnn.set_weights(prev_w)\n",
    "                            #         del test_mod\n",
    "                            #         test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                            # else:\n",
    "                            #     print('keeping new model')\n",
    "                            #     mod_recal[i] = -2\n",
    "                        elif 'cr' in mod:\n",
    "                            clean_data_0 = cp.deepcopy(x_clean_cnn)\n",
    "                            clean_params_0 = cp.deepcopy(y_clean)\n",
    "                        if 'ewc' in mod: \n",
    "                            cnn.compute_fisher(x_train_cnn, y_train, num_samples=200, plot_diffs=False) \n",
    "                            cnn.star()\n",
    "                    else:\n",
    "                        if i == 0:\n",
    "                            N = np.zeros((n_dof),)\n",
    "                            cov_class = np.zeros([x_train_lda.shape[1],x_train_lda.shape[1]])\n",
    "                            mu_class = np.zeros([n_dof,x_train_lda.shape[1]])\n",
    "                        prev_lda = [mu_class,cov_class,N]\n",
    "                        start_time = time.time()\n",
    "                        if mod[0] != 'a' or (i == 0 and mod[0] == 'a'):\n",
    "                            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda, key)\n",
    "                        else:\n",
    "                            w, c, mu_class, cov_class, N = dlda.update_lda(x_train_lda, y_train_lda, N, mu_class, cov_class, key, prev_ndof[1])\n",
    "                        all_times[i,mod_tot.index(mod)] = time.time() - start_time\n",
    "\n",
    "                        acc_val[i,:],out = lp.test_models(None, None, x_val_lda, y_val_lda, lda=[w,c])\n",
    "                        acc_train[i,:],out = lp.test_models(None, None, x_train_lda, y_train_lda, lda=[w,c])\n",
    "                        if i > 0:\n",
    "                            acc_prev[i,:],out = lp.test_models(None, None, prev_x_lda, prev_y_lda, lda=[w,c])\n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            mod_recal[i] = -1\n",
    "                            print('bad recal')\n",
    "                            # if acc_val[i,acc_i] < 75:\n",
    "                            #     if i > 0:\n",
    "                            #         mu_class, cov_class, N = prev_lda\n",
    "                            #         n_dof, key, train_dof = prev_ndof\n",
    "                            # else:\n",
    "                            #     print('keeping new model')\n",
    "                            #     mod_recal[i] = -2\n",
    "                        del x_train_lda, y_train_lda\n",
    "                    \n",
    "                    if mod_recal[i] != -1 or i == 0 :\n",
    "                        prev_x = cp.deepcopy(x_val_cnn)\n",
    "                        prev_y = cp.deepcopy(y_val)\n",
    "                        prev_x_lda = cp.deepcopy(x_val_lda)\n",
    "                        prev_y_lda = cp.deepcopy(y_val_lda)\n",
    "                    \n",
    "                    del x_train_cnn, y_train, x_val_cnn, y_val, x_val_lda, y_val_lda, x_clean_cnn, y_clean\n",
    "                \n",
    "                # load data\n",
    "                test_file = all_files[i+1]\n",
    "                test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "                \n",
    "                # check class labels\n",
    "                test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "                test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key,True)\n",
    "\n",
    "                # test \n",
    "                y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "                \n",
    "                # test \n",
    "                if 'lda' in mod:\n",
    "                    acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "                else:\n",
    "                    acc[i+1,:] = lp.test_models(x_test_cnn, y_test, x_test_lda, y_test_lda, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "\n",
    "                print ('Set: ' + train_file + ', Test: ' + test_file + ',', f'Accuracy: {acc[i+1,acc_i]:.2f}', f', Val: {acc_val[i,acc_i]:.2f}', f', Prev: {acc_prev[i,acc_i]:.2f}', f', Train: {acc_train[i,acc_i]:.2f}')\n",
    "                del y_test, x_test_cnn, x_test_lda, y_test_lda#, test_data, test_params\n",
    "\n",
    "            all_acc[:,mod_tot.index(mod)] = acc[:,acc_i]\n",
    "            all_val[:,mod_tot.index(mod)] = acc_val[:,acc_i]\n",
    "            all_prev[:,mod_tot.index(mod)] = acc_prev[:,acc_i]\n",
    "            all_train[:,mod_tot.index(mod)] = acc_train[:,acc_i]\n",
    "            all_recal[:,mod_tot.index(mod)] = mod_recal\n",
    "\n",
    "            print('------------------------' + mod + ' ' + str(np.sum(mod_recal==1)) + ' - ' + str(np.sum(mod_recal==-1)) + ' -- ' + str(np.sum(mod_recal==-2)) + '-------------')\n",
    "            mod_i += 1\n",
    "\n",
    "            # if 'cr' in mod:\n",
    "            #     del train_data_0, train_params_0\n",
    "\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','wb') as f:\n",
    "            pickle.dump([all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof, mod_all, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale],f)\n",
    "        \n",
    "        gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64),)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:69: RuntimeWarning: Mean of empty slice\n",
      "  ave_acc2 = np.nanmean(np.abs(np.array(it_acc2)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:70: RuntimeWarning: Mean of empty slice\n",
      "  ave_acc = np.nanmean(np.abs(np.array(it_acc)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:71: RuntimeWarning: Mean of empty slice\n",
      "  ave_val = np.nanmean(np.abs(np.array(it_val)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:72: RuntimeWarning: Mean of empty slice\n",
      "  ave_prev = np.nanmean(np.abs(np.array(it_prev)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:73: RuntimeWarning: Mean of empty slice\n",
      "  ave_train = np.nanmean(np.abs(np.array(it_train)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:74: RuntimeWarning: Mean of empty slice\n",
      "  ave_times = np.nanmean(np.abs(np.array(it_times)),axis=0)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1670: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "ft = 'tdar'\n",
    "iter = 6\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "plot_mod = ['lda','alda','cnn','acnn03','avcnn','crcnn']\n",
    "plot_mod = ['lda','avcnn']\n",
    "plot_mod = ['acnn05','avcnn']\n",
    "for plot_i in range(1):\n",
    "    for sub in range(4,5):    \n",
    "        it_acc = []\n",
    "        it_recal = []\n",
    "        it_fail = []\n",
    "        it_val = []\n",
    "        it_prev = []\n",
    "        it_train = []\n",
    "        it_times = []\n",
    "        it_replaced = []\n",
    "        for it in range(1):#iter):\n",
    "        \n",
    "            # load or initialize cnn weights\n",
    "            if plot_i == 1:\n",
    "                with open('0323 full run pre and post/' + subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            else:\n",
    "                with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            \n",
    "            lda_ind = mod_tot.index('alda') + 1\n",
    "            all_acc[all_acc==0] = np.nan\n",
    "            all_val[all_val==0] = np.nan\n",
    "            all_prev[all_prev==0] = np.nan\n",
    "            all_train[all_train==0] = np.nan\n",
    "            all_times[all_times==0] = np.nan\n",
    "\n",
    "            it_acc.append(all_acc)\n",
    "            it_recal.append(np.sum(all_recal==1,axis=0))\n",
    "            it_fail.append(np.sum(all_recal==-1,axis=0))\n",
    "            it_replaced.append(np.sum(all_recal==-2,axis=0))\n",
    "            it_val.append(all_val)\n",
    "            it_prev.append(all_prev)\n",
    "            it_train.append(all_train)\n",
    "            it_times.append(all_times)\n",
    "\n",
    "            it_acc[it][:,:lda_ind] = it_acc[0][:,:lda_ind]\n",
    "            it_recal[it][:lda_ind] = it_recal[0][:lda_ind]\n",
    "            it_fail[it][:lda_ind] = it_fail[0][:lda_ind]\n",
    "            it_replaced[it][:lda_ind] = it_replaced[0][:lda_ind]\n",
    "            it_val[it][:,:lda_ind] = it_val[0][:,:lda_ind]\n",
    "            it_prev[it][:,:lda_ind] = it_prev[0][:,:lda_ind]\n",
    "            it_train[it][:,:lda_ind] = it_train[0][:,:lda_ind]\n",
    "            it_times[it][:,:lda_ind] = it_times[0][:,:lda_ind]\n",
    "\n",
    "\n",
    "        it_acc2 = cp.deepcopy(it_acc)\n",
    "        for i in range(len(it_acc2)):\n",
    "            x = it_val[i] < 0\n",
    "            # print(x.type)\n",
    "            # print(it_acc2[i].shape)\n",
    "            # print(ave_val.shape)\n",
    "            it_acc2[i][(it_acc[i]< 0) & (it_val[i] > 0)]= it_val[i][(it_acc[i]< 0)& (it_val[i] > 0)]\n",
    "            # it_acc2[i][(it_acc[i]< 0)]= it_val[i][(it_acc[i]< 0)]\n",
    "\n",
    "\n",
    "        ave_acc2 = np.nanmean(np.abs(np.array(it_acc2)),axis=0)\n",
    "        ave_acc = np.nanmean(np.abs(np.array(it_acc)),axis=0)\n",
    "        ave_val = np.nanmean(np.abs(np.array(it_val)),axis=0)\n",
    "        ave_prev = np.nanmean(np.abs(np.array(it_prev)),axis=0)\n",
    "        ave_train = np.nanmean(np.abs(np.array(it_train)),axis=0)\n",
    "        ave_times = np.nanmean(np.abs(np.array(it_times)),axis=0)\n",
    "        ave_recal = np.nanmean(np.array(it_recal),axis=0)\n",
    "        ave_fail = np.nanmean(np.array(it_fail),axis=0)\n",
    "        ave_replaced = np.nanmean(np.array(it_replaced),axis=0)\n",
    "\n",
    "        std_acc2 = np.nanstd(np.abs(np.array(it_acc2)),axis=0)/np.sum(~np.isnan(np.array(it_acc2)),axis=0)\n",
    "        std_acc = np.nanstd(np.abs(np.array(it_acc)),axis=0)/np.sum(~np.isnan(np.array(it_acc)),axis=0)\n",
    "        std_val = np.nanstd(np.abs(np.array(it_val)),axis=0)/np.sum(~np.isnan(np.array(it_val)),axis=0)\n",
    "        std_prev = np.nanstd(np.abs(np.array(it_prev)),axis=0)/np.sum(~np.isnan(np.array(it_prev)),axis=0)\n",
    "        std_train = np.nanstd(np.abs(np.array(it_train)),axis=0)/np.sum(~np.isnan(np.array(it_train)),axis=0)\n",
    "        std_times = np.nanstd(np.abs(np.array(it_times)),axis=0)/np.sum(~np.isnan(np.array(it_times)),axis=0)\n",
    "        std_recal = np.nanstd(np.array(it_recal),axis=0)/np.sum(~np.isnan(np.array(it_recal)),axis=0)\n",
    "        std_fail = np.nanstd(np.array(it_fail),axis=0)/np.sum(~np.isnan(np.array(it_fail)),axis=0)\n",
    "        std_replaced = np.nanstd(np.array(it_replaced),axis=0)/np.sum(~np.isnan(np.array(it_replaced)),axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "        for mod in plot_mod:\n",
    "            plot_ind = mod_tot.index(mod)\n",
    "            # ax.plot(ave_acc[2:,plot_ind],'.-',label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            x = ~np.isnan(ave_val[:,plot_ind]) \n",
    "            # ave_acc2 = cp.deepcopy(ave_acc)\n",
    "            # ave_acc[x,plot_ind] = ave_val[x,plot_ind]\n",
    "\n",
    "            ax[0].plot(ave_acc[:,plot_ind],'.-',ms=8,label= mod + ' = '+ \"{:.2f}\".format(ave_recal[plot_ind]) + ' +/- ' + \"{:.2f}\".format(std_recal[plot_ind]) + ', ' + \"{:.2f}\".format(ave_fail[plot_ind]) +' failed'+ ', ' + \"{:.2f}\".format(ave_replaced[plot_ind]) +' replaced')#str(std_recal[plot_ind,0]))\n",
    "            # ax[0].plot(np.squeeze(np.where(x)),ave_acc[x,plot_ind],'kx',ms=12)\n",
    "            ax[1].plot(ave_acc2[:,plot_ind],'.-',ms=8,label= mod + ' = '+ \"{:.2f}\".format(ave_recal[plot_ind]) + ' +/- ' + \"{:.2f}\".format(std_recal[plot_ind])+ ', ' + \"{:.2f}\".format(ave_fail[plot_ind]) + ' failed'+ ', ' + \"{:.2f}\".format(ave_replaced[plot_ind]) +' replaced')#+ str(std_recal[plot_ind,0]))\n",
    "            # ax.plot(np.squeeze(np.where(x)), ave_val[~np.isnan(ave_val[:,plot_ind]),plot_ind],'.-',ms=8,label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            # plt.fill_between(np.arange(ave_acc[2:,plot_ind].shape[0]),ave_acc[2:,plot_ind]-std_acc[2:,plot_ind],ave_acc[2:,plot_ind]+std_acc[2:,plot_ind],alpha=.3)\n",
    "        ax[1].legend()\n",
    "        for i in range(2):\n",
    "            ax[i].axhline(75, ls='--', color='grey')\n",
    "            ax[i].set_ylim([0,100])\n",
    "        # ax[1].axhline(75, ls='--', color='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\utils\\data_utils.py:638: RuntimeWarning: invalid value encountered in sqrt\n",
      "  m = np.sqrt((m1-m2).T*(cov_inv)*(m1-m2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init test dof: [ 1  6 16 17 19 48 53]\n",
      "test_dof: [ 1  6 16 17 19 48 53], key: [0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\main.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=91'>92</a>\u001b[0m test_data, test_params \u001b[39m=\u001b[39m lp\u001b[39m.\u001b[39mcheck_labels(test_data,test_params,train_dof,key,\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=93'>94</a>\u001b[0m \u001b[39m# test \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=94'>95</a>\u001b[0m y_test, _, x_test_cnn, x_test_lda, y_test_lda \u001b[39m=\u001b[39m prd\u001b[39m.\u001b[39;49mprep_test_caps(test_data, test_params, scaler, emg_scale, num_classes\u001b[39m=\u001b[39;49mn_dof, ft\u001b[39m=\u001b[39;49mft, split\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=96'>97</a>\u001b[0m \u001b[39m# test \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=97'>98</a>\u001b[0m acc[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,:] \u001b[39m=\u001b[39m lp\u001b[39m.\u001b[39mtest_models(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m,  x_test_lda, y_test_lda, lda\u001b[39m=\u001b[39m[w,c])\n",
      "File \u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\utils\\data_utils.py:157\u001b[0m, in \u001b[0;36mprep_test_caps\u001b[1;34m(x, params, scaler, emg_scale, num_classes, ft, split)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=154'>155</a>\u001b[0m \u001b[39m# LDA data\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=155'>156</a>\u001b[0m y_lda \u001b[39m=\u001b[39m params[:,[\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=156'>157</a>\u001b[0m x_lda \u001b[39m=\u001b[39m extract_feats_caps(x_orig,ft\u001b[39m=\u001b[39;49mft)\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=157'>158</a>\u001b[0m \u001b[39m# y_lda = np.argmax(y_train_noise,axis=1)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=158'>159</a>\u001b[0m \u001b[39m# x_lda = extract_feats_caps(x_train_noise,ft=ft)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=160'>161</a>\u001b[0m \u001b[39mreturn\u001b[39;00m y_test, x_test_mlp, x_test_cnn, x_lda, y_lda\n",
      "File \u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\utils\\data_utils.py:280\u001b[0m, in \u001b[0;36mextract_feats_caps\u001b[1;34m(raw, ft, uint, order)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=276'>277</a>\u001b[0m     z_th \u001b[39m=\u001b[39m \u001b[39m0.025\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=277'>278</a>\u001b[0m     s_th \u001b[39m=\u001b[39m \u001b[39m0.015\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=279'>280</a>\u001b[0m mean_mav \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile(np\u001b[39m.\u001b[39;49mmean(raw,axis\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,np\u001b[39m.\u001b[39mnewaxis],(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,N))\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=280'>281</a>\u001b[0m raw_demean \u001b[39m=\u001b[39m raw\u001b[39m-\u001b[39mmean_mav\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=282'>283</a>\u001b[0m mav\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mabs(raw_demean),axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3436'>3437</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3437'>3438</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3439'>3440</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39m_mean(a, axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3440'>3441</a>\u001b[0m                       out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\core\\_methods.py:179\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=175'>176</a>\u001b[0m         dtype \u001b[39m=\u001b[39m mu\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39mf4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=176'>177</a>\u001b[0m         is_float16_result \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=178'>179</a>\u001b[0m ret \u001b[39m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=179'>180</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, mu\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=180'>181</a>\u001b[0m     ret \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mtrue_divide(\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=181'>182</a>\u001b[0m             ret, rcount, out\u001b[39m=\u001b[39mret, casting\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m'\u001b[39m, subok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = False\n",
    "ft = 'tdar'\n",
    "iter = 10\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_mod = 0\n",
    "\n",
    "sub_b = []\n",
    "sub_s = []\n",
    "sub_sep=[]\n",
    "for sub in range(2,7):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    # load or initialize cnn weights\n",
    "    acc = np.zeros((len(all_files),2))\n",
    "    acc_val = np.zeros((len(all_files),2))\n",
    "    acc_prev = np.zeros((len(all_files),2))\n",
    "    acc_train = np.zeros((len(all_files),2))\n",
    "    \n",
    "\n",
    "    acc_i = 0\n",
    "\n",
    "    # Loop through files\n",
    "    for i in range(len(all_files)-1):              \n",
    "        # load training file\n",
    "        train_file = all_files[i]\n",
    "        train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "        # load training file\n",
    "        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "        val_data, val_params = train_data, train_params\n",
    "            \n",
    "        # get current dofs and create key\n",
    "        if i == 0:\n",
    "            train_dof = np.unique(train_params[:,-1])\n",
    "            key = np.arange(len(train_dof))\n",
    "\n",
    "            n_dof = len(train_dof)\n",
    "\n",
    "            h = np.ones((len(all_files),n_dof))\n",
    "            b = np.ones((len(all_files),n_dof))\n",
    "            sep = np.zeros((len(all_files),n_dof))\n",
    "            tot_b = np.ones((len(all_files,)))\n",
    "            h[:] = np.nan\n",
    "            b[:] = np.nan\n",
    "\n",
    "            train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key,False)\n",
    "            val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key,False)\n",
    "\n",
    "            _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=False, split=False,num_classes=n_dof)\n",
    "\n",
    "            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda, key)\n",
    "            \n",
    "            cl_s = np.zeros((n_dof,))\n",
    "            # cl_s[:] = np.nan\n",
    "            m_cl = np.zeros((n_dof,x_train_lda.shape[1]))\n",
    "            s_cl = np.zeros((n_dof,x_train_lda.shape[1],x_train_lda.shape[1]))\n",
    "            for cl in train_dof:\n",
    "                train_ind = np.squeeze(y_train_lda == key[train_dof==cl])\n",
    "                temp_x = x_train_lda[train_ind,...]\n",
    "                m1 = np.nanmean(temp_x,axis=0)\n",
    "                s1 = np.cov(temp_x.T)\n",
    "                m_cl[key[train_dof==cl],...] = np.nanmean(temp_x,axis=0)\n",
    "                s_cl[key[train_dof==cl],...] = np.cov(temp_x.T)\n",
    "                ct = 0\n",
    "                for cl_i in train_dof:\n",
    "                    temp_ind = np.squeeze(y_train_lda == key[train_dof==cl_i])\n",
    "                    temp_x2 = x_train_lda[temp_ind,...]\n",
    "                    m2 = np.nanmean(temp_x2,axis=0)\n",
    "                    s2 = np.cov(temp_x2.T)\n",
    "                    cl_s[key[train_dof==cl]] += np.nanmean(prd.mahal(m1,s1,m2,s2))\n",
    "                    ct+=1\n",
    "                cl_s[key[train_dof==cl]] /= ct\n",
    "\n",
    "        #del x_train_lda, y_train_lda, x_train_cnn, y_train, x_clean_cnn, y_clean\n",
    "        \n",
    "        # load data\n",
    "        test_file = all_files[i+1]\n",
    "        test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "        \n",
    "        # check class labels\n",
    "        test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "        test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key,True)\n",
    "\n",
    "        # test \n",
    "        y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "        \n",
    "        # test \n",
    "        acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "\n",
    "        for cl in train_dof:\n",
    "            test_ind = np.squeeze(y_test_lda == key[train_dof==cl])\n",
    "            if i == 0:\n",
    "                train_ind = np.squeeze(y_train_lda == key[train_dof==cl])\n",
    "                m1 = np.nanmean(x_train_lda[train_ind,:],axis=0)\n",
    "                s1 = np.cov(x_train_lda[train_ind,:].T)\n",
    "            if np.sum(test_ind) > 50:\n",
    "                # print(np.sum(test_ind))\n",
    "                m2 = np.nanmean(x_test_lda[test_ind,:],axis=0)\n",
    "                s2 = np.cov(x_test_lda[test_ind,:].T)\n",
    "                h[i+1,key[train_dof==cl]] = prd.hellinger(m1,s1,m2,s2)\n",
    "                b[i+1,key[train_dof==cl]] = np.nanmean(prd.mahal(m1,s1,m2,s2))\n",
    "                ct = 0\n",
    "                for cl_i in train_dof:\n",
    "                    cl_sep = np.nanmean(prd.mahal(np.squeeze(m_cl[key[train_dof==cl_i],...]),np.squeeze(s_cl[key[train_dof==cl_i],...]),m2,s2))\n",
    "                    if np.isnan(cl_sep):\n",
    "                        print('oops')\n",
    "                    else:\n",
    "                        ct += 1\n",
    "                        sep[i+1,key[train_dof==cl]] += cl_sep\n",
    "                sep[i+1,key[train_dof==cl]] /= ct\n",
    "                # b[i+1,key[train_dof==cl]] = prd.bhatta(m1,s1,m2,s2)\n",
    "        # print(h[i+1,:])\n",
    "        # m1 = np.nanmean(x_train_lda,axis=0)\n",
    "        # s1 = np.cov(x_train_lda.T)\n",
    "        # m2 = np.nanmean(x_test_lda,axis=0)\n",
    "        # s2 = np.cov(x_test_lda.T)\n",
    "        # b[i+1,0] = np.min(h[i+1,:])\n",
    "\n",
    "        print(\"{:.2f}\".format(acc[i+1,0]))\n",
    "        del y_test, x_test_cnn#, x_test_lda, y_test_lda, test_data, test_params\n",
    "\n",
    "    print(np.median(acc,axis=0))\n",
    "    with open(subs[sub] + '_hell.p','wb') as f:\n",
    "        pickle.dump([acc,h,b,cl_s,sep],f)\n",
    "    sub_b.append(b)\n",
    "    sub_s.append(cl_s)\n",
    "    sub_sep.append(sep)\n",
    "    \n",
    "    gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 4\n",
    "h[h==0] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "for sub in range(7):\n",
    "    with open(subs[sub] + '_hell.p','rb') as f:\n",
    "        acc,h,b,cl_s,sep = pickle.load(f)\n",
    "    min_h = np.nanmax(h[1:,:],axis=1)\n",
    "    norm_m = b/np.nanmean(cl_s)\n",
    "    acc_p = acc[1:,0]\n",
    "    fig,ax = plt.subplots()\n",
    "    fig1,ax1 = plt.subplots(1,2,figsize=(15,3))\n",
    "    # ax.plot(-np.nanmean(b[1:,:],axis=1),acc_p, 'x')\n",
    "    ax.plot(np.nanmax(b[1:,:],axis=1),acc_p, 'x')\n",
    "    ax1[0].plot(np.nanmax(b[1:,:],axis=1), 'x')\n",
    "    ax1[1].plot(acc_p, 'x')\n",
    "    ax1[1].set_ylim([0,100])\n",
    "    # ax1[0].set_ylim([0,20])\n",
    "    # ax.set_xlim([0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for sub in range(7):\n",
    "    with open(subs[sub] + '_lda_accs.p','rb') as f:\n",
    "        acc, _ = pickle.load(f)\n",
    "    temp.append(acc[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 128\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn','acnn03','acnn30','acewc00','acewc30', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn03', 'avcnn15', 'acnnl03','crvcnn','acewclm','xtra','xtra1','xtra2']\n",
    "ft = 'feat'\n",
    "iter = 1\n",
    "\n",
    "for sub in range(4,5):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    mod_all = ['vcnn']\n",
    "\n",
    "    # load or initialize cnn weights\n",
    "    if load_mod:\n",
    "        with open(subs[sub] + '_' + str(0) + '_r_accs.p','rb') as f:\n",
    "            all_acc, all_recal, all_val, all_prev, all_train, all_times, _, _, c_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "    else:\n",
    "        c_weights = None\n",
    "        v_weights = None\n",
    "        v_wc = None\n",
    "        cl_wc = None\n",
    "        all_recal = np.empty((len(mod_tot),1))\n",
    "        all_recal[:] = np.nan\n",
    "        all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "    mod_i = 0\n",
    "    for mod in mod_all:\n",
    "        acc = np.zeros((len(all_files),5))\n",
    "        acc_val = np.zeros((len(all_files),5))\n",
    "        acc_prev = np.zeros((len(all_files),5))\n",
    "        acc_train = np.zeros((len(all_files),5))\n",
    "\n",
    "        if 'cnn' in mod:\n",
    "            acc_i = 2\n",
    "        elif 'cewc' in mod:\n",
    "            acc_i = 4\n",
    "        elif 'lda' in mod:\n",
    "            acc_i = 0\n",
    "\n",
    "        cnn = None\n",
    "        ewc = None\n",
    "\n",
    "        ep = 50\n",
    "        recal = 0\n",
    "        skip = False\n",
    "\n",
    "        # Loop through files\n",
    "        for i in range(1,2):#len(all_files)-1):\n",
    "            # load training file\n",
    "            train_file = all_files[i]\n",
    "            train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "            train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "            val_data = train_data\n",
    "            val_params = train_params\n",
    "\n",
    "            train_dof = np.unique(train_params[:,-1])\n",
    "            key = np.empty(train_dof.shape)\n",
    "            for key_i in range(len(train_dof)):\n",
    "                key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "            n_dof = int(np.max(key))\n",
    "            \n",
    "            train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "            val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key)\n",
    "\n",
    "            _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "\n",
    "            _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "            del train_data, train_params, val_data, val_params\n",
    "\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, print_b=True)\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=[cnn], n_dof=n_dof, ep=30, dec=False,print_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_clean_cnn)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    x_out[y_clean[:,cl]==1,...] = np.random.normal(np.mean(x_clean_cnn[y_clean[:,cl]==1,...],axis=0), np.std(x_clean_cnn[y_clean[:,cl]==1,...],axis=0),x_clean_cnn[y_clean[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_train_cnn)\n",
    "for cl in range(y_train.shape[1]):\n",
    "    x_out[y_train[:,cl]==1,...] = np.random.normal(np.mean(x_train_cnn[y_train[:,cl]==1,...],axis=0), np.std(x_train_cnn[y_train[:,cl]==1,...],axis=0),x_train_cnn[y_train[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = np.ones((1*y_clean.shape[0],8))\n",
    "x_out,_,_ = cnn.dec(samp1,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),samp=True)\n",
    "# _,x_out,_,_ = cnn(x_clean_cnn,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),dec=True)\n",
    "x_out = x_out.numpy()\n",
    "# for i in range(y_clean.shape[1]):\n",
    "#     adjust1 = np.std(x_clean_cnn[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     rescale = np.mean(adjust1)/np.mean(np.std(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0))\n",
    "#     gmean = np.mean(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     x_out[np.argmax(y_clean,axis=1)==i,...] = (x_out[np.argmax(y_clean,axis=1)==i,...] - gmean)*rescale + gmean\n",
    "# x_out = np.maximum(np.minimum(x_out,1),0)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    ind = np.tile(np.argmax(y_clean,axis=1)==cl,[1])\n",
    "    ind2 = np.argmax(y_clean,axis=1)==cl\n",
    "    x_temp = x_out[ind,...].reshape((np.sum(ind),-1))\n",
    "    x_true = x_clean_cnn[ind2,...].reshape((np.sum(ind2),-1))\n",
    "    # for i in range()\n",
    "    plt.figure()\n",
    "    # for i in range(x_true.shape[0]):\n",
    "    #     plt.plot(x_true[i,...],'k-')\n",
    "        \n",
    "    for i in range(x_temp.shape[0]):\n",
    "        plt.plot(x_temp[i,...],'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lda = x_clean_cnn.reshape(x_clean_cnn.shape[0],-1)\n",
    "y_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "y_train_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_lda,y_lda)\n",
    "y_out = dlda.predict(x_lda, w, c)\n",
    "print(dlda.eval_lda(w, c, x_lda, y_lda))\n",
    "x_out_lda = x_out.reshape(x_out.shape[0],-1)\n",
    "print(dlda.eval_lda(w,c, x_out_lda,np.tile(y_train_lda,[1,1])))\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_out_lda,np.tile(y_train_lda,[1,1]))\n",
    "print(dlda.eval_lda(w, c, x_out_lda,y_train_lda))\n",
    "print(dlda.eval_lda(w, c, x_lda, np.argmax(y_clean,axis=1)[...,np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn1,test_accuracy)\n",
    "print(lp.test_models(x_out, y_clean, None, None, cnn=cnn1, test_mod=test_mod, test_accuracy=test_accuracy))\n",
    "\n",
    "cnn2, _ = lp.train_models(traincnn=x_out,y_train=y_clean, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn2,test_accuracy)\n",
    "print(lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn2, test_mod=test_mod, test_accuracy=test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "fig,ax = plt.subplots(1,5,figsize=(30,4))\n",
    "for sub in range(2,3):#,5):\n",
    "    with open(subs[sub] + '_0_r_accs.p','rb') as f:\n",
    "        acc_all, recal_all, cur_all, prev_all, val_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "    # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "    colors =  cm.get_cmap('tab20c')\n",
    "    c = np.empty((20,4))\n",
    "    for i in range(20):\n",
    "        c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "    nn_c[0,-1] = 1\n",
    "    all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "    pt_m = ['ko-','o-','o-','o-','s','s','s','s','D']\n",
    "    nn_c = np.vstack((np.array([0,0,0,1]), c[0,:],c[1,:],c[2,:],c[3,:],c[4,:],c[5,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "    # nn_c[0,-1] = 1\n",
    "\n",
    "    labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "    labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "    # labels = mod_tot\n",
    "\n",
    "    ax_ind = sub\n",
    "    it = 0\n",
    "    for v in [1,2]: \n",
    "        i = mod_tot.index(mod_all[v])\n",
    "        acc_temp = acc_all[1:-1,i]\n",
    "        if not np.isnan(acc_temp).all():\n",
    "            x = np.arange(len(acc_temp))\n",
    "            recal_i = (acc_temp < 0)\n",
    "            ax[ax_ind].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v],color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "            it+=1\n",
    "\n",
    "    for i in range(5):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        \n",
    "        ax[i].set_ylim([0,100])\n",
    "        ax[i].set_title('TR' + str(i+1))\n",
    "    ax[0].legend()\n",
    "    ax[2].set_xlabel('Calibration Set')\n",
    "    ax[0].set_ylabel('Accuracy (%)')\n",
    "    plt.rc('font', size=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "for sub in range(2,3):#,5):\n",
    "    fig,ax = plt.subplots(1,4,figsize=(20,4))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, val_all,mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [1,0,0,1,2,2,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[it]+': ' + str(int(recal_all[i,0])),color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "                it+=1\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "cv_iter = 1\n",
    "for sub in range(0,5):\n",
    "    fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','o','*','o','s','D','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','f-cnn-5','f-cnn-3','f-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in [0, 3, 5, 4, 6, 7]: #range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v]+': ' + str(int(recal_all[i,0])),color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                it+=1\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "        \n",
    "\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e4d54467b05e62951c9fd7929782b99429e3b62c1a3b146d4f3dbf79f907e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('adapt_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
