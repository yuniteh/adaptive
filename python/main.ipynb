{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpu import set_gpu\n",
    "import numpy as np\n",
    "import os\n",
    "import adapt.utils.data_utils as prd\n",
    "import adapt.loop as lp\n",
    "import adapt.ml.lda as dlda\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import adapt.ml.dl_subclass as dl\n",
    "import copy as cp\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR62\n",
      "Set: 20180515_060621, Test: 20180521_090336, Accuracy: 60.18 , Val: 81.83 , Prev: 0.00 , Train: 81.83\n",
      "skip bad set: 20180521_090336, accuracy: 61.71\n",
      "Set: 20180521_090336, Test: 20180524_161811, Accuracy: 71.26 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180525_084201, Accuracy: 67.93 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20180525_084201\n",
      "Set: 20180525_084201, Test: 20180531_073149, Accuracy: 82.25 , Val: 75.03 , Prev: 54.19 , Train: 86.25\n",
      "Set: 20180525_084201, Test: 20180602_105936, Accuracy: 76.63 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180525_084201, Test: 20180604_090437, Accuracy: 50.49 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20180604_090437\n",
      "Set: 20180604_090437, Test: 20180604_214053, Accuracy: 52.06 , Val: 74.71 , Prev: 51.20 , Train: 92.29\n",
      "recal: 3 20180604_214053\n",
      "Set: 20180604_214053, Test: 20180605_080547, Accuracy: 48.10 , Val: 80.02 , Prev: 47.66 , Train: 93.65\n",
      "recal: 4 20180605_080547\n",
      "Set: 20180605_080547, Test: 20180606_174322, Accuracy: 55.54 , Val: 76.17 , Prev: 74.40 , Train: 96.56\n",
      "recal: 5 20180606_174322\n",
      "Set: 20180606_174322, Test: 20180610_081839, Accuracy: 53.83 , Val: 91.58 , Prev: 37.04 , Train: 96.04\n",
      "recal: 6 20180610_081839\n",
      "Set: 20180610_081839, Test: 20180612_085047, Accuracy: 59.55 , Val: 77.32 , Prev: 59.56 , Train: 98.02\n",
      "recal: 7 20180612_085047\n",
      "Set: 20180612_085047, Test: 20180614_081624, Accuracy: 72.31 , Val: 83.98 , Prev: 78.88 , Train: 98.33\n",
      "Set: 20180612_085047, Test: 20180616_182930, Accuracy: 63.30 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 8 20180616_182930\n",
      "Set: 20180616_182930, Test: 20180617_184248, Accuracy: 59.92 , Val: 77.21 , Prev: 72.11 , Train: 90.21\n",
      "skip bad set: 20180617_184248, accuracy: 44.95\n",
      "Set: 20180617_184248, Test: 20180619_184831, Accuracy: 21.86 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20180619_184831, accuracy: 38.98\n",
      "Set: 20180619_184831, Test: 20180619_193541, Accuracy: 51.95 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 9 20180619_193541\n",
      "Set: 20180619_193541, Test: 20180620_073139, Accuracy: 62.57 , Val: 86.58 , Prev: 71.07 , Train: 97.29\n",
      "recal: 10 20180620_073139\n",
      "Set: 20180620_073139, Test: 20180620_073838, Accuracy: 75.12 , Val: 90.01 , Prev: 57.54 , Train: 99.90\n",
      "Set: 20180620_073139, Test: 20180620_083903, Accuracy: 48.20 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 11 20180620_083903\n",
      "Set: 20180620_083903, Test: 20180621_093741, Accuracy: 32.54 , Val: 81.27 , Prev: 71.28 , Train: 86.77\n",
      "skip bad set: 20180621_093741, accuracy: 29.63\n",
      "Set: 20180621_093741, Test: 20180621_094013, Accuracy: 19.89 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 12 20180621_094013\n",
      "Set: 20180621_094013, Test: 20180623_200107, Accuracy: 60.44 , Val: 78.77 , Prev: 63.89 , Train: 93.54\n",
      "recal: 13 20180623_200107\n",
      "Set: 20180623_200107, Test: 20180703_072425, Accuracy: 73.82 , Val: 74.51 , Prev: 77.63 , Train: 93.96\n",
      "Set: 20180623_200107, Test: 20180711_074144, Accuracy: 54.03 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 14 20180711_074144\n",
      "Set: 20180711_074144, Test: 20180711_113445, Accuracy: 36.86 , Val: 73.67 , Prev: 62.23 , Train: 93.23\n",
      "recal: 15 20180711_113445\n",
      "Set: 20180711_113445, Test: 20180713_102029, Accuracy: 77.46 , Val: 84.08 , Prev: 72.01 , Train: 90.00\n",
      "Set: 20180711_113445, Test: 20180717_073851, Accuracy: 73.19 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180711_113445, Test: 20180717_112511, Accuracy: 66.63 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "lda 15\n",
      "training nn\n",
      "time: 35.6032919883728\n",
      "Set: 20180515_060621, Test: 20180521_090336, Accuracy: 68.87 , Val: 97.21 , Prev: 0.00 , Train: 97.21\n",
      "skip bad set: 20180521_090336, accuracy: 61.71\n",
      "Set: 20180521_090336, Test: 20180524_161811, Accuracy: 86.99 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180525_084201, Accuracy: 80.84 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180531_073149, Accuracy: 80.79 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180602_105936, Accuracy: 82.25 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180604_090437, Accuracy: 75.79 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180604_214053, Accuracy: 79.28 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180605_080547, Accuracy: 78.55 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180606_174322, Accuracy: 71.42 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180610_081839, Accuracy: 74.13 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180612_085047, Accuracy: 74.49 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180614_081624, Accuracy: 73.66 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180616_182930, Accuracy: 77.04 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180617_184248, Accuracy: 63.04 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20180617_184248, accuracy: 44.95\n",
      "Set: 20180617_184248, Test: 20180619_184831, Accuracy: 36.49 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20180619_184831, accuracy: 38.98\n",
      "Set: 20180619_184831, Test: 20180619_193541, Accuracy: 77.56 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180619_184831, Test: 20180620_073139, Accuracy: 72.83 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180619_184831, Test: 20180620_073838, Accuracy: 65.23 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20180620_073838\n",
      "training nn\n",
      "time: 10.549525737762451\n",
      "Set: 20180620_073838, Test: 20180620_083903, Accuracy: 57.57 , Val: 71.80 , Prev: 39.28 , Train: 100.00\n",
      "recal: 2 20180620_083903\n",
      "training nn\n",
      "time: 11.461034536361694\n",
      "WARNING:tensorflow:5 out of the last 23 calls to <function get_test.<locals>.test_step at 0x0000022C31E3EF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 25 calls to <function get_test.<locals>.test_step at 0x0000022C31E3EF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Set: 20180620_083903, Test: 20180621_093741, Accuracy: 34.77 , Val: 85.22 , Prev: 79.92 , Train: 99.27\n",
      "skip bad set: 20180621_093741, accuracy: 29.63\n",
      "Set: 20180621_093741, Test: 20180621_094013, Accuracy: 72.83 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180621_093741, Test: 20180623_200107, Accuracy: 60.54 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 3 20180623_200107\n",
      "training nn\n",
      "time: 11.468899488449097\n",
      "Set: 20180623_200107, Test: 20180703_072425, Accuracy: 74.18 , Val: 77.11 , Prev: 75.44 , Train: 100.00\n",
      "Set: 20180623_200107, Test: 20180711_074144, Accuracy: 68.77 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 4 20180711_074144\n",
      "training nn\n",
      "time: 11.461658716201782\n",
      "Set: 20180711_074144, Test: 20180711_113445, Accuracy: 54.24 , Val: 74.19 , Prev: 62.75 , Train: 99.06\n",
      "recal: 5 20180711_113445\n",
      "training nn\n",
      "time: 11.494391202926636\n",
      "Set: 20180711_113445, Test: 20180713_102029, Accuracy: 64.08 , Val: 84.81 , Prev: 55.57 , Train: 99.69\n",
      "recal: 6 20180713_102029\n",
      "training nn\n",
      "time: 11.581781387329102\n",
      "Set: 20180713_102029, Test: 20180717_073851, Accuracy: 79.44 , Val: 87.94 , Prev: 66.39 , Train: 99.79\n",
      "Set: 20180713_102029, Test: 20180717_112511, Accuracy: 71.21 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "cnn 6\n",
      "setting CNN weights\n",
      "Set: 20180515_060621, Test: 20180521_090336, Accuracy: 68.87 , Val: 97.21 , Prev: 0.00 , Train: 97.45\n",
      "skip bad set: 20180521_090336, accuracy: 61.71\n",
      "Set: 20180521_090336, Test: 20180524_161811, Accuracy: 86.99 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180525_084201, Accuracy: 80.84 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180531_073149, Accuracy: 80.79 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180602_105936, Accuracy: 82.25 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180604_090437, Accuracy: 75.79 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180604_214053, Accuracy: 79.28 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180605_080547, Accuracy: 78.55 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180606_174322, Accuracy: 71.42 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180610_081839, Accuracy: 74.13 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180612_085047, Accuracy: 74.49 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180614_081624, Accuracy: 73.66 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180616_182930, Accuracy: 77.04 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180521_090336, Test: 20180617_184248, Accuracy: 63.04 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20180617_184248, accuracy: 44.95\n",
      "Set: 20180617_184248, Test: 20180619_184831, Accuracy: 36.49 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20180619_184831, accuracy: 38.98\n",
      "Set: 20180619_184831, Test: 20180619_193541, Accuracy: 77.56 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180619_184831, Test: 20180620_073139, Accuracy: 72.83 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180619_184831, Test: 20180620_073838, Accuracy: 65.23 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20180620_073838\n",
      "training nn\n",
      "time: 1.7688822746276855\n",
      "Set: 20180620_073838, Test: 20180620_083903, Accuracy: 72.46 , Val: 73.88 , Prev: 94.82 , Train: 73.12\n",
      "Set: 20180620_073838, Test: 20180621_093741, Accuracy: 44.46 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "skip bad set: 20180621_093741, accuracy: 29.63\n",
      "Set: 20180621_093741, Test: 20180621_094013, Accuracy: 81.52 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180621_093741, Test: 20180623_200107, Accuracy: 68.66 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20180623_200107\n",
      "training nn\n",
      "time: 1.8127048015594482\n",
      "Set: 20180623_200107, Test: 20180703_072425, Accuracy: 80.48 , Val: 76.59 , Prev: 70.97 , Train: 68.75\n",
      "Set: 20180623_200107, Test: 20180711_074144, Accuracy: 76.99 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180623_200107, Test: 20180711_113445, Accuracy: 62.21 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 3 20180711_113445\n",
      "training nn\n",
      "time: 2.06286883354187\n",
      "Set: 20180711_113445, Test: 20180713_102029, Accuracy: 80.27 , Val: 60.15 , Prev: 73.78 , Train: 68.65\n",
      "Set: 20180711_113445, Test: 20180717_073851, Accuracy: 70.85 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180711_113445, Test: 20180717_112511, Accuracy: 65.80 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "acnn03 3\n"
     ]
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 128\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn','acnn03','acnn30','acewc00','acewc30', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn', 'avcnn15', 'acnnl03','crvcnn','acewclm','xtra','xtra1','xtra2']\n",
    "ft = 'feat'\n",
    "iter = 1\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "for sub in range(4,5):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    for it in range(0,iter):\n",
    "        # first iteration, includes LDA; others exclude LDA\n",
    "        if it == 0:\n",
    "            mod_all = ['lda','cnn','acnn03']\n",
    "        else:\n",
    "            mod_all = ['cnnl','acnnl3','acnnl5','acnnl30','acewcl']\n",
    "\n",
    "        # load or initialize cnn weights\n",
    "        if load_mod:\n",
    "            with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                all_acc, all_recal, all_val, all_prev, all_train, all_times, _, _, c_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "        else:\n",
    "            c_weights = None\n",
    "            v_weights = None\n",
    "            v_wc = None\n",
    "            cl_wc = None\n",
    "            all_recal = np.empty((len(mod_tot),1))\n",
    "            all_recal[:] = np.nan\n",
    "            all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "        mod_i = 0\n",
    "        for mod in mod_all:\n",
    "            acc = np.zeros((len(all_files),2))\n",
    "            acc_val = np.zeros((len(all_files),2))\n",
    "            acc_prev = np.zeros((len(all_files),2))\n",
    "            acc_train = np.zeros((len(all_files),2))\n",
    "\n",
    "            if 'lda' in mod:\n",
    "                acc_i = 0\n",
    "            else:\n",
    "                acc_i = 1\n",
    "\n",
    "            cnn = None\n",
    "            ewc = None\n",
    "            clda = None\n",
    "\n",
    "            ep = 30\n",
    "            recal = 0\n",
    "            skip = False\n",
    "\n",
    "            # Loop through files\n",
    "            for i in range(1,len(all_files)-1):\n",
    "                # Check if need to recalibrate\n",
    "                if i > 1:\n",
    "                    if 'b' in mod:\n",
    "                        skip = True\n",
    "                    elif acc[i,acc_i] < 70:\n",
    "                        skip = False\n",
    "                    else:\n",
    "                        skip = True\n",
    "                    \n",
    "                    if not skip:\n",
    "                        train_file = all_files[i]\n",
    "                        train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                            \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "\n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "                        _, _, _, _, _, _, x_tr, y_tr, emg_scale_tr, _, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, ft=ft, noise=False, split=False)\n",
    "\n",
    "                        _, _, _, _, _, _, x_val, y_val, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale_tr, prop_b=False, ft=ft, noise=False, split=False)\n",
    "\n",
    "                        w, c, _, _, _, _, _ = dlda.train_lda(x_tr, y_tr)\n",
    "                        skip_test = lp.test_models(None, None, x_val, y_val, lda=[w,c])[0]\n",
    "\n",
    "                        if skip_test > 70:\n",
    "                            skip = False\n",
    "                            recal += 1\n",
    "                            print('recal: ' + str(recal) + ' ' + all_files[i])\n",
    "                            acc[i,acc_i] *= -1\n",
    "                        else:\n",
    "                            skip = True\n",
    "                            print('skip bad set: ' + all_files[i] + ', ' + f'accuracy: {skip_test:.2f}')\n",
    "                        \n",
    "                        del skip_test, train_data, train_params, val_data, val_params, x_tr, y_tr, x_val, y_val, emg_scale_tr, train_temp, params_temp, tr_i, te_i\n",
    "\n",
    "                if not skip:\n",
    "                    # load training file\n",
    "                    train_file = all_files[i]\n",
    "                    train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "                    # if first train, use two train files\n",
    "                    if i == 1:\n",
    "                        train_data2, train_params2 = prd.load_caps_train(sub_path + all_files[i-1] + '/traindata.mat')\n",
    "                        train_data = np.vstack((train_data,train_data2))\n",
    "                        train_params = np.vstack((train_params,train_params2))\n",
    "                        del train_data2, train_params2\n",
    "\n",
    "                        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "                        val_data = train_data\n",
    "                        val_params = train_params\n",
    "                    else:\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                    \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "\n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "\n",
    "                        del train_temp, params_temp, tr_i, te_i\n",
    "\n",
    "                    if (i == 1 and mod[0] == 'a') or (mod[0] != 'a'):\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        key = np.empty(train_dof.shape)\n",
    "                        for key_i in range(len(train_dof)):\n",
    "                            key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "                        n_dof = int(np.max(key))\n",
    "                    \n",
    "                    train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "                    val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key)\n",
    "\n",
    "                    # if combining, save current training data\n",
    "                    # if 'cr' in mod:\n",
    "                    #     # combine old and new training data\n",
    "                    #     if i > 1:\n",
    "                    #         train_data = np.vstack((train_data_0,train_data))\n",
    "                    #         train_params = np.vstack((train_params_0,train_params))\n",
    "\n",
    "                    #     train_data_0 = cp.deepcopy(train_data)\n",
    "                    #     train_params_0 = cp.deepcopy(train_params)\n",
    "\n",
    "                    if (mod[0] == 'a' and i > 1) or ('cr' in mod and i > 1) or (mod == 'vcnn' and i > 1):\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, _, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale, scaler=scaler, num_classes=n_dof, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False)\n",
    "                    else:\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "                        if ((i == 1) and (c_weights is not None)) or ((i == 1) and (v_weights is not None)):\n",
    "                            scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                    _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "                    if 'cr' in mod:\n",
    "                        # combine old and new training data\n",
    "                        if i > 1:\n",
    "                            x_clean_cnn = np.vstack((clean_data_0,x_clean_cnn))\n",
    "                            y_clean = np.vstack((clean_params_0,y_clean))\n",
    "                            x_train_cnn = np.vstack((x_clean_cnn,x_train_cnn))\n",
    "                            y_train = np.vstack((y_clean,y_train))\n",
    "\n",
    "                        clean_data_0 = cp.deepcopy(x_clean_cnn)\n",
    "                        clean_params_0 = cp.deepcopy(y_clean)\n",
    "\n",
    "                    del train_data, train_params, val_data, val_params\n",
    "\n",
    "                    if 'lda' not in mod:\n",
    "                        cnnlda = 'l' in mod\n",
    "                        if 'vcnn' in mod:\n",
    "                            if i == 1:\n",
    "                                if v_weights is None:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_clean_cnn,y_train=y_clean, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, trainable=True)\n",
    "                                    # save current autoencoder weights\n",
    "                                    b_enc_w = cp.deepcopy(cnn.var.get_weights())\n",
    "                                    dec_w = cp.deepcopy(cnn.dec.get_weights())\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=[cnn], n_dof=n_dof, ep=30, dec=False, trainable=True)\n",
    "                                    # calculate normalization scale\n",
    "                                    adjust = []\n",
    "                                    for cl_i in range(n_dof):\n",
    "                                        adjust.append(np.std(x_clean_cnn[np.argmax(y_clean,axis=1)==cl_i,...],axis=0))\n",
    "                                    v_weights = cp.deepcopy(cnn.get_weights())\n",
    "                                    b_enc_0 = cp.deepcopy(b_enc_w)\n",
    "                                    dec_0 = cp.deepcopy(dec_w)\n",
    "                                    scaler_0 = cp.deepcopy(scaler)    \n",
    "                                else:\n",
    "                                    print('setting VCNN weights')\n",
    "                                    cnn = dl.VCNN(n_class = n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.add_dec(x_train_cnn[:1,...])\n",
    "                                    cnn(x_train_cnn[:2,...],np.ones((2,)),dec=True) \n",
    "                                    cnn.set_weights(v_weights)\n",
    "                                test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                            else:\n",
    "                                # generate old training data, same size as clean data\n",
    "                                samp = np.ones((y_train.shape[0],4))\n",
    "                                x_out = cnn.dec(samp,np.argmax(y_train,axis=1).astype('float32'),samp=True,bn_training=False,bn_trainable=False)[0].numpy()\n",
    "\n",
    "                                # scale generated data variance\n",
    "                                for cl_i in range(y_train.shape[1]):\n",
    "                                    rescale = np.mean(adjust)/np.mean(np.std(x_out[np.argmax(y_train,axis=1)==cl_i,...],axis=0))\n",
    "                                    gmean = np.mean(x_out[np.argmax(y_train,axis=1)==cl_i,...],axis=0)\n",
    "                                    x_out[np.argmax(y_train,axis=1)==cl_i,...] = (x_out[np.argmax(y_train,axis=1)==cl_i,...] - gmean)*rescale + gmean\n",
    "                                x_out = np.maximum(np.minimum(x_out,1),0)\n",
    "\n",
    "                                x_train_aug = np.vstack((x_out,x_train_cnn))\n",
    "                                y_train_aug = np.vstack((y_train,y_train))\n",
    "\n",
    "                                samp = np.ones((y_clean.shape[0],4))\n",
    "                                x_out = cnn.dec(samp,np.argmax(y_clean,axis=1).astype('float32'),samp=True,bn_training=False,bn_trainable=False)[0].numpy()\n",
    "\n",
    "                                # scale generated data variance\n",
    "                                for cl_i in range(y_clean.shape[1]):\n",
    "                                    rescale = np.mean(adjust)/np.mean(np.std(x_out[np.argmax(y_clean,axis=1)==cl_i,...],axis=0))\n",
    "                                    gmean = np.mean(x_out[np.argmax(y_clean,axis=1)==cl_i,...],axis=0)\n",
    "                                    x_out[np.argmax(y_clean,axis=1)==cl_i,...] = (x_out[np.argmax(y_clean,axis=1)==cl_i,...] - gmean)*rescale + gmean\n",
    "                                x_out = np.maximum(np.minimum(x_out,1),0)\n",
    "\n",
    "                                x_clean_aug = np.vstack((x_out,x_clean_cnn))\n",
    "                                y_clean_aug = np.vstack((y_clean,y_clean))\n",
    "\n",
    "                                # set new normalization scales\n",
    "                                for cl_i in range(n_dof):\n",
    "                                    adjust[cl_i] = np.std(x_clean_aug[np.argmax(y_clean_aug,axis=1)==cl_i,...],axis=0)\n",
    "\n",
    "                                if 'avcnn' in mod: # update whole CNN and lda weights\n",
    "                                    # ep = int(mod[-2:])\n",
    "                                    # save current classifier weights\n",
    "                                    enc_w = cp.deepcopy(cnn.var.get_weights())\n",
    "                                    clf_w = cp.deepcopy(cnn.clf.get_weights())\n",
    "\n",
    "                                    # set autoencoder weights, finetune and save new autoencoder weights\n",
    "                                    cnn.var.set_weights(b_enc_w)\n",
    "                                    cnn.dec.set_weights(dec_w)\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_clean_aug,y_train=y_clean_aug, mod=['vcnn'], n_dof=n_dof, ep=10, dec=True, lr=0.00001, trainable=False)\n",
    "                                    b_enc_w = cp.deepcopy(cnn.var.get_weights())\n",
    "                                    dec_w = cp.deepcopy(cnn.dec.get_weights())\n",
    "\n",
    "                                    # set old classifier weights and finetune\n",
    "                                    cnn.var.set_weights(enc_w)\n",
    "                                    cnn.clf.set_weights(clf_w)\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=5, dec=False, lr=0.00001, trainable=False)\n",
    "                                else:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_clean_aug,y_train=y_clean_aug, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, trainable=True)\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=30, dec=False, trainable=True)\n",
    "                                test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                                acc_prev[i,:] = lp.test_models(prev_x, prev_y, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        else:\n",
    "                            if i == 1:\n",
    "                                if c_weights is None:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                                    c_weights = cp.deepcopy(cnn.get_weights())\n",
    "                                    scaler_0 = cp.deepcopy(scaler)    \n",
    "                                    cl_wc = cp.deepcopy([w_c,c_c])\n",
    "                                else:\n",
    "                                    print('setting CNN weights')\n",
    "                                    cnn = dl.CNN(n_class=n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.set_weights(c_weights)\n",
    "                                    if cnnlda:\n",
    "                                        print('setting LDA weights')\n",
    "                                        w_c = cp.deepcopy(cl_wc[0].astype('float32'))\n",
    "                                        c_c = cp.deepcopy(cl_wc[1].astype('float32'))\n",
    "                                if 'ewc' in mod:\n",
    "                                    cnn = dl.EWC(n_class=n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.set_weights(c_weights)\n",
    "                                if 'ad' in mod:\n",
    "                                    cnn = dl.CNN(n_class=n_dof,adapt=True)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.set_weights(c_weights)\n",
    "\n",
    "                                if 'l' in mod:\n",
    "                                    clda = [w_c, c_c]\n",
    "\n",
    "                                test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                            else:\n",
    "                                if mod =='acnnlm': # update CNN encoder using lda for loss\n",
    "                                    ep = 5\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[[cnn,w_c,c_c]],cnnlda=cnnlda)\n",
    "                                elif 'adcnn' in mod: # adapt first layer only\n",
    "                                    # cnn.base.trainable=False\n",
    "                                    cnn.clf.trainable=False\n",
    "                                    ep = int(mod[-2:])\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], adapt=True, cnnlda=cnnlda, lr=0.00001)\n",
    "                                elif 'acnn' in mod: # update whole CNN and lda weights\n",
    "                                    ep = int(mod[-2:])\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], cnnlda=cnnlda, lr=0.00001)\n",
    "                                elif mod == 'afcnnl': # update lda only \n",
    "                                    w_c, c_c = lp.train_models(x_train_lda=cnn.enc(x_train_cnn).numpy(), y_train_lda=np.argmax(y_train,axis=1)[...,np.newaxis], mod=['lda'])\n",
    "                                elif 'cnn' in mod: # recalibrate cnnlda\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                                elif mod == 'acewclm':\n",
    "                                    _, _, all_times[i,mod_tot.index(mod)] = lp.train_task(cnn, ep, 1, x_train_cnn, y_train, [x_val_cnn], [y_val], lams=[int(mod[-2:])], bat=bat, clda=[w_c,c_c], cnnlda=cnnlda)\n",
    "                                elif 'acewc' in mod:\n",
    "                                    w_c, c_c, all_times[i,mod_tot.index(mod)] = lp.train_task(cnn, ep, 1, x_train_cnn, y_train, [prev_x, x_val_cnn],[prev_y, y_val], lams=[0,int(mod[-2:])], bat=bat, cnnlda=cnnlda)\n",
    "                                \n",
    "                                if 'l' in mod:\n",
    "                                    clda = [w_c, c_c]\n",
    "\n",
    "                                test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                                acc_prev[i,:] = lp.test_models(prev_x, prev_y, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        \n",
    "                        acc_val[i,:] = lp.test_models(x_val_cnn, y_val, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_train[i,:] = lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "\n",
    "                        if 'ewc' in mod: \n",
    "                            cnn.compute_fisher(x_clean_cnn, y_clean, num_samples=200, plot_diffs=False) \n",
    "                            cnn.star()\n",
    "                    else:\n",
    "                        start_time = time.time()\n",
    "                        if mod[0] != 'a' or (i == 1 and mod[0] == 'a'):\n",
    "                            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda)\n",
    "                        else:\n",
    "                            w, c, mu_class, cov_class, N = dlda.update_lda(x_train_lda, y_train_lda, N, mu_class, cov_class)\n",
    "                        all_times[i,mod_tot.index(mod)] = time.time() - start_time\n",
    "\n",
    "                        acc_val[i,:] = lp.test_models(None, None, x_val_lda, y_val_lda, lda=[w,c])\n",
    "                        acc_train[i,:] = lp.test_models(None, None, x_train_lda, y_train_lda, lda=[w,c])\n",
    "                        if i > 1:\n",
    "                            acc_prev[i,:] = lp.test_models(None, None, prev_x_lda, prev_y_lda, lda=[w,c])\n",
    "                        del x_train_lda, y_train_lda\n",
    "                    \n",
    "                    prev_x = cp.deepcopy(x_val_cnn)\n",
    "                    prev_y = cp.deepcopy(y_val)\n",
    "                    prev_x_lda = cp.deepcopy(x_val_lda)\n",
    "                    prev_y_lda = cp.deepcopy(y_val_lda)\n",
    "                    \n",
    "                    del x_train_cnn, y_train, x_val_cnn, y_val, x_val_lda, y_val_lda, x_clean_cnn, y_clean\n",
    "                \n",
    "                # load data\n",
    "                test_file = all_files[i+1]\n",
    "                test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "                \n",
    "                # check class labels\n",
    "                test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "                test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key)\n",
    "\n",
    "                # test \n",
    "                y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "\n",
    "                # test \n",
    "                if 'lda' in mod:\n",
    "                    acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "                else:\n",
    "                    acc[i+1,:] = lp.test_models(x_test_cnn, y_test, x_test_lda, y_test_lda, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "\n",
    "                print ('Set: ' + train_file + ', Test: ' + test_file + ',', f'Accuracy: {acc[i+1,acc_i]:.2f}', f', Val: {acc_val[i,acc_i]:.2f}', f', Prev: {acc_prev[i,acc_i]:.2f}', f', Train: {acc_train[i,acc_i]:.2f}')\n",
    "                del y_test, x_test_cnn, x_test_lda, y_test_lda, test_data, test_params\n",
    "\n",
    "            all_acc[:,mod_tot.index(mod)] = acc[:,acc_i]\n",
    "            all_recal[mod_tot.index(mod)] = recal\n",
    "            all_val[:,mod_tot.index(mod)] = acc_val[:,acc_i]\n",
    "            all_prev[:,mod_tot.index(mod)] = acc_prev[:,acc_i]\n",
    "            all_train[:,mod_tot.index(mod)] = acc_train[:,acc_i]\n",
    "\n",
    "            print(mod + ' ' + str(recal))\n",
    "            mod_i += 1\n",
    "\n",
    "            if 'cr' in mod:\n",
    "                del train_data_0, train_params_0\n",
    "\n",
    "        # with open(subs[sub] + '_' + str(it) + '_r_accs.p','wb') as f:\n",
    "        #     pickle.dump([all_acc, all_recal, all_val, all_prev, all_train, all_times, mod_all, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 128\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn','acnn03','acnn30','acewc00','acewc30', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn03', 'avcnn15', 'acnnl03','crvcnn','acewclm','xtra','xtra1','xtra2']\n",
    "ft = 'feat'\n",
    "iter = 1\n",
    "\n",
    "for sub in range(4,5):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    mod_all = ['vcnn']\n",
    "\n",
    "    # load or initialize cnn weights\n",
    "    if load_mod:\n",
    "        with open(subs[sub] + '_' + str(0) + '_r_accs.p','rb') as f:\n",
    "            all_acc, all_recal, all_val, all_prev, all_train, all_times, _, _, c_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "    else:\n",
    "        c_weights = None\n",
    "        v_weights = None\n",
    "        v_wc = None\n",
    "        cl_wc = None\n",
    "        all_recal = np.empty((len(mod_tot),1))\n",
    "        all_recal[:] = np.nan\n",
    "        all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "    mod_i = 0\n",
    "    for mod in mod_all:\n",
    "        acc = np.zeros((len(all_files),5))\n",
    "        acc_val = np.zeros((len(all_files),5))\n",
    "        acc_prev = np.zeros((len(all_files),5))\n",
    "        acc_train = np.zeros((len(all_files),5))\n",
    "\n",
    "        if 'cnn' in mod:\n",
    "            acc_i = 2\n",
    "        elif 'cewc' in mod:\n",
    "            acc_i = 4\n",
    "        elif 'lda' in mod:\n",
    "            acc_i = 0\n",
    "\n",
    "        cnn = None\n",
    "        ewc = None\n",
    "\n",
    "        ep = 50\n",
    "        recal = 0\n",
    "        skip = False\n",
    "\n",
    "        # Loop through files\n",
    "        for i in range(1,2):#len(all_files)-1):\n",
    "            # load training file\n",
    "            train_file = all_files[i]\n",
    "            train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "            train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "            val_data = train_data\n",
    "            val_params = train_params\n",
    "\n",
    "            train_dof = np.unique(train_params[:,-1])\n",
    "            key = np.empty(train_dof.shape)\n",
    "            for key_i in range(len(train_dof)):\n",
    "                key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "            n_dof = int(np.max(key))\n",
    "            \n",
    "            train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "            val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key)\n",
    "\n",
    "            _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "\n",
    "            _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "            del train_data, train_params, val_data, val_params\n",
    "\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_clean_cnn,y_train=y_clean, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True)\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=[cnn], n_dof=n_dof, ep=15, dec=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = np.ones((1*y_clean.shape[0],4))\n",
    "x_out,_,_ = cnn.dec(samp,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),samp=True)\n",
    "x_out = x_out.numpy()\n",
    "for i in range(y_clean.shape[1]):\n",
    "    adjust1 = np.std(x_clean_cnn[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "    rescale = np.mean(adjust1)/np.mean(np.std(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0))\n",
    "    gmean = np.mean(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "    x_out[np.argmax(y_clean,axis=1)==i,...] = (x_out[np.argmax(y_clean,axis=1)==i,...] - gmean)*rescale + gmean\n",
    "x_out = np.maximum(np.minimum(x_out,1),0)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    ind = np.tile(np.argmax(y_clean,axis=1)==cl,[1])\n",
    "    ind2 = np.argmax(y_clean,axis=1)==cl\n",
    "    x_temp = x_out[ind,...].reshape((np.sum(ind),-1))\n",
    "    x_true = x_clean_cnn[ind2,...].reshape((np.sum(ind2),-1))\n",
    "    # for i in range()\n",
    "    plt.figure()\n",
    "    for i in range(x_true.shape[0]):\n",
    "        plt.plot(x_true[i,...],'k-')\n",
    "        \n",
    "    for i in range(x_temp.shape[0]):\n",
    "        plt.plot(x_temp[i,...],'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lda = x_clean_cnn.reshape(x_clean_cnn.shape[0],-1)\n",
    "y_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_lda,y_lda)\n",
    "y_out = dlda.predict(x_lda, w, c)\n",
    "print(dlda.eval_lda(w, c, x_lda, y_lda))\n",
    "x_out_lda = x_out.reshape(x_out.shape[0],-1)\n",
    "print(dlda.eval_lda(w,c, x_out_lda,np.tile(y_lda,[1,1])))\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_out_lda,np.tile(y_lda,[1,1]))\n",
    "print(dlda.eval_lda(w, c, x_out.reshape(x_out.shape[0],-1), np.argmax(y_clean,axis=1)[...,np.newaxis]))\n",
    "print(dlda.eval_lda(w, c, x_lda, np.argmax(y_clean,axis=1)[...,np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "fig,ax = plt.subplots(1,5,figsize=(30,4))\n",
    "for sub in range(2,3):#,5):\n",
    "    with open(subs[sub] + '_0_r_accs.p','rb') as f:\n",
    "        acc_all, recal_all, cur_all, prev_all, val_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "    # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "    colors =  cm.get_cmap('tab20c')\n",
    "    c = np.empty((20,4))\n",
    "    for i in range(20):\n",
    "        c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "    nn_c[0,-1] = 1\n",
    "    all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "    pt_m = ['ko-','o-','o-','o-','s','s','s','s','D']\n",
    "    nn_c = np.vstack((np.array([0,0,0,1]), c[0,:],c[1,:],c[2,:],c[3,:],c[4,:],c[5,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "    # nn_c[0,-1] = 1\n",
    "\n",
    "    labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "    labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "    # labels = mod_tot\n",
    "\n",
    "    ax_ind = sub\n",
    "    it = 0\n",
    "    for v in [1,2]: \n",
    "        i = mod_tot.index(mod_all[v])\n",
    "        acc_temp = acc_all[1:-1,i]\n",
    "        if not np.isnan(acc_temp).all():\n",
    "            x = np.arange(len(acc_temp))\n",
    "            recal_i = (acc_temp < 0)\n",
    "            ax[ax_ind].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v],color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "            it+=1\n",
    "\n",
    "    for i in range(5):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        \n",
    "        ax[i].set_ylim([0,100])\n",
    "        ax[i].set_title('TR' + str(i+1))\n",
    "    ax[0].legend()\n",
    "    ax[2].set_xlabel('Calibration Set')\n",
    "    ax[0].set_ylabel('Accuracy (%)')\n",
    "    plt.rc('font', size=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "for sub in range(2,3):#,5):\n",
    "    fig,ax = plt.subplots(1,4,figsize=(20,4))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, val_all,mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [1,0,0,1,2,2,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[it]+': ' + str(int(recal_all[i,0])),color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "                it+=1\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "cv_iter = 1\n",
    "for sub in range(0,5):\n",
    "    fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','o','*','o','s','D','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','f-cnn-5','f-cnn-3','f-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in [0, 3, 5, 4, 6, 7]: #range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v]+': ' + str(int(recal_all[i,0])),color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                it+=1\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "        \n",
    "\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e4d54467b05e62951c9fd7929782b99429e3b62c1a3b146d4f3dbf79f907e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('adapt_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
