{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpu import set_gpu\n",
    "import numpy as np\n",
    "import os\n",
    "import adapt.utils.data_utils as prd\n",
    "import adapt.loop as lp\n",
    "import adapt.ml.lda as dlda\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import adapt.ml.dl_subclass as dl\n",
    "import copy as cp\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython import display\n",
    "import gc as gc\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR59\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170424_082640, Accuracy: 40.44 , Val: 85.57 , Prev: 0.00 , Train: 85.57\n",
      "recal: 1 20170424_082640\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_082640, Test: 20170424_085656, Accuracy: 65.69 , Val: 52.79 , Prev: 43.50 , Train: 95.64\n",
      "recal: 2 20170424_085656\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_085656, Test: 20170424_091501, Accuracy: 74.13 , Val: 89.40 , Prev: 52.93 , Train: 99.05\n",
      "recal: 3 20170424_091501\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_091501, Test: 20170424_094328, Accuracy: 69.84 , Val: 75.41 , Prev: 58.97 , Train: 99.32\n",
      "recal: 4 20170424_094328\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_094328, Test: 20170424_104554, Accuracy: 27.84 , Val: 71.33 , Prev: 48.64 , Train: 98.09\n",
      "recal: 5 20170424_104554\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_104554, Test: 20170424_114643, Accuracy: 33.08 , Val: 60.24 , Prev: 63.45 , Train: 98.22\n",
      "recal: 6 20170424_114643\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_114643, Test: 20170424_142122, Accuracy: 33.42 , Val: 53.33 , Prev: 47.28 , Train: 97.00\n",
      "recal: 7 20170424_142122\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_142122, Test: 20170427_161648, Accuracy: 29.82 , Val: 73.13 , Prev: 62.91 , Train: 97.40\n",
      "recal: 8 20170427_161648\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170427_161648, Test: 20170427_191711, Accuracy: 45.54 , Val: 80.95 , Prev: 15.49 , Train: 98.23\n",
      "recal: 9 20170427_191711\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170427_191711, Test: 20170428_151357, Accuracy: 19.54 , Val: 80.68 , Prev: 29.39 , Train: 98.09\n",
      "recal: 10 20170428_151357\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170428_151357, Test: 20170428_151659, Accuracy: 19.61 , Val: 43.40 , Prev: 15.37 , Train: 100.00\n",
      "recal: 11 20170428_151659\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170428_151659, Test: 20170430_144524, Accuracy: 19.26 , Val: 83.81 , Prev: 50.75 , Train: 99.73\n",
      "recal: 12 20170430_144524\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170430_144524, Test: 20170430_144857, Accuracy: 67.60 , Val: 76.73 , Prev: 25.44 , Train: 100.00\n",
      "recal: 13 20170430_144857\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170430_144857, Test: 20170430_151517, Accuracy: 27.50 , Val: 51.43 , Prev: 42.72 , Train: 98.91\n",
      "recal: 14 20170430_151517\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170430_151517, Test: 20170501_140917, Accuracy: 62.63 , Val: 82.61 , Prev: 48.44 , Train: 98.36\n",
      "recal: 15 20170501_140917\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170501_140917, Test: 20170501_141731, Accuracy: 80.53 , Val: 78.40 , Prev: 64.95 , Train: 99.32\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170501_140917, Test: 20170501_142024, Accuracy: 52.28 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 16 20170501_142024\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170501_142024, Test: 20170502_071722, Accuracy: 48.54 , Val: 51.63 , Prev: 66.71 , Train: 96.18\n",
      "recal: 17 20170502_071722\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_071722, Test: 20170502_072300, Accuracy: 8.30 , Val: 48.71 , Prev: 11.01 , Train: 99.59\n",
      "recal: 18 20170502_072300\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_072300, Test: 20170502_073440, Accuracy: 14.43 , Val: 40.68 , Prev: 47.28 , Train: 100.00\n",
      "recal: 19 20170502_073440\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_073440, Test: 20170502_075048, Accuracy: 30.97 , Val: 56.87 , Prev: 20.79 , Train: 98.77\n",
      "recal: 20 20170502_075048\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_075048, Test: 20170502_092225, Accuracy: 58.61 , Val: 44.70 , Prev: 69.57 , Train: 94.00\n",
      "recal: 21 20170502_092225\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_092225, Test: 20170502_092652, Accuracy: 65.83 , Val: 76.05 , Prev: 71.47 , Train: 99.73\n",
      "recal: 22 20170502_092652\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_092652, Test: 20170502_170413, Accuracy: 15.38 , Val: 67.07 , Prev: 37.96 , Train: 98.23\n",
      "recal: 23 20170502_170413\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_170413, Test: 20170503_160818, Accuracy: 7.69 , Val: 78.37 , Prev: 40.27 , Train: 100.00\n",
      "recal: 24 20170503_160818\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170503_160818, Test: 20170503_161240, Accuracy: 28.80 , Val: 71.70 , Prev: 32.79 , Train: 99.59\n",
      "recal: 25 20170503_161240\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170503_161240, Test: 20170503_165907, Accuracy: 58.07 , Val: 89.54 , Prev: 15.37 , Train: 99.59\n",
      "recal: 26 20170503_165907\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170503_165907, Test: 20170504_105348, Accuracy: 15.38 , Val: 45.58 , Prev: 36.41 , Train: 97.00\n",
      "recal: 27 20170504_105348\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170504_105348, Test: 20170504_105753, Accuracy: 24.78 , Val: 78.91 , Prev: 34.92 , Train: 99.59\n",
      "recal: 28 20170504_105753\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170504_105753, Test: 20170504_111151, Accuracy: 31.72 , Val: 80.98 , Prev: 15.37 , Train: 99.59\n",
      "recal: 29 20170504_111151\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170504_111151, Test: 20170506_131247, Accuracy: 69.57 , Val: 82.04 , Prev: 77.17 , Train: 100.00\n",
      "recal: 30 20170506_131247\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170506_131247, Test: 20170531_164633, Accuracy: 49.56 , Val: 61.96 , Prev: 50.75 , Train: 99.05\n",
      "recal: 31 20170531_164633\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170531_164633, Test: 20170531_170707, Accuracy: 25.05 , Val: 91.85 , Prev: 25.17 , Train: 99.86\n",
      "recal: 32 20170531_170707\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170531_170707, Test: 20170531_174743, Accuracy: 13.82 , Val: 65.17 , Prev: 12.50 , Train: 98.09\n",
      "recal: 33 20170531_174743\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170531_174743, Test: 20170602_095356, Accuracy: 18.24 , Val: 92.53 , Prev: 91.17 , Train: 99.86\n",
      "recal: 34 20170602_095356\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170602_095356, Test: 20170602_095749, Accuracy: 9.94 , Val: 49.66 , Prev: 14.54 , Train: 98.64\n",
      "recal: 35 20170602_095749\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 48]\n",
      "Set: 20170602_095749, Test: 20170602_141330, Accuracy: 56.84 , Val: 87.36 , Prev: 84.51 , Train: 100.00\n",
      "recal: 36 20170602_141330\n",
      "train dof: [ 1  6 16 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170602_141330, Test: 20170602_141548, Accuracy: 55.00 , Val: 55.71 , Prev: 54.62 , Train: 88.27\n",
      "recal: 37 20170602_141548\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19]\n",
      "Set: 20170602_141548, Test: 20170604_133359, Accuracy: 88.97 , Val: 78.50 , Prev: 85.73 , Train: 99.86\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170602_141548, Test: 20170604_133553, Accuracy: 71.68 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 38 20170604_133553\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170604_133553, Test: 20170604_134046, Accuracy: 77.67 , Val: 60.46 , Prev: 42.72 , Train: 100.00\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170604_133553, Test: 20170604_134326, Accuracy: 33.97 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 39 20170604_134326\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170604_134326, Test: 20170605_110918, Accuracy: 70.12 , Val: 66.85 , Prev: 68.71 , Train: 96.04\n",
      "recal: 40 20170605_110918\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_110918, Test: 20170605_111438, Accuracy: 28.86 , Val: 77.58 , Prev: 26.12 , Train: 99.73\n",
      "recal: 41 20170605_111438\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_111438, Test: 20170605_112816, Accuracy: 67.12 , Val: 86.01 , Prev: 78.12 , Train: 95.63\n",
      "recal: 42 20170605_112816\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_112816, Test: 20170605_171921, Accuracy: 47.72 , Val: 63.95 , Prev: 35.46 , Train: 99.18\n",
      "recal: 43 20170605_171921\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_171921, Test: 20170605_172143, Accuracy: 16.00 , Val: 85.17 , Prev: 61.41 , Train: 96.59\n",
      "recal: 44 20170605_172143\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_172143, Test: 20170605_172358, Accuracy: 40.03 , Val: 57.14 , Prev: 43.81 , Train: 99.32\n",
      "recal: 45 20170605_172358\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_172358, Test: 20170605_172605, Accuracy: 8.30 , Val: 71.70 , Prev: 29.93 , Train: 99.73\n",
      "recal: 46 20170605_172605\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_172605, Test: 20170605_172838, Accuracy: 11.57 , Val: 62.59 , Prev: 11.70 , Train: 97.41\n",
      "recal: 47 20170605_172838\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_172838, Test: 20170606_172325, Accuracy: 17.84 , Val: 76.22 , Prev: 76.33 , Train: 94.68\n",
      "recal: 48 20170606_172325\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170606_172325, Test: 20170606_172621, Accuracy: 17.63 , Val: 26.67 , Prev: 44.97 , Train: 97.00\n",
      "recal: 49 20170606_172621\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170606_172621, Test: 20170606_172943, Accuracy: 7.69 , Val: 43.40 , Prev: 26.36 , Train: 98.50\n",
      "recal: 50 20170606_172943\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170606_172943, Test: 20170606_173231, Accuracy: 17.56 , Val: 51.29 , Prev: 16.44 , Train: 99.86\n",
      "recal: 51 20170606_173231\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170606_173231, Test: 20170606_173444, Accuracy: 9.94 , Val: 48.30 , Prev: 27.85 , Train: 96.87\n",
      "recal: 52 20170606_173444\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170606_173444, Test: 20170607_111519, Accuracy: 26.89 , Val: 65.31 , Prev: 22.01 , Train: 99.05\n",
      "recal: 53 20170607_111519\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170607_111519, Test: 20170607_111831, Accuracy: 23.55 , Val: 82.31 , Prev: 11.55 , Train: 99.86\n",
      "recal: 54 20170607_111831\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170607_111831, Test: 20170607_132942, Accuracy: 41.73 , Val: 86.94 , Prev: 60.41 , Train: 99.73\n",
      "recal: 55 20170607_132942\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170607_132942, Test: 20170607_134433, Accuracy: 54.25 , Val: 82.04 , Prev: 18.37 , Train: 97.96\n",
      "recal: 56 20170607_134433\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170607_134433, Test: 20170608_105908, Accuracy: 15.38 , Val: 70.11 , Prev: 74.01 , Train: 98.50\n",
      "recal: 57 20170608_105908\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170608_105908, Test: 20170608_110127, Accuracy: 15.32 , Val: 83.27 , Prev: 14.83 , Train: 99.46\n",
      "recal: 58 20170608_110127\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170608_110127, Test: 20170610_091855, Accuracy: 49.01 , Val: 66.44 , Prev: 26.53 , Train: 99.32\n",
      "recal: 59 20170610_091855\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170610_091855, Test: 20170610_142504, Accuracy: 55.75 , Val: 66.26 , Prev: 18.91 , Train: 99.46\n",
      "recal: 60 20170610_142504\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170610_142504, Test: 20170612_115630, Accuracy: 87.13 , Val: 89.40 , Prev: 46.12 , Train: 99.86\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170610_142504, Test: 20170612_193216, Accuracy: 46.15 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 61 20170612_193216\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170612_193216, Test: 20170613_171441, Accuracy: 22.87 , Val: 71.97 , Prev: 10.33 , Train: 97.55\n",
      "recal: 62 20170613_171441\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170613_171441, Test: 20170613_171735, Accuracy: 64.74 , Val: 75.92 , Prev: 33.15 , Train: 99.46\n",
      "recal: 63 20170613_171735\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170613_171735, Test: 20170613_174657, Accuracy: 52.14 , Val: 72.93 , Prev: 48.30 , Train: 98.37\n",
      "recal: 64 20170613_174657\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170613_174657, Test: 20170613_174902, Accuracy: 47.31 , Val: 73.88 , Prev: 20.00 , Train: 99.46\n",
      "recal: 65 20170613_174902\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170613_174902, Test: 20170613_175924, Accuracy: 60.72 , Val: 58.64 , Prev: 43.54 , Train: 99.05\n",
      "recal: 66 20170613_175924\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170613_175924, Test: 20170616_155518, Accuracy: 46.97 , Val: 89.52 , Prev: 32.79 , Train: 99.32\n",
      "recal: 67 20170616_155518\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170616_155518, Test: 20170616_155727, Accuracy: 72.29 , Val: 63.27 , Prev: 59.32 , Train: 98.91\n",
      "recal: 68 20170616_155727\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170616_155727, Test: 20170618_125006, Accuracy: 52.76 , Val: 85.73 , Prev: 40.14 , Train: 96.45\n",
      "recal: 69 20170618_125006\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170618_125006, Test: 20170619_181445, Accuracy: 49.15 , Val: 68.89 , Prev: 72.96 , Train: 95.36\n",
      "recal: 70 20170619_181445\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170619_181445, Test: 20170619_183646, Accuracy: 28.05 , Val: 78.12 , Prev: 13.45 , Train: 97.95\n",
      "recal: 71 20170619_183646\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170619_183646, Test: 20170620_164658, Accuracy: 55.07 , Val: 74.86 , Prev: 50.54 , Train: 98.50\n",
      "recal: 72 20170620_164658\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170620_164658, Test: 20170620_165204, Accuracy: 53.51 , Val: 80.82 , Prev: 28.12 , Train: 98.91\n",
      "recal: 73 20170620_165204\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170620_165204, Test: 20170622_142516, Accuracy: 39.96 , Val: 88.98 , Prev: 59.46 , Train: 99.05\n",
      "recal: 74 20170622_142516\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170622_142516, Test: 20170623_130233, Accuracy: 19.20 , Val: 57.55 , Prev: 38.64 , Train: 98.09\n",
      "recal: 75 20170623_130233\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_130233, Test: 20170623_131239, Accuracy: 12.66 , Val: 32.11 , Prev: 9.39 , Train: 98.37\n",
      "recal: 76 20170623_131239\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_131239, Test: 20170623_180256, Accuracy: 9.87 , Val: 47.62 , Prev: 49.25 , Train: 100.00\n",
      "recal: 77 20170623_180256\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_180256, Test: 20170623_180953, Accuracy: 41.87 , Val: 72.79 , Prev: 33.88 , Train: 99.18\n",
      "recal: 78 20170623_180953\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_180953, Test: 20170623_185445, Accuracy: 36.83 , Val: 74.29 , Prev: 40.54 , Train: 99.32\n",
      "recal: 79 20170623_185445\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_185445, Test: 20170623_185857, Accuracy: 45.34 , Val: 56.05 , Prev: 23.40 , Train: 99.86\n",
      "recal: 80 20170623_185857\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_185857, Test: 20170625_113904, Accuracy: 17.29 , Val: 51.16 , Prev: 62.31 , Train: 99.73\n",
      "recal: 81 20170625_113904\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170625_113904, Test: 20170625_114119, Accuracy: 44.45 , Val: 68.30 , Prev: 57.82 , Train: 99.86\n",
      "recal: 82 20170625_114119\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170625_114119, Test: 20170708_130614, Accuracy: 48.47 , Val: 70.61 , Prev: 31.56 , Train: 99.32\n",
      "recal: 83 20170708_130614\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170708_130614, Test: 20170710_093530, Accuracy: 15.38 , Val: 69.02 , Prev: 65.99 , Train: 95.63\n",
      "recal: 84 20170710_093530\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19]\n",
      "Set: 20170710_093530, Test: 20170710_093802, Accuracy: 5.24 , Val: 54.15 , Prev: 31.70 , Train: 99.46\n",
      "recal: 85 20170710_093802\n",
      "train dof: [ 1  6 16 19]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170710_093802, Test: 20170710_093945, Accuracy: 4.63 , Val: 43.95 , Prev: 45.85 , Train: 100.00\n",
      "recal: 86 20170710_093945\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170710_093945, Test: 20170710_094211, Accuracy: 34.92 , Val: 74.69 , Prev: 33.47 , Train: 98.77\n",
      "recal: 87 20170710_094211\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170710_094211, Test: 20170727_160330, Accuracy: 86.11 , Val: 97.55 , Prev: 40.68 , Train: 99.46\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170710_094211, Test: 20180530_172736, Accuracy: 67.60 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 88 20180530_172736\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180530_172736, Test: 20180602_113609, Accuracy: 80.32 , Val: 89.51 , Prev: 82.59 , Train: 98.43\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180530_172736, Test: 20180602_113944, Accuracy: 79.54 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180530_172736, Test: 20180604_172706, Accuracy: 65.33 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 89 20180604_172706\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180604_172706, Test: 20180605_164702, Accuracy: 54.24 , Val: 77.42 , Prev: 40.81 , Train: 93.44\n",
      "recal: 90 20180605_164702\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180605_164702, Test: 20180605_165004, Accuracy: 47.16 , Val: 77.21 , Prev: 38.40 , Train: 97.60\n",
      "recal: 91 20180605_165004\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180605_165004, Test: 20180606_195649, Accuracy: 33.73 , Val: 70.48 , Prev: 46.31 , Train: 96.66\n",
      "recal: 92 20180606_195649\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180606_195649, Test: 20180609_120622, Accuracy: 33.11 , Val: 84.30 , Prev: 36.94 , Train: 98.75\n",
      "recal: 93 20180609_120622\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180609_120622, Test: 20180609_132458, Accuracy: 43.36 , Val: 69.61 , Prev: 22.25 , Train: 93.02\n",
      "recal: 94 20180609_132458\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180609_132458, Test: 20180611_210422, Accuracy: 45.50 , Val: 67.12 , Prev: 73.70 , Train: 97.71\n",
      "recal: 95 20180611_210422\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180611_210422, Test: 20180612_180710, Accuracy: 46.95 , Val: 73.36 , Prev: 14.97 , Train: 96.98\n",
      "recal: 96 20180612_180710\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180612_180710, Test: 20180612_181625, Accuracy: 43.88 , Val: 39.02 , Prev: 7.69 , Train: 96.98\n",
      "recal: 97 20180612_181625\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180612_181625, Test: 20180614_170623, Accuracy: 51.48 , Val: 77.26 , Prev: 63.93 , Train: 99.58\n",
      "recal: 98 20180614_170623\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180614_170623, Test: 20180619_174839, Accuracy: 44.09 , Val: 59.52 , Prev: 49.12 , Train: 96.35\n",
      "recal: 99 20180619_174839\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180619_174839, Test: 20180623_135541, Accuracy: 70.95 , Val: 56.19 , Prev: 72.79 , Train: 95.94\n",
      "recal: 100 20180623_135541\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180623_135541, Test: 20180625_175216, Accuracy: 60.54 , Val: 76.72 , Prev: 75.29 , Train: 96.45\n",
      "recal: 101 20180625_175216\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180625_175216, Test: 20180626_171859, Accuracy: 53.36 , Val: 73.67 , Prev: 65.70 , Train: 97.08\n",
      "recal: 102 20180626_171859\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180626_171859, Test: 20180626_180300, Accuracy: 71.47 , Val: 80.02 , Prev: 77.34 , Train: 97.60\n",
      "recal: 103 20180626_180300\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180626_180300, Test: 20180626_180540, Accuracy: 49.92 , Val: 68.47 , Prev: 47.55 , Train: 96.88\n",
      "recal: 104 20180626_180540\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180626_180540, Test: 20180627_181932, Accuracy: 70.12 , Val: 85.03 , Prev: 64.20 , Train: 96.77\n",
      "recal: 105 20180627_181932\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180627_181932, Test: 20180701_171739, Accuracy: 31.49 , Val: 85.34 , Prev: 65.38 , Train: 96.45\n",
      "recal: 106 20180701_171739\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180701_171739, Test: 20180701_172049, Accuracy: 34.04 , Val: 68.57 , Prev: 42.20 , Train: 96.67\n",
      "recal: 107 20180701_172049\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180701_172049, Test: 20180704_130403, Accuracy: 34.62 , Val: 83.16 , Prev: 55.61 , Train: 99.37\n",
      "recal: 108 20180704_130403\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180704_130403, Test: 20180706_185352, Accuracy: 27.90 , Val: 90.32 , Prev: 35.76 , Train: 99.48\n",
      "recal: 109 20180706_185352\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180706_185352, Test: 20180706_202732, Accuracy: 47.68 , Val: 73.67 , Prev: 35.48 , Train: 96.56\n",
      "recal: 110 20180706_202732\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180706_202732, Test: 20180711_181347, Accuracy: 39.46 , Val: 90.32 , Prev: 27.99 , Train: 99.58\n",
      "recal: 111 20180711_181347\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180711_181347, Test: 20180712_190835, Accuracy: 11.97 , Val: 92.40 , Prev: 45.06 , Train: 99.17\n",
      "recal: 112 20180712_190835\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "bad recal\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180712_190835, Test: 20180714_105107, Accuracy: 13.07 , Val: 63.89 , Prev: 37.36 , Train: 96.25\n",
      "recal: 113 20180714_105107\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180714_105107, Test: 20180718_192200, Accuracy: 27.64 , Val: 79.71 , Prev: 34.96 , Train: 98.65\n",
      "recal: 114 20180718_192200\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20180718_192200, Test: 20180726_173515, Accuracy: 59.55 , Val: 85.74 , Prev: 62.12 , Train: 98.23\n",
      "lda 114 - 65\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(6,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170424_082640, Accuracy: 40.44 , Val: 85.57 , Prev: 0.00 , Train: 85.57\n",
      "recal: 1 20170424_082640\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170424_085656, Accuracy: 80.46 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170424_091501, Accuracy: 74.00 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20170424_091501\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170424_094328, Accuracy: 69.37 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 3 20170424_094328\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170424_104554, Accuracy: 49.22 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 4 20170424_104554\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170424_114643, Accuracy: 56.64 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 5 20170424_114643\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170424_142122, Accuracy: 51.60 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 6 20170424_142122\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170427_161648, Accuracy: 33.42 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 7 20170427_161648\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170427_191711, Accuracy: 69.78 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 8 20170427_191711\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170428_151357, Accuracy: 16.88 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 9 20170428_151357\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170428_151659, Accuracy: 25.12 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 10 20170428_151659\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170430_144524, Accuracy: 25.94 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 11 20170430_144524\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170430_144857, Accuracy: 18.31 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 12 20170430_144857\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170430_151517, Accuracy: 36.62 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 13 20170430_151517\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170501_140917, Accuracy: 79.92 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170501_141731, Accuracy: 81.89 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170501_142024, Accuracy: 52.76 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 14 20170501_142024\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170502_071722, Accuracy: 18.18 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 15 20170502_071722\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170502_072300, Accuracy: 15.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 16 20170502_072300\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170502_073440, Accuracy: 48.60 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 17 20170502_073440\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170502_075048, Accuracy: 54.53 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 18 20170502_075048\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170502_092225, Accuracy: 72.02 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 19 20170502_092225\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170502_092652, Accuracy: 55.55 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 20 20170502_092652\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170502_170413, Accuracy: 15.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 21 20170502_170413\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170503_160818, Accuracy: 13.21 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 22 20170503_160818\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170503_161240, Accuracy: 65.01 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 23 20170503_161240\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170503_165907, Accuracy: 30.09 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 24 20170503_165907\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170504_105348, Accuracy: 15.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 25 20170504_105348\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170504_105753, Accuracy: 62.01 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 26 20170504_105753\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170504_111151, Accuracy: 74.81 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 27 20170504_111151\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170506_131247, Accuracy: 62.70 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 28 20170506_131247\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170531_164633, Accuracy: 78.97 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170531_170707, Accuracy: 43.16 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 29 20170531_170707\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170531_174743, Accuracy: 79.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170602_095356, Accuracy: 21.51 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 30 20170602_095356\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170602_095749, Accuracy: 80.46 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 48]\n",
      "Set: 20170424_081251, Test: 20170602_141330, Accuracy: 60.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 31 20170602_141330\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170602_141548, Accuracy: 62.70 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 32 20170602_141548\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19]\n",
      "Set: 20170424_081251, Test: 20170604_133359, Accuracy: 86.39 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170604_133553, Accuracy: 60.25 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 33 20170604_133553\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170604_134046, Accuracy: 81.55 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170604_134326, Accuracy: 53.78 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 34 20170604_134326\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170605_110918, Accuracy: 71.34 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 35 20170605_110918\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170605_111438, Accuracy: 56.84 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 36 20170605_111438\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170605_112816, Accuracy: 47.45 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 37 20170605_112816\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170605_171921, Accuracy: 70.25 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 38 20170605_171921\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170605_172143, Accuracy: 15.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 39 20170605_172143\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170605_172358, Accuracy: 15.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 40 20170605_172358\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170605_172605, Accuracy: 25.26 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 41 20170605_172605\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170605_172838, Accuracy: 64.74 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 42 20170605_172838\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170606_172325, Accuracy: 24.85 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 43 20170606_172325\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170606_172621, Accuracy: 15.66 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 44 20170606_172621\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170606_172943, Accuracy: 2.45 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 45 20170606_172943\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170606_173231, Accuracy: 8.30 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 46 20170606_173231\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170606_173444, Accuracy: 19.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 47 20170606_173444\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170607_111519, Accuracy: 18.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 48 20170607_111519\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170607_111831, Accuracy: 17.56 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 49 20170607_111831\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170607_132942, Accuracy: 64.19 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 50 20170607_132942\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170607_134433, Accuracy: 51.12 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 51 20170607_134433\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170608_105908, Accuracy: 15.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 52 20170608_105908\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170608_110127, Accuracy: 44.52 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 53 20170608_110127\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170610_091855, Accuracy: 47.79 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 54 20170610_091855\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170610_142504, Accuracy: 79.92 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170612_115630, Accuracy: 72.57 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 55 20170612_115630\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170612_193216, Accuracy: 15.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 56 20170612_193216\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170613_171441, Accuracy: 28.25 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 57 20170613_171441\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170613_171735, Accuracy: 32.81 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 58 20170613_171735\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170613_174657, Accuracy: 30.91 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 59 20170613_174657\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170613_174902, Accuracy: 35.74 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 60 20170613_174902\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170613_175924, Accuracy: 65.15 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 61 20170613_175924\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170616_155518, Accuracy: 45.54 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 62 20170616_155518\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170616_155727, Accuracy: 47.11 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 63 20170616_155727\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170618_125006, Accuracy: 46.63 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 64 20170618_125006\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170619_181445, Accuracy: 56.57 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 65 20170619_181445\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170619_183646, Accuracy: 56.84 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 66 20170619_183646\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170620_164658, Accuracy: 28.86 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 67 20170620_164658\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170620_165204, Accuracy: 43.70 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 68 20170620_165204\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170622_142516, Accuracy: 50.44 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 69 20170622_142516\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170623_130233, Accuracy: 15.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 70 20170623_130233\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170623_131239, Accuracy: 27.50 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 71 20170623_131239\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170623_180256, Accuracy: 29.75 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 72 20170623_180256\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170623_180953, Accuracy: 19.33 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 73 20170623_180953\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170623_185445, Accuracy: 31.04 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 74 20170623_185445\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170623_185857, Accuracy: 30.09 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 75 20170623_185857\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170625_113904, Accuracy: 15.52 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 76 20170625_113904\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170625_114119, Accuracy: 24.51 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 77 20170625_114119\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170708_130614, Accuracy: 42.68 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 78 20170708_130614\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170710_093530, Accuracy: 25.19 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 79 20170710_093530\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19]\n",
      "Set: 20170424_081251, Test: 20170710_093802, Accuracy: 52.01 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 80 20170710_093802\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170710_093945, Accuracy: 45.27 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 81 20170710_093945\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170710_094211, Accuracy: 68.89 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 82 20170710_094211\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170727_160330, Accuracy: 54.94 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 83 20170727_160330\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180530_172736, Accuracy: 43.50 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 84 20180530_172736\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180602_113609, Accuracy: 47.84 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 85 20180602_113609\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180602_113944, Accuracy: 53.58 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 86 20180602_113944\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180604_172706, Accuracy: 40.71 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 87 20180604_172706\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180605_164702, Accuracy: 26.14 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 88 20180605_164702\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180605_165004, Accuracy: 31.27 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 89 20180605_165004\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180606_195649, Accuracy: 59.37 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 90 20180606_195649\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180609_120622, Accuracy: 38.64 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 91 20180609_120622\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180609_132458, Accuracy: 51.23 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 92 20180609_132458\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180611_210422, Accuracy: 28.91 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 93 20180611_210422\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180612_180710, Accuracy: 21.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 94 20180612_180710\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180612_181625, Accuracy: 52.66 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 95 20180612_181625\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180614_170623, Accuracy: 42.24 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 96 20180614_170623\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180619_174839, Accuracy: 27.91 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 97 20180619_174839\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180623_135541, Accuracy: 40.27 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 98 20180623_135541\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180625_175216, Accuracy: 48.74 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 99 20180625_175216\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180626_171859, Accuracy: 40.18 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 100 20180626_171859\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180626_180300, Accuracy: 40.41 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 101 20180626_180300\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180626_180540, Accuracy: 40.68 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 102 20180626_180540\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180627_181932, Accuracy: 44.93 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 103 20180627_181932\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180701_171739, Accuracy: 14.45 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 104 20180701_171739\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180701_172049, Accuracy: 67.80 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 105 20180701_172049\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180704_130403, Accuracy: 21.95 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 106 20180704_130403\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180706_185352, Accuracy: 31.92 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 107 20180706_185352\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180706_202732, Accuracy: 49.27 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 108 20180706_202732\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180711_181347, Accuracy: 12.51 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 109 20180711_181347\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180712_190835, Accuracy: 13.33 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 110 20180712_190835\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180714_105107, Accuracy: 25.66 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 111 20180714_105107\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180718_192200, Accuracy: 19.41 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 112 20180718_192200\n",
      "Removing [90]\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20180726_173515, Accuracy: 20.73 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "blda 112 - 0\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_081251, Test: 20170424_082640, Accuracy: 40.44 , Val: 85.57 , Prev: 0.00 , Train: 85.57\n",
      "recal: 1 20170424_082640\n",
      "train dof: [ 1  6 16 19 48]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_082640, Test: 20170424_085656, Accuracy: 76.51 , Val: 56.87 , Prev: 74.17 , Train: 91.55\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_082640, Test: 20170424_091501, Accuracy: 69.64 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20170424_091501\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_091501, Test: 20170424_094328, Accuracy: 72.43 , Val: 72.55 , Prev: 75.43 , Train: 93.04\n",
      "recal: 3 20170424_094328\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_094328, Test: 20170424_104554, Accuracy: 56.91 , Val: 69.97 , Prev: 76.58 , Train: 87.59\n",
      "recal: 4 20170424_104554\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_104554, Test: 20170424_114643, Accuracy: 57.52 , Val: 58.62 , Prev: 77.40 , Train: 83.88\n",
      "recal: 5 20170424_114643\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_114643, Test: 20170424_142122, Accuracy: 64.26 , Val: 61.22 , Prev: 77.50 , Train: 81.61\n",
      "recal: 6 20170424_142122\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170424_142122, Test: 20170427_161648, Accuracy: 48.94 , Val: 64.31 , Prev: 77.88 , Train: 81.83\n",
      "recal: 7 20170427_161648\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170427_161648, Test: 20170427_191711, Accuracy: 79.78 , Val: 77.82 , Prev: 77.50 , Train: 82.97\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170427_161648, Test: 20170428_151357, Accuracy: 35.13 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 8 20170428_151357\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170428_151357, Test: 20170428_151659, Accuracy: 62.15 , Val: 68.84 , Prev: 77.41 , Train: 75.61\n",
      "recal: 9 20170428_151659\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170428_151659, Test: 20170430_144524, Accuracy: 60.93 , Val: 67.21 , Prev: 79.18 , Train: 83.79\n",
      "recal: 10 20170430_144524\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170430_144524, Test: 20170430_144857, Accuracy: 64.26 , Val: 63.13 , Prev: 78.37 , Train: 90.33\n",
      "recal: 11 20170430_144857\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170430_144857, Test: 20170430_151517, Accuracy: 53.78 , Val: 65.99 , Prev: 77.01 , Train: 79.29\n",
      "recal: 12 20170430_151517\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170430_151517, Test: 20170501_140917, Accuracy: 74.81 , Val: 80.30 , Prev: 77.69 , Train: 83.63\n",
      "recal: 13 20170501_140917\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170501_140917, Test: 20170501_141731, Accuracy: 85.77 , Val: 81.25 , Prev: 80.84 , Train: 88.81\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170501_140917, Test: 20170501_142024, Accuracy: 45.95 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 14 20170501_142024\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170501_142024, Test: 20170502_071722, Accuracy: 63.10 , Val: 46.33 , Prev: 80.43 , Train: 65.48\n",
      "recal: 15 20170502_071722\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_071722, Test: 20170502_072300, Accuracy: 8.51 , Val: 60.68 , Prev: 79.76 , Train: 71.80\n",
      "recal: 16 20170502_072300\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_072300, Test: 20170502_073440, Accuracy: 73.04 , Val: 63.81 , Prev: 79.48 , Train: 75.61\n",
      "recal: 17 20170502_073440\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_073440, Test: 20170502_075048, Accuracy: 63.38 , Val: 71.29 , Prev: 79.21 , Train: 79.43\n",
      "recal: 18 20170502_075048\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_075048, Test: 20170502_092225, Accuracy: 76.72 , Val: 49.46 , Prev: 77.99 , Train: 79.81\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_075048, Test: 20170502_092652, Accuracy: 71.20 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 19 20170502_092652\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_092652, Test: 20170502_170413, Accuracy: 35.81 , Val: 70.61 , Prev: 78.26 , Train: 85.56\n",
      "recal: 20 20170502_170413\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170502_170413, Test: 20170503_160818, Accuracy: 66.03 , Val: 75.10 , Prev: 77.85 , Train: 90.46\n",
      "recal: 21 20170503_160818\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170503_160818, Test: 20170503_161240, Accuracy: 81.76 , Val: 74.97 , Prev: 71.43 , Train: 82.15\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170503_160818, Test: 20170503_165907, Accuracy: 68.07 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 22 20170503_165907\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170503_165907, Test: 20170504_105348, Accuracy: 68.21 , Val: 71.56 , Prev: 73.74 , Train: 71.25\n",
      "recal: 23 20170504_105348\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170504_105348, Test: 20170504_105753, Accuracy: 81.35 , Val: 76.46 , Prev: 71.02 , Train: 77.11\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170504_105348, Test: 20170504_111151, Accuracy: 74.27 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 24 20170504_111151\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170504_111151, Test: 20170506_131247, Accuracy: 77.06 , Val: 71.29 , Prev: 78.10 , Train: 84.06\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170504_111151, Test: 20170531_164633, Accuracy: 87.13 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170504_111151, Test: 20170531_170707, Accuracy: 43.84 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 25 20170531_170707\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170531_170707, Test: 20170531_174743, Accuracy: 92.10 , Val: 64.63 , Prev: 76.46 , Train: 65.26\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170531_170707, Test: 20170602_095356, Accuracy: 55.41 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 26 20170602_095356\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170602_095356, Test: 20170602_095749, Accuracy: 91.29 , Val: 43.67 , Prev: 75.37 , Train: 74.52\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 48]\n",
      "Set: 20170602_095356, Test: 20170602_141330, Accuracy: 66.64 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 27 20170602_141330\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170602_141330, Test: 20170602_141548, Accuracy: 71.61 , Val: 72.83 , Prev: 65.17 , Train: 69.99\n",
      "recal: 28 20170602_141548\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19]\n",
      "Set: 20170602_141548, Test: 20170604_133359, Accuracy: 88.70 , Val: 80.14 , Prev: 76.60 , Train: 91.14\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170602_141548, Test: 20170604_133553, Accuracy: 74.47 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 29 20170604_133553\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170604_133553, Test: 20170604_134046, Accuracy: 92.24 , Val: 76.36 , Prev: 81.09 , Train: 78.31\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170604_133553, Test: 20170604_134326, Accuracy: 56.36 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 30 20170604_134326\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170604_134326, Test: 20170605_110918, Accuracy: 74.20 , Val: 60.05 , Prev: 77.45 , Train: 57.98\n",
      "recal: 31 20170605_110918\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_110918, Test: 20170605_111438, Accuracy: 66.51 , Val: 73.23 , Prev: 76.22 , Train: 78.17\n",
      "recal: 32 20170605_111438\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_111438, Test: 20170605_112816, Accuracy: 70.80 , Val: 73.10 , Prev: 76.36 , Train: 77.90\n",
      "recal: 33 20170605_112816\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_112816, Test: 20170605_171921, Accuracy: 82.23 , Val: 79.86 , Prev: 76.77 , Train: 76.98\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_112816, Test: 20170605_172143, Accuracy: 67.53 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 34 20170605_172143\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_172143, Test: 20170605_172358, Accuracy: 70.05 , Val: 71.84 , Prev: 80.54 , Train: 69.75\n",
      "recal: 35 20170605_172358\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_172358, Test: 20170605_172605, Accuracy: 73.59 , Val: 67.48 , Prev: 79.59 , Train: 78.34\n",
      "recal: 36 20170605_172605\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_172605, Test: 20170605_172838, Accuracy: 79.99 , Val: 69.80 , Prev: 78.23 , Train: 78.20\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170605_172605, Test: 20170606_172325, Accuracy: 26.48 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 37 20170606_172325\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170606_172325, Test: 20170606_172621, Accuracy: 27.37 , Val: 51.70 , Prev: 77.69 , Train: 45.78\n",
      "recal: 38 20170606_172621\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170606_172621, Test: 20170606_172943, Accuracy: 25.73 , Val: 44.90 , Prev: 76.60 , Train: 41.01\n",
      "recal: 39 20170606_172943\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170606_172943, Test: 20170606_173231, Accuracy: 55.75 , Val: 62.31 , Prev: 75.51 , Train: 65.94\n",
      "recal: 40 20170606_173231\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170606_173231, Test: 20170606_173444, Accuracy: 36.83 , Val: 59.32 , Prev: 73.88 , Train: 60.35\n",
      "recal: 41 20170606_173444\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170606_173444, Test: 20170607_111519, Accuracy: 41.25 , Val: 47.89 , Prev: 73.33 , Train: 68.26\n",
      "recal: 42 20170607_111519\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170607_111519, Test: 20170607_111831, Accuracy: 75.49 , Val: 63.27 , Prev: 67.89 , Train: 72.34\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170607_111519, Test: 20170607_132942, Accuracy: 86.11 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170607_111519, Test: 20170607_134433, Accuracy: 68.89 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 43 20170607_134433\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170607_134433, Test: 20170608_105908, Accuracy: 32.88 , Val: 66.17 , Prev: 67.35 , Train: 74.76\n",
      "recal: 44 20170608_105908\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170608_105908, Test: 20170608_110127, Accuracy: 70.32 , Val: 75.78 , Prev: 67.35 , Train: 69.62\n",
      "recal: 45 20170608_110127\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170608_110127, Test: 20170610_091855, Accuracy: 63.04 , Val: 77.17 , Prev: 75.65 , Train: 66.85\n",
      "recal: 46 20170610_091855\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170610_091855, Test: 20170610_142504, Accuracy: 69.71 , Val: 61.90 , Prev: 77.17 , Train: 68.26\n",
      "recal: 47 20170610_142504\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170610_142504, Test: 20170612_115630, Accuracy: 77.60 , Val: 73.91 , Prev: 76.90 , Train: 72.31\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170610_142504, Test: 20170612_193216, Accuracy: 15.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 48 20170612_193216\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170612_193216, Test: 20170613_171441, Accuracy: 61.13 , Val: 48.30 , Prev: 76.22 , Train: 59.54\n",
      "recal: 49 20170613_171441\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170613_171441, Test: 20170613_171735, Accuracy: 50.58 , Val: 68.30 , Prev: 75.82 , Train: 58.58\n",
      "recal: 50 20170613_171735\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170613_171735, Test: 20170613_174657, Accuracy: 53.37 , Val: 52.65 , Prev: 75.54 , Train: 55.04\n",
      "recal: 51 20170613_174657\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170613_174657, Test: 20170613_174902, Accuracy: 73.86 , Val: 58.50 , Prev: 75.68 , Train: 69.07\n",
      "recal: 52 20170613_174902\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170613_174902, Test: 20170613_175924, Accuracy: 73.04 , Val: 67.35 , Prev: 75.54 , Train: 82.70\n",
      "recal: 53 20170613_175924\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170613_175924, Test: 20170616_155518, Accuracy: 50.99 , Val: 74.15 , Prev: 74.86 , Train: 80.11\n",
      "recal: 54 20170616_155518\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170616_155518, Test: 20170616_155727, Accuracy: 64.87 , Val: 50.75 , Prev: 74.73 , Train: 64.17\n",
      "recal: 55 20170616_155727\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170616_155727, Test: 20170618_125006, Accuracy: 59.63 , Val: 68.21 , Prev: 74.46 , Train: 71.90\n",
      "recal: 56 20170618_125006\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170618_125006, Test: 20170619_181445, Accuracy: 72.70 , Val: 60.87 , Prev: 75.14 , Train: 72.31\n",
      "recal: 57 20170619_181445\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170619_181445, Test: 20170619_183646, Accuracy: 66.85 , Val: 75.82 , Prev: 74.86 , Train: 73.67\n",
      "recal: 58 20170619_183646\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170619_183646, Test: 20170620_164658, Accuracy: 49.83 , Val: 63.99 , Prev: 76.77 , Train: 73.12\n",
      "recal: 59 20170620_164658\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170620_164658, Test: 20170620_165204, Accuracy: 75.15 , Val: 60.27 , Prev: 76.09 , Train: 61.31\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170620_164658, Test: 20170622_142516, Accuracy: 65.01 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 60 20170622_142516\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170622_142516, Test: 20170623_130233, Accuracy: 15.38 , Val: 61.90 , Prev: 76.49 , Train: 73.71\n",
      "recal: 61 20170623_130233\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_130233, Test: 20170623_131239, Accuracy: 30.16 , Val: 28.16 , Prev: 73.23 , Train: 53.00\n",
      "recal: 62 20170623_131239\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_131239, Test: 20170623_180256, Accuracy: 32.40 , Val: 35.10 , Prev: 73.78 , Train: 44.28\n",
      "recal: 63 20170623_180256\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_180256, Test: 20170623_180953, Accuracy: 45.13 , Val: 54.97 , Prev: 73.78 , Train: 61.31\n",
      "recal: 64 20170623_180953\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_180953, Test: 20170623_185445, Accuracy: 26.55 , Val: 49.93 , Prev: 74.32 , Train: 60.76\n",
      "recal: 65 20170623_185445\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_185445, Test: 20170623_185857, Accuracy: 49.22 , Val: 32.52 , Prev: 73.37 , Train: 61.44\n",
      "recal: 66 20170623_185857\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170623_185857, Test: 20170625_113904, Accuracy: 46.97 , Val: 57.96 , Prev: 73.51 , Train: 67.30\n",
      "recal: 67 20170625_113904\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170625_113904, Test: 20170625_114119, Accuracy: 39.55 , Val: 53.74 , Prev: 73.78 , Train: 46.32\n",
      "recal: 68 20170625_114119\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170625_114119, Test: 20170708_130614, Accuracy: 73.93 , Val: 56.05 , Prev: 74.05 , Train: 37.33\n",
      "recal: 69 20170708_130614\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170708_130614, Test: 20170710_093530, Accuracy: 46.90 , Val: 70.38 , Prev: 74.32 , Train: 78.72\n",
      "recal: 70 20170710_093530\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19]\n",
      "Set: 20170710_093530, Test: 20170710_093802, Accuracy: 50.10 , Val: 60.82 , Prev: 73.91 , Train: 57.36\n",
      "recal: 71 20170710_093802\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170710_093802, Test: 20170710_093945, Accuracy: 38.67 , Val: 55.24 , Prev: 69.29 , Train: 61.58\n",
      "recal: 72 20170710_093945\n",
      "Missing classes\n",
      "Missing classes\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n",
      "bad recal\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170710_093945, Test: 20170710_094211, Accuracy: 81.89 , Val: 46.26 , Prev: 73.78 , Train: 53.27\n",
      "Missing classes\n",
      "test dof: [ 1  6 16 19 48]\n",
      "Set: 20170710_093945, Test: 20170727_160330, Accuracy: 76.72 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "test dof: [ 1  6 16 19 48 90]\n",
      "Set: 20170710_093945, Test: 20180530_172736, Accuracy: 65.49 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 73 20180530_172736\n",
      "train dof: [ 1  6 16 19 48 90]\n",
      "(5,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    372\u001b[0m     w, c, mu_class, _, _, N, cov_class \u001b[39m=\u001b[39m dlda\u001b[39m.\u001b[39mtrain_lda(x_train_lda, y_train_lda)\n\u001b[0;32m    373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     w, c, mu_class, cov_class, N \u001b[39m=\u001b[39m dlda\u001b[39m.\u001b[39;49mupdate_lda(x_train_lda, y_train_lda, N, mu_class, cov_class)\n\u001b[0;32m    375\u001b[0m all_times[i,mod_tot\u001b[39m.\u001b[39mindex(mod)] \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    377\u001b[0m acc_val[i,:] \u001b[39m=\u001b[39m lp\u001b[39m.\u001b[39mtest_models(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, x_val_lda, y_val_lda, lda\u001b[39m=\u001b[39m[w,c])\n",
      "File \u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\ml\\lda.py:83\u001b[0m, in \u001b[0;36mupdate_lda\u001b[1;34m(data, label, N, mu_class, cov_class)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/ml/lda.py?line=80'>81</a>\u001b[0m ind \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(label \u001b[39m==\u001b[39m u_class[i])\n\u001b[0;32m     <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/ml/lda.py?line=81'>82</a>\u001b[0m N_new[i] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(ind)\n\u001b[1;32m---> <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/ml/lda.py?line=82'>83</a>\u001b[0m ALPHA[i] \u001b[39m=\u001b[39m N[i] \u001b[39m/\u001b[39m (N[i] \u001b[39m+\u001b[39m N_new[i])\n\u001b[0;32m     <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/ml/lda.py?line=83'>84</a>\u001b[0m zero_mean_feats_old \u001b[39m=\u001b[39m data[ind,\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39m-\u001b[39m mu_class[i,\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]                                    \u001b[39m# De-mean based on old mean value\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/ml/lda.py?line=84'>85</a>\u001b[0m mu_class[i,\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39m=\u001b[39m ALPHA[i] \u001b[39m*\u001b[39m mu_class[i,\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m ALPHA[i]) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmean(data[ind,\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)                       \u001b[39m# Update the mean vector\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn2','acnn03','acnn30','acewc30','acewc15', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn', 'avcnn15', 'acnnl03','crvcnn','acewclm','crcnn','acewc00','xtra2']\n",
    "ft = 'tdar'\n",
    "iter = 1\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_mod = 0\n",
    "\n",
    "for it in range(iter):\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    if it == 0:\n",
    "        mod_all = ['lda','blda','alda','bcnn','cnn','acnn03','avcnn']#,'crcnn']\n",
    "        # mod_all = ['avcnn']\n",
    "    else:\n",
    "        mod_all = ['bcnn','cnn','acnn03','avcnn','crcnn']\n",
    "\n",
    "    for sub in range(3,7):\n",
    "        print(subs[sub])\n",
    "        sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "        all_files = os.listdir(sub_path)\n",
    "        if 'skip' in all_files:\n",
    "            all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "        # load or initialize cnn weights\n",
    "        if load_mod:\n",
    "            with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                all_acc, all_recal, all_val, all_prev, all_train, all_times, _, _, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        else:\n",
    "            c_weights = None\n",
    "            v_weights = None\n",
    "            v_wc = None\n",
    "            cl_wc = None\n",
    "            scaler_0 = None\n",
    "            all_recal = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "        mod_i = 0\n",
    "        for mod in mod_all:\n",
    "            acc = np.zeros((len(all_files),2))\n",
    "            acc_val = np.zeros((len(all_files),2))\n",
    "            acc_prev = np.zeros((len(all_files),2))\n",
    "            acc_train = np.zeros((len(all_files),2))\n",
    "            mod_recal = np.zeros((len(all_files),))\n",
    "\n",
    "            if 'lda' in mod:\n",
    "                acc_i = 0\n",
    "            else:\n",
    "                acc_i = 1\n",
    "\n",
    "            cnn = None\n",
    "            ewc = None\n",
    "            clda = None\n",
    "\n",
    "            ep = 30\n",
    "            recal = 0\n",
    "            skip_recal = 0\n",
    "            skip = False\n",
    "\n",
    "            # Loop through files\n",
    "            for i in range(1,len(all_files)-1):\n",
    "                # Check if need to recalibrate\n",
    "                if i > 1:\n",
    "                    if acc[i,acc_i] < 75:\n",
    "                        skip = False\n",
    "                        recal += 1\n",
    "                        print('recal: ' + str(recal) + ' ' + all_files[i])\n",
    "                        acc[i,acc_i] *= -1\n",
    "                        mod_recal[i] = 1\n",
    "                    else:\n",
    "                        skip = True\n",
    "                    \n",
    "                    if 'b' in mod:\n",
    "                        skip = True\n",
    "                        \n",
    "                if not skip:\n",
    "                    # load training file\n",
    "                    train_file = all_files[i]\n",
    "                    train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "                    # if first train, use two train files\n",
    "                    if i == 1:\n",
    "                        train_data2, train_params2 = prd.load_caps_train(sub_path + all_files[i-1] + '/traindata.mat')\n",
    "                        train_data = np.vstack((train_data,train_data2))\n",
    "                        train_params = np.vstack((train_params,train_params2))\n",
    "                        del train_data2, train_params2\n",
    "\n",
    "                        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "                        val_data = train_data\n",
    "                        val_params = train_params\n",
    "\n",
    "                        # tr_i = np.zeros((train_params.shape[0],))\n",
    "                        # te_i = np.zeros((train_params.shape[0],))\n",
    "                        # for cls in np.unique(train_params[:,-1]):\n",
    "                        #     dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                        #     tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                        #     te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        # train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        # params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        # val_data = train_data[te_i.astype(bool),...]\n",
    "                        # val_params = train_params[te_i.astype(bool),...]\n",
    "\n",
    "                        # train_data, train_params = train_temp, params_temp\n",
    "\n",
    "                        # del train_temp, params_temp, tr_i, te_i\n",
    "                    else:\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                    \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "\n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "\n",
    "                        del train_temp, params_temp, tr_i, te_i\n",
    "                        \n",
    "                    # if combining, save current training data\n",
    "                    # if 'cr' in mod:\n",
    "                    #     # combine old and new training data\n",
    "                    #     if i > 1:\n",
    "                    #         train_data = np.vstack((train_data_0,train_data))\n",
    "                    #         train_params = np.vstack((train_params_0,train_params))\n",
    "\n",
    "                    #     train_data_0 = cp.deepcopy(train_data)\n",
    "                    #     train_params_0 = cp.deepcopy(train_params)\n",
    "\n",
    "                    if (i == 1 and mod[0] == 'a') or (i == 1 and mod[:2] == 'cr') or (mod[0] != 'a' and mod[:2] != 'cr'):\n",
    "                        if i > 1:\n",
    "                            prev_ndof = [n_dof, key, train_dof]\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        key = np.empty(train_dof.shape)\n",
    "                        for key_i in range(len(train_dof)):\n",
    "                            key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "                        n_dof = int(np.max(key))\n",
    "                        if mod[0] == 'a' and 'cnn' in mod:\n",
    "                            test_key = True\n",
    "                        else:\n",
    "                            test_key = False\n",
    "                    else:\n",
    "                        test_key = True\n",
    "\n",
    "                    train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key,test_key)\n",
    "                    val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key,test_key)\n",
    "\n",
    "                    for cl in train_dof:\n",
    "                        train_params[train_params[:,-1] == cl,0] = key[train_dof==cl]\n",
    "                        val_params[val_params[:,-1] == cl,0] = key[train_dof==cl]\n",
    "\n",
    "                    print('train dof:' ,end=' ')\n",
    "                    print(train_dof)\n",
    "\n",
    "                    if (mod[0] == 'a' and i > 1) or ('cr' in mod and i > 1) or (mod == 'vcnn' and i > 1):\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, _, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale, scaler=scaler, num_classes=n_dof, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False)\n",
    "                    else:\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "                        if ((i == 1) and (c_weights is not None)) or ((i == 1) and (v_weights is not None)):\n",
    "                            scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                    _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "                    if 'cr' in mod:\n",
    "                        # combine old and new training data\n",
    "                        if i > 1:\n",
    "                            x_clean_cnn = np.vstack((clean_data_0,x_clean_cnn))\n",
    "                            y_clean = np.vstack((clean_params_0,y_clean))\n",
    "                            x_train_cnn = np.vstack((x_clean_cnn,x_train_cnn))\n",
    "                            y_train = np.vstack((y_clean,y_train))\n",
    "\n",
    "                    del train_data, train_params, val_data, val_params\n",
    "\n",
    "                    if 'lda' not in mod:\n",
    "                        cnnlda = 'l' in mod\n",
    "                        if 'vcnn' in mod:\n",
    "                            if i == 1:\n",
    "                                # calculate normalization scale\n",
    "                                adjust = []\n",
    "                                # for cl_i in range(n_dof):\n",
    "                                #     adjust.append(np.std(x_clean_cnn[np.argmax(y_clean,axis=1)==cl_i,...],axis=0))\n",
    "                                old_y = cp.deepcopy(y_clean)\n",
    "\n",
    "                                if v_weights is None:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_clean_cnn,y_train=y_clean, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, trainable=True)\n",
    "                                    # save current autoencoder weights\n",
    "                                    b_enc_w = cp.deepcopy(cnn.enc.get_weights())\n",
    "                                    dec_w = cp.deepcopy(cnn.dec.get_weights())\n",
    "                                    v_weights = cp.deepcopy(cnn.dec.get_weights())\n",
    "                                else:\n",
    "                                    cnn = dl.VCNN(n_class = n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.add_dec(x_train_cnn[:1,...])\n",
    "                                    cnn(x_train_cnn[:2,...],np.ones((2,)),dec=True) \n",
    "                                    cnn.dec.set_weights(v_weights)\n",
    "\n",
    "                                if c_weights is None:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=[cnn], n_dof=n_dof, ep=30, dec=False, trainable=True)\n",
    "                                    c_weights = [cnn.enc.get_weights(),cnn.clf.get_weights()]\n",
    "                                else:\n",
    "                                    print('setting CNN weights')\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "\n",
    "                                if scaler_0 is None:\n",
    "                                    scaler_0 = cp.deepcopy(scaler)\n",
    "                                else:\n",
    "                                    scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                                mu_class, std_class, N = prd.update_mean(x_clean_cnn,y_clean)\n",
    "\n",
    "                                # prev_train = cp.deepcopy(x_clean_cnn)\n",
    "                                # prev_trainy = cp.deepcopy(y_clean)\n",
    "                            else:\n",
    "                                prev_w = cnn.get_weights()\n",
    "                                prev_mu = [mu_class, std_class, N]\n",
    "                                # generate old training data, same size as clean data\n",
    "\n",
    "                                x_out = cp.deepcopy(x_clean_cnn)\n",
    "                                for cl in range((y_clean).shape[1]):\n",
    "                                    # y_ind = prev_trainy[:,cl]==1\n",
    "                                    x_ind = y_clean[:,cl]==1\n",
    "                                    # x_out[x_ind,...] = np.random.normal(np.mean(prev_train[y_ind,...],axis=0), np.std(prev_train[y_ind,...],axis=0),x_clean_cnn[x_ind,...].shape)\n",
    "                                    x_out[x_ind,...] = np.random.normal(mu_class[cl], std_class[cl],x_clean_cnn[x_ind,...].shape)\n",
    "\n",
    "                                x_train_aug = np.vstack((x_out,x_train_cnn))\n",
    "                                y_train_aug = np.vstack((y_clean,y_train))\n",
    "\n",
    "                                x_clean_aug = np.vstack((x_out,x_clean_cnn))\n",
    "                                y_clean_aug = np.vstack((y_clean,y_clean))\n",
    "\n",
    "                                old_y = cp.deepcopy(y_clean_aug)\n",
    "                                mu_class, std_class, N = prd.update_mean(x_clean_cnn,y_clean,N,mu_class,std_class)\n",
    "\n",
    "                                # set new normalization scales\n",
    "                                # for cl_i in range(n_dof):\n",
    "                                #     adjust[cl_i] = np.std(x_clean_aug[np.argmax(y_clean_aug,axis=1)==cl_i,...],axis=0)\n",
    "\n",
    "                                if 'avcnn' in mod: # update whole CNN and lda weights\n",
    "                                    # ep = int(mod[-2:])\n",
    "                                    # save current classifier weights\n",
    "                                    # enc_w = cp.deepcopy(cnn.enc.get_weights())\n",
    "                                    # clf_w = cp.deepcopy(cnn.clf.get_weights())\n",
    "\n",
    "                                    # # set autoencoder weights, finetune and save new autoencoder weights\n",
    "                                    # cnn.enc.set_weights(b_enc_w)\n",
    "                                    # cnn.dec.set_weights(dec_w)\n",
    "                                    # cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_clean_aug,y_train=y_clean_aug, mod=['vcnn'], n_dof=n_dof, ep=10, dec=True, lr=0.00001, trainable=False)\n",
    "                                    # b_enc_w = cp.deepcopy(cnn.enc.get_weights())\n",
    "                                    # dec_w = cp.deepcopy(cnn.dec.get_weights())\n",
    "\n",
    "                                    # # set old classifier weights and finetune\n",
    "                                    # cnn.enc.set_weights(enc_w)\n",
    "                                    # cnn.clf.set_weights(clf_w)\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=5, dec=False, lr=0.00001, trainable=False)\n",
    "                                else:\n",
    "                                    # cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_clean_aug,y_train=y_clean_aug, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, trainable=True)\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=30, dec=False, trainable=True)\n",
    "\n",
    "                                    w, c, _, _, _, _, _ = dlda.train_lda(x_train_aug.reshape((x_train_aug.shape[0],-1)), np.argmax(y_train_aug,axis=1)[...,np.newaxis])\n",
    "                                \n",
    "                                # prev_train = cp.deepcopy(x_clean_aug)\n",
    "                                # prev_trainy = cp.deepcopy(y_clean_aug)\n",
    "                            \n",
    "                        else:\n",
    "                            if i == 1:\n",
    "                                if c_weights is None:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                                    c_weights = cp.deepcopy([cnn.enc.get_weights(),cnn.clf.get_weights()])\n",
    "                                    scaler_0 = cp.deepcopy(scaler)    \n",
    "                                else:\n",
    "                                    print('setting CNN weights')\n",
    "                                    cnn = dl.CNN(n_class=n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "                                    if cnnlda:\n",
    "                                        print('setting LDA weights')\n",
    "                                        w_c = cp.deepcopy(cl_wc[0].astype('float32'))\n",
    "                                        c_c = cp.deepcopy(cl_wc[1].astype('float32'))\n",
    "                                if 'ewc' in mod:\n",
    "                                    cnn = dl.EWC(n_class=n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "                                if 'ad' in mod:\n",
    "                                    cnn = dl.CNN(n_class=n_dof,adapt=True)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "\n",
    "                                if 'l' in mod:\n",
    "                                    clda = [w_c, c_c]\n",
    "                                \n",
    "                                if scaler_0 is None:\n",
    "                                    scaler_0 = cp.deepcopy(scaler)\n",
    "                                else:\n",
    "                                    scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                            else:\n",
    "                                prev_w = cnn.get_weights()\n",
    "                                if mod =='acnnlm': # update CNN encoder using lda for loss\n",
    "                                    ep = 5\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[[cnn,w_c,c_c]],cnnlda=cnnlda)\n",
    "                                elif 'adcnn' in mod: # adapt first layer only\n",
    "                                    # cnn.base.trainable=False\n",
    "                                    cnn.clf.trainable=False\n",
    "                                    ep = int(mod[-2:])\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], adapt=True, cnnlda=cnnlda, lr=0.00001)\n",
    "                                elif 'acnn' in mod: # update whole CNN and lda weights\n",
    "                                    ep = int(mod[-2:])\n",
    "                                    ep =5\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], cnnlda=cnnlda, lr=0.00001)\n",
    "                                elif mod == 'afcnnl': # update lda only \n",
    "                                    w_c, c_c = lp.train_models(x_train_lda=cnn.enc(x_train_cnn).numpy(), y_train_lda=np.argmax(y_train,axis=1)[...,np.newaxis], mod=['lda'])\n",
    "                                elif 'cnn' in mod: # recalibrate cnnlda\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                                elif mod == 'acewclm':\n",
    "                                    _, _, all_times[i,mod_tot.index(mod)] = lp.train_task(cnn, ep, 1, x_train_cnn, y_train, [x_val_cnn], [y_val], lams=[int(mod[-2:])], bat=bat, clda=[w_c,c_c], cnnlda=cnnlda)\n",
    "                                elif 'acewc' in mod:\n",
    "                                    w_c, c_c, all_times[i,mod_tot.index(mod)] = lp.train_task(cnn, 15, 1, x_train_cnn, y_train, [prev_x, x_val_cnn],[prev_y, y_val], lams=[int(mod[-2:])], bat=bat, cnnlda=cnnlda)\n",
    "                                \n",
    "                                if 'l' in mod:\n",
    "                                    clda = [w_c, c_c]\n",
    "                        del test_mod\n",
    "                        test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                        if i > 1:\n",
    "                            acc_prev[i,:] = lp.test_models(prev_x, prev_y, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_val[i,:] = lp.test_models(x_val_cnn, y_val, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_train[i,:] = lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        \n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            n_dof, key, train_dof = prev_ndof\n",
    "                            if 'vcnn' in mod:\n",
    "                                mu_class, std_class, N = prev_mu\n",
    "                                cnn = dl.VCNN(n_class = n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.add_dec(x_train_cnn[:1,...])\n",
    "                                cnn(x_train_cnn[:2,...],np.ones((2,)),dec=True) \n",
    "                            elif 'cnn' in mod:\n",
    "                                cnn = dl.CNN(n_class = n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                            cnn.set_weights(prev_w)\n",
    "                            del test_mod\n",
    "                            test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                            \n",
    "                            mod_recal[i] = -1\n",
    "                            print('bad recal')\n",
    "                        elif 'cr' in mod:\n",
    "                            clean_data_0 = cp.deepcopy(x_clean_cnn)\n",
    "                            clean_params_0 = cp.deepcopy(y_clean)\n",
    "                        if 'ewc' in mod: \n",
    "                            cnn.compute_fisher(x_train_cnn, y_train, num_samples=200, plot_diffs=False) \n",
    "                            cnn.star()\n",
    "                    else:\n",
    "                        print(N.shape)\n",
    "                        if i == 1:\n",
    "                            N = np.zeros((len(np.unique(y_train_lda)),))\n",
    "                            cov_class = np.zeros([x_train_lda.shape[1],x_train_lda.shape[1]])\n",
    "                            mu_class = np.zeros([len(np.unique(y_train_lda)),x_train_lda.shape[1]])\n",
    "                        prev_lda = [mu_class,cov_class,N]\n",
    "                        start_time = time.time()\n",
    "                        if mod[0] != 'a' or (i == 1 and mod[0] == 'a'):\n",
    "                            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda)\n",
    "                        else:\n",
    "                            w, c, mu_class, cov_class, N = dlda.update_lda(x_train_lda, y_train_lda, N, mu_class, cov_class)\n",
    "                        all_times[i,mod_tot.index(mod)] = time.time() - start_time\n",
    "\n",
    "                        acc_val[i,:] = lp.test_models(None, None, x_val_lda, y_val_lda, lda=[w,c])\n",
    "                        acc_train[i,:] = lp.test_models(None, None, x_train_lda, y_train_lda, lda=[w,c])\n",
    "                        if i > 1:\n",
    "                            acc_prev[i,:] = lp.test_models(None, None, prev_x_lda, prev_y_lda, lda=[w,c])\n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            mod_recal[i] = -1\n",
    "                            mu_class, cov_class, N = prev_lda\n",
    "                            n_dof, key, train_dof = prev_ndof\n",
    "                            print('bad recal')\n",
    "                        del x_train_lda, y_train_lda\n",
    "                    \n",
    "                    if mod_recal[i] != -1:\n",
    "                        prev_x = cp.deepcopy(x_val_cnn)\n",
    "                        prev_y = cp.deepcopy(y_val)\n",
    "                        prev_x_lda = cp.deepcopy(x_val_lda)\n",
    "                        prev_y_lda = cp.deepcopy(y_val_lda)\n",
    "                    \n",
    "                    del x_train_cnn, y_train, x_val_cnn, y_val, x_val_lda, y_val_lda, x_clean_cnn, y_clean\n",
    "                \n",
    "                # load data\n",
    "                test_file = all_files[i+1]\n",
    "                test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "                \n",
    "                # check class labels\n",
    "                test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "                test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key)\n",
    "\n",
    "                for cl in train_dof:\n",
    "                    test_params[test_params[:,-1] == cl,0] = key[train_dof==cl]\n",
    "\n",
    "                print('test dof:',end=' ')\n",
    "                print(np.unique(test_params[:,-1]))\n",
    "                # test \n",
    "                y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "                \n",
    "                # test \n",
    "                if 'lda' in mod:\n",
    "                    acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "                else:\n",
    "                    acc[i+1,:] = lp.test_models(x_test_cnn, y_test, x_test_lda, y_test_lda, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "\n",
    "                print ('Set: ' + train_file + ', Test: ' + test_file + ',', f'Accuracy: {acc[i+1,acc_i]:.2f}', f', Val: {acc_val[i,acc_i]:.2f}', f', Prev: {acc_prev[i,acc_i]:.2f}', f', Train: {acc_train[i,acc_i]:.2f}')\n",
    "                del y_test, x_test_cnn, x_test_lda, y_test_lda, test_data, test_params\n",
    "\n",
    "            all_acc[:,mod_tot.index(mod)] = acc[:,acc_i]\n",
    "            # all_recal[mod_tot.index(mod),:] = np.hstack((recal,skip_recal))\n",
    "            all_val[:,mod_tot.index(mod)] = acc_val[:,acc_i]\n",
    "            all_prev[:,mod_tot.index(mod)] = acc_prev[:,acc_i]\n",
    "            all_train[:,mod_tot.index(mod)] = acc_train[:,acc_i]\n",
    "            all_recal[:,mod_tot.index(mod)] = mod_recal\n",
    "\n",
    "            print(mod + ' ' + str(recal) + ' - ' + str(np.sum(mod_recal==-1)))\n",
    "            mod_i += 1\n",
    "\n",
    "            # if 'cr' in mod:\n",
    "            #     del train_data_0, train_params_0\n",
    "\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','wb') as f:\n",
    "            pickle.dump([all_acc, all_recal, all_val, all_prev, all_train, all_times, mod_all, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale],f)\n",
    "        \n",
    "        gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn2','acnn03','acnn30','acewc30','acewc15', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn', 'avcnn15', 'acnnl03','crvcnn','acewclm','crcnn','acewc00','xtra2']\n",
    "ft = 'tdar'\n",
    "iter = 5\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "\n",
    "for it in range(iter):\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    if it == 0:\n",
    "        mod_all = ['blda','alda','bcnn','lda','cnn','acnn03','avcnn','crcnn']\n",
    "        # mod_all = ['avcnn']\n",
    "    else:\n",
    "        mod_all = ['bcnn','cnn','acnn03','avcnn','crcnn']\n",
    "\n",
    "    for sub in range(7):\n",
    "        print(subs[sub])\n",
    "        sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "        all_files = os.listdir(sub_path)\n",
    "        if 'skip' in all_files:\n",
    "            all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "        # load or initialize cnn weights\n",
    "        if load_mod:\n",
    "            with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                all_acc, all_recal, all_val, all_prev, all_train, all_times, _, _, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        else:\n",
    "            c_weights = None\n",
    "            v_weights = None\n",
    "            v_wc = None\n",
    "            cl_wc = None\n",
    "            scaler_0 = None\n",
    "            # all_recal = np.empty((len(mod_tot),len(mod_tot)))\n",
    "            # all_recal[:] = np.nan\n",
    "            all_recal = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "        mod_i = 0\n",
    "        for mod in mod_all:\n",
    "            acc = np.zeros((len(all_files),2))\n",
    "            acc_val = np.zeros((len(all_files),2))\n",
    "            acc_prev = np.zeros((len(all_files),2))\n",
    "            acc_train = np.zeros((len(all_files),2))\n",
    "            mod_recal = np.zeros((len(all_files),))\n",
    "\n",
    "            if 'lda' in mod:\n",
    "                acc_i = 0\n",
    "            else:\n",
    "                acc_i = 1\n",
    "\n",
    "            cnn = None\n",
    "            ewc = None\n",
    "            clda = None\n",
    "\n",
    "            ep = 30\n",
    "            recal = 0\n",
    "            skip_recal = 0\n",
    "            skip = False\n",
    "\n",
    "            # Loop through files\n",
    "            for i in range(0,len(all_files)-1):\n",
    "                # Check if need to recalibrate\n",
    "                if i > 0:\n",
    "                    if acc[i,acc_i] < 75:\n",
    "                        skip = False\n",
    "                        recal += 1\n",
    "                        print('recal: ' + str(recal) + ' ' + all_files[i])\n",
    "                        acc[i,acc_i] *= -1\n",
    "                        mod_recal[i] = 1\n",
    "                    else:\n",
    "                        skip = True\n",
    "                    \n",
    "                    if 'b' in mod:\n",
    "                        skip = True\n",
    "                        \n",
    "                if not skip:\n",
    "                    # load training file\n",
    "                    train_file = all_files[i]\n",
    "                    train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "                    # if first train, use two train files\n",
    "                    if i == 0:\n",
    "                        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "                    else:\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                    \n",
    "                    tr_i = np.zeros((train_params.shape[0],))\n",
    "                    te_i = np.zeros((train_params.shape[0],))\n",
    "                    for cls in np.unique(train_params[:,-1]):\n",
    "                        dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                        tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                        te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                    train_temp = train_data[tr_i.astype(bool),...]\n",
    "                    params_temp = train_params[tr_i.astype(bool),...]\n",
    "                    val_data = train_data[te_i.astype(bool),...]\n",
    "                    val_params = train_params[te_i.astype(bool),...]\n",
    "\n",
    "                    train_data, train_params = train_temp, params_temp\n",
    "\n",
    "                    del train_temp, params_temp, tr_i, te_i\n",
    "\n",
    "                    if (i == 0 and mod[0] == 'a') or (i == 0 and mod[:2] == 'cr') or (mod[0] != 'a' and mod[:2] != 'cr'):\n",
    "                        if i > 0:\n",
    "                            prev_ndof = n_dof\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        key = np.empty(train_dof.shape)\n",
    "                        for key_i in range(len(train_dof)):\n",
    "                            key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "                        n_dof = int(np.max(key))\n",
    "\n",
    "                    train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "                    val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key)\n",
    "\n",
    "                    for cl in train_dof:\n",
    "                        train_params[train_params[:,-1] == cl,0] = key[train_dof==cl]\n",
    "                        val_params[val_params[:,-1] == cl,0] = key[train_dof==cl]\n",
    "\n",
    "                    print('train dof:' ,end=' ')\n",
    "                    print(train_dof)\n",
    "\n",
    "                    if (mod[0] == 'a' and i > 0) or ('cr' in mod and i > 0) or (mod == 'vcnn' and i > 0):\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, _, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale, scaler=scaler, num_classes=n_dof, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False)\n",
    "                    else:\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "                        if ((i == 0) and (c_weights is not None)) or ((i == 0) and (v_weights is not None)):\n",
    "                            scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                    _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "                    if 'cr' in mod:\n",
    "                        # combine old and new training data\n",
    "                        if i > 0:\n",
    "                            x_clean_cnn = np.vstack((clean_data_0,x_clean_cnn))\n",
    "                            y_clean = np.vstack((clean_params_0,y_clean))\n",
    "                            x_train_cnn = np.vstack((x_clean_cnn,x_train_cnn))\n",
    "                            y_train = np.vstack((y_clean,y_train))\n",
    "\n",
    "                    del train_data, train_params, val_data, val_params\n",
    "\n",
    "                    if 'lda' not in mod:\n",
    "                        cnnlda = 'l' in mod\n",
    "                        if 'vcnn' in mod:\n",
    "                            if i == 0:\n",
    "                                # calculate normalization scale\n",
    "                                adjust = []\n",
    "\n",
    "                                old_y = cp.deepcopy(y_clean)\n",
    "\n",
    "                                if v_weights is None:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_clean_cnn,y_train=y_clean, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, trainable=True)\n",
    "                                    # save current autoencoder weights\n",
    "                                    b_enc_w = cp.deepcopy(cnn.enc.get_weights())\n",
    "                                    dec_w = cp.deepcopy(cnn.dec.get_weights())\n",
    "                                    v_weights = cp.deepcopy(cnn.dec.get_weights())\n",
    "                                else:\n",
    "                                    cnn = dl.VCNN(n_class = n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.add_dec(x_train_cnn[:1,...])\n",
    "                                    cnn(x_train_cnn[:2,...],np.ones((2,)),dec=True) \n",
    "                                    cnn.dec.set_weights(v_weights)\n",
    "\n",
    "                                if c_weights is None:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=[cnn], n_dof=n_dof, ep=30, dec=False, trainable=True)\n",
    "                                    c_weights = [cnn.enc.get_weights(),cnn.clf.get_weights()]\n",
    "                                else:\n",
    "                                    print('setting CNN weights')\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "\n",
    "                                if scaler_0 is None:\n",
    "                                    scaler_0 = cp.deepcopy(scaler)\n",
    "                                else:\n",
    "                                    scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                                mu_class, std_class, N = prd.update_mean(x_clean_cnn,y_clean)\n",
    "\n",
    "                            else:\n",
    "                                prev_w = cnn.get_weights()\n",
    "                                prev_mu = [mu_class, std_class, N]\n",
    "                                \n",
    "                                # generate old training data, same size as clean data\n",
    "\n",
    "                                x_out = cp.deepcopy(x_clean_cnn)\n",
    "                                for cl in range((y_clean).shape[1]):\n",
    "                                    x_ind = y_clean[:,cl]==1\n",
    "                                    x_out[x_ind,...] = np.random.normal(mu_class[cl], std_class[cl],x_clean_cnn[x_ind,...].shape)\n",
    "\n",
    "                                x_train_aug = np.vstack((x_out,x_train_cnn))\n",
    "                                y_train_aug = np.vstack((y_clean,y_train))\n",
    "\n",
    "                                x_clean_aug = np.vstack((x_out,x_clean_cnn))\n",
    "                                y_clean_aug = np.vstack((y_clean,y_clean))\n",
    "\n",
    "                                old_y = cp.deepcopy(y_clean_aug)\n",
    "                                mu_class, std_class, N = prd.update_mean(x_clean_cnn,y_clean,N,mu_class,std_class)\n",
    "\n",
    "\n",
    "                                if 'avcnn' in mod: # update whole CNN and lda weights\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=5, dec=False, lr=0.00001, trainable=False)\n",
    "                                else:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=30, dec=False, trainable=True)\n",
    "\n",
    "                                    w, c, _, _, _, _, _ = dlda.train_lda(x_train_aug.reshape((x_train_aug.shape[0],-1)), np.argmax(y_train_aug,axis=1)[...,np.newaxis])                            \n",
    "                        else:\n",
    "                            if i == 0:\n",
    "                                if c_weights is None:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                                    c_weights = cp.deepcopy([cnn.enc.get_weights(),cnn.clf.get_weights()])\n",
    "                                    scaler_0 = cp.deepcopy(scaler)    \n",
    "                                else:\n",
    "                                    print('setting CNN weights')\n",
    "                                    cnn = dl.CNN(n_class=n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "                                    if cnnlda:\n",
    "                                        print('setting LDA weights')\n",
    "                                        w_c = cp.deepcopy(cl_wc[0].astype('float32'))\n",
    "                                        c_c = cp.deepcopy(cl_wc[1].astype('float32'))\n",
    "                                if 'ewc' in mod:\n",
    "                                    cnn = dl.EWC(n_class=n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "                                if 'ad' in mod:\n",
    "                                    cnn = dl.CNN(n_class=n_dof,adapt=True)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "\n",
    "                                if 'l' in mod:\n",
    "                                    clda = [w_c, c_c]\n",
    "                                \n",
    "                                if scaler_0 is None:\n",
    "                                    scaler_0 = cp.deepcopy(scaler)\n",
    "                                else:\n",
    "                                    scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                            else:\n",
    "                                prev_w = cnn.get_weights()\n",
    "                                if mod =='acnnlm': # update CNN encoder using lda for loss\n",
    "                                    ep = 5\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[[cnn,w_c,c_c]],cnnlda=cnnlda)\n",
    "                                elif 'adcnn' in mod: # adapt first layer only\n",
    "                                    # cnn.base.trainable=False\n",
    "                                    cnn.clf.trainable=False\n",
    "                                    ep = int(mod[-2:])\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], adapt=True, cnnlda=cnnlda, lr=0.00001)\n",
    "                                elif 'acnn' in mod: # update whole CNN and lda weights\n",
    "                                    ep = int(mod[-2:])\n",
    "                                    ep =5\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], cnnlda=cnnlda, lr=0.00001)\n",
    "                                elif mod == 'afcnnl': # update lda only \n",
    "                                    w_c, c_c = lp.train_models(x_train_lda=cnn.enc(x_train_cnn).numpy(), y_train_lda=np.argmax(y_train,axis=1)[...,np.newaxis], mod=['lda'])\n",
    "                                elif 'cnn' in mod: # recalibrate cnnlda\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                                elif mod == 'acewclm':\n",
    "                                    _, _, all_times[i,mod_tot.index(mod)] = lp.train_task(cnn, ep, 1, x_train_cnn, y_train, [x_val_cnn], [y_val], lams=[int(mod[-2:])], bat=bat, clda=[w_c,c_c], cnnlda=cnnlda)\n",
    "                                elif 'acewc' in mod:\n",
    "                                    w_c, c_c, all_times[i,mod_tot.index(mod)] = lp.train_task(cnn, 15, 1, x_train_cnn, y_train, [prev_x, x_val_cnn],[prev_y, y_val], lams=[int(mod[-2:])], bat=bat, cnnlda=cnnlda)\n",
    "                                \n",
    "                                if 'l' in mod:\n",
    "                                    clda = [w_c, c_c]\n",
    "                        \n",
    "                        test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                        if i > 0:\n",
    "                            acc_prev[i,:] = lp.test_models(prev_x, prev_y, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_val[i,:] = lp.test_models(x_val_cnn, y_val, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_train[i,:] = lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "\n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            if 'vcnn' in mod:\n",
    "                                mu_class, std_class, N = prev_mu\n",
    "                                cnn = dl.VCNN(n_class = prev_ndof)\n",
    "                            elif 'cnn' in mod:\n",
    "                                cnn = dl.CNN(n_class = prev_ndof)\n",
    "                            cnn(x_train_cnn[:1,...])\n",
    "                            cnn.set_weights(prev_w)\n",
    "                            del test_mod\n",
    "                            test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                            mod_recal[i] = -1\n",
    "                            print('bad recal')\n",
    "                        elif 'cr' in mod:\n",
    "                            clean_data_0 = cp.deepcopy(x_clean_cnn)\n",
    "                            clean_params_0 = cp.deepcopy(y_clean)\n",
    "                        if 'ewc' in mod: \n",
    "                            cnn.compute_fisher(x_train_cnn, y_train, num_samples=200, plot_diffs=False) \n",
    "                            cnn.star()\n",
    "                    else:\n",
    "                        if i == 0:\n",
    "                            N = np.zeros((len(np.unique(y_train_lda)),))\n",
    "                            cov_class = np.zeros([x_train_lda.shape[1],x_train_lda.shape[1]])\n",
    "                            mu_class = np.zeros([len(np.unique(y_train_lda)),x_train_lda.shape[1]])\n",
    "                        prev_lda = [mu_class,cov_class,N]\n",
    "                        start_time = time.time()\n",
    "                        if mod[0] != 'a' or (i == 0 and mod[0] == 'a'):\n",
    "                            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda)\n",
    "                        else:\n",
    "                            w, c, mu_class, cov_class, N = dlda.update_lda(x_train_lda, y_train_lda, N, mu_class, cov_class)\n",
    "                        all_times[i,mod_tot.index(mod)] = time.time() - start_time\n",
    "\n",
    "                        acc_val[i,:] = lp.test_models(None, None, x_val_lda, y_val_lda, lda=[w,c])\n",
    "                        acc_train[i,:] = lp.test_models(None, None, x_train_lda, y_train_lda, lda=[w,c])\n",
    "                        if i > 0:\n",
    "                            acc_prev[i,:] = lp.test_models(None, None, prev_x_lda, prev_y_lda, lda=[w,c])\n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            mod_recal[i] = -1\n",
    "                            mu_class, cov_class, N = prev_lda\n",
    "                            print('bad recal')\n",
    "                        del x_train_lda, y_train_lda\n",
    "                    \n",
    "                    if mod_recal[i] != -1:\n",
    "                        prev_x = cp.deepcopy(x_val_cnn)\n",
    "                        prev_y = cp.deepcopy(y_val)\n",
    "                        prev_x_lda = cp.deepcopy(x_val_lda)\n",
    "                        prev_y_lda = cp.deepcopy(y_val_lda)\n",
    "                    \n",
    "                    del x_train_cnn, y_train, x_val_cnn, y_val, x_val_lda, y_val_lda, x_clean_cnn, y_clean\n",
    "                \n",
    "                # load data\n",
    "                test_file = all_files[i+1]\n",
    "                test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "                \n",
    "                # check class labels\n",
    "                test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "                test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key)\n",
    "\n",
    "                for cl in train_dof:\n",
    "                    test_params[test_params[:,-1] == cl,0] = key[train_dof==cl]\n",
    "\n",
    "                print('test dof:',end=' ')\n",
    "                print(np.unique(test_params[:,-1]))\n",
    "                # test \n",
    "                y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "                \n",
    "                # test \n",
    "                if 'lda' in mod:\n",
    "                    acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "                else:\n",
    "                    acc[i+1,:] = lp.test_models(x_test_cnn, y_test, x_test_lda, y_test_lda, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "\n",
    "                print ('Set: ' + train_file + ', Test: ' + test_file + ',', f'Accuracy: {acc[i+1,acc_i]:.2f}', f', Val: {acc_val[i,acc_i]:.2f}', f', Prev: {acc_prev[i,acc_i]:.2f}', f', Train: {acc_train[i,acc_i]:.2f}')\n",
    "                del y_test, x_test_cnn, x_test_lda, y_test_lda, test_data, test_params\n",
    "\n",
    "            all_acc[:,mod_tot.index(mod)] = acc[:,acc_i]\n",
    "            all_val[:,mod_tot.index(mod)] = acc_val[:,acc_i]\n",
    "            all_prev[:,mod_tot.index(mod)] = acc_prev[:,acc_i]\n",
    "            all_train[:,mod_tot.index(mod)] = acc_train[:,acc_i]\n",
    "            all_recal[:,mod_tot.index(mod)] = mod_recal\n",
    "\n",
    "            print(mod + ' ' + str(recal))\n",
    "            mod_i += 1\n",
    "\n",
    "        with open('0326 half init train/' + subs[sub] + '_' + str(it) + '_r_accs.p','wb') as f:\n",
    "            pickle.dump([all_acc, all_recal, all_val, all_prev, all_train, all_times, mod_all, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "ft = 'tdar'\n",
    "iter = 10\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "plot_mod = ['lda','alda','cnn','acnn03','avcnn','crcnn']\n",
    "plot_mod = ['lda','avcnn']\n",
    "for plot_i in range(1):\n",
    "    for sub in range(1):    \n",
    "        it_acc = []\n",
    "        it_recal = []\n",
    "        it_val = []\n",
    "        it_prev = []\n",
    "        it_train = []\n",
    "        it_times = []\n",
    "        for it in range(iter):\n",
    "            # load or initialize cnn weights\n",
    "            if plot_i == 1:\n",
    "                with open('0323 full run pre and post/' + subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            else:\n",
    "                with open('0325 big run/' + subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            \n",
    "            all_acc[all_acc==0] = np.nan\n",
    "            # all_recal[all_recal==0] = np.nan\n",
    "            all_val[all_val==0] = np.nan\n",
    "            all_prev[all_prev==0] = np.nan\n",
    "            all_train[all_train==0] = np.nan\n",
    "            all_times[all_times==0] = np.nan\n",
    "\n",
    "            it_acc.append(all_acc)\n",
    "            it_recal.append(all_recal)\n",
    "            it_val.append(all_val)\n",
    "            it_prev.append(all_prev)\n",
    "            it_train.append(all_train)\n",
    "            it_times.append(all_times)\n",
    "\n",
    "            it_acc[it][:,:2] = it_acc[0][:,:2]\n",
    "            it_recal[it][:2] = it_recal[0][:2]\n",
    "            it_val[it][:,:2] = it_val[0][:,:2]\n",
    "            it_prev[it][:,:2] = it_prev[0][:,:2]\n",
    "            it_train[it][:,:2] = it_train[0][:,:2]\n",
    "            it_times[it][:,:2] = it_times[0][:,:2]\n",
    "\n",
    "\n",
    "        it_acc2 = cp.deepcopy(it_acc)\n",
    "        for i in range(len(it_acc2)):\n",
    "            x = it_val[i]< 0\n",
    "            # print(x.type)\n",
    "            # print(it_acc2[i].shape)\n",
    "            # print(ave_val.shape)\n",
    "            it_acc2[i][it_acc[i]< 0]= it_val[i][it_acc[i]< 0]\n",
    "\n",
    "        ave_acc2 = np.nanmean(np.abs(np.array(it_acc2)),axis=0)\n",
    "        ave_acc = np.nanmean(np.abs(np.array(it_acc)),axis=0)\n",
    "        ave_recal = np.nanmean(np.abs(np.array(it_recal)),axis=0)\n",
    "        ave_val = np.nanmean(np.abs(np.array(it_val)),axis=0)\n",
    "        ave_prev = np.nanmean(np.abs(np.array(it_prev)),axis=0)\n",
    "        ave_train = np.nanmean(np.abs(np.array(it_train)),axis=0)\n",
    "        ave_times = np.nanmean(np.abs(np.array(it_times)),axis=0)\n",
    "\n",
    "        std_acc2 = np.nanstd(np.abs(np.array(it_acc2)),axis=0)/np.sum(~np.isnan(np.array(it_acc2)),axis=0)\n",
    "        std_acc = np.nanstd(np.abs(np.array(it_acc)),axis=0)/np.sum(~np.isnan(np.array(it_acc)),axis=0)\n",
    "        std_recal = np.nanstd(np.abs(np.array(it_recal)),axis=0)/np.sum(~np.isnan(np.array(it_recal)),axis=0)\n",
    "        std_val = np.nanstd(np.abs(np.array(it_val)),axis=0)/np.sum(~np.isnan(np.array(it_val)),axis=0)\n",
    "        std_prev = np.nanstd(np.abs(np.array(it_prev)),axis=0)/np.sum(~np.isnan(np.array(it_prev)),axis=0)\n",
    "        std_train = np.nanstd(np.abs(np.array(it_train)),axis=0)/np.sum(~np.isnan(np.array(it_train)),axis=0)\n",
    "        std_times = np.nanstd(np.abs(np.array(it_times)),axis=0)/np.sum(~np.isnan(np.array(it_times)),axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "        for mod in plot_mod:\n",
    "            plot_ind = mod_tot.index(mod)\n",
    "            # ax.plot(ave_acc[2:,plot_ind],'.-',label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            x = ~np.isnan(ave_val[:,plot_ind])\n",
    "            # ave_acc2 = cp.deepcopy(ave_acc)\n",
    "            # ave_acc[x,plot_ind] = ave_val[x,plot_ind]\n",
    "\n",
    "            ax[0].plot(ave_acc[:,plot_ind],'.-',ms=8,label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            ax[0].plot(np.squeeze(np.where(x)),ave_acc[x,plot_ind],'kx',ms=12)\n",
    "            ax[1].plot(ave_acc2[:,plot_ind],'.-',ms=8,label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            # ax.plot(np.squeeze(np.where(x)), ave_val[~np.isnan(ave_val[:,plot_ind]),plot_ind],'.-',ms=8,label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            # plt.fill_between(np.arange(ave_acc[2:,plot_ind].shape[0]),ave_acc[2:,plot_ind]-std_acc[2:,plot_ind],ave_acc[2:,plot_ind]+std_acc[2:,plot_ind],alpha=.3)\n",
    "        ax[0].legend()\n",
    "        for i in range(2):\n",
    "            ax[i].axhline(75, ls='--', color='grey')\n",
    "            ax[i].set_ylim([0,100])\n",
    "        # ax[1].axhline(75, ls='--', color='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ave_val < 75,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 128\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn','acnn03','acnn30','acewc00','acewc30', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn03', 'avcnn15', 'acnnl03','crvcnn','acewclm','xtra','xtra1','xtra2']\n",
    "ft = 'feat'\n",
    "iter = 1\n",
    "\n",
    "for sub in range(4,5):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    mod_all = ['vcnn']\n",
    "\n",
    "    # load or initialize cnn weights\n",
    "    if load_mod:\n",
    "        with open(subs[sub] + '_' + str(0) + '_r_accs.p','rb') as f:\n",
    "            all_acc, all_recal, all_val, all_prev, all_train, all_times, _, _, c_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "    else:\n",
    "        c_weights = None\n",
    "        v_weights = None\n",
    "        v_wc = None\n",
    "        cl_wc = None\n",
    "        all_recal = np.empty((len(mod_tot),1))\n",
    "        all_recal[:] = np.nan\n",
    "        all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "    mod_i = 0\n",
    "    for mod in mod_all:\n",
    "        acc = np.zeros((len(all_files),5))\n",
    "        acc_val = np.zeros((len(all_files),5))\n",
    "        acc_prev = np.zeros((len(all_files),5))\n",
    "        acc_train = np.zeros((len(all_files),5))\n",
    "\n",
    "        if 'cnn' in mod:\n",
    "            acc_i = 2\n",
    "        elif 'cewc' in mod:\n",
    "            acc_i = 4\n",
    "        elif 'lda' in mod:\n",
    "            acc_i = 0\n",
    "\n",
    "        cnn = None\n",
    "        ewc = None\n",
    "\n",
    "        ep = 50\n",
    "        recal = 0\n",
    "        skip = False\n",
    "\n",
    "        # Loop through files\n",
    "        for i in range(1,2):#len(all_files)-1):\n",
    "            # load training file\n",
    "            train_file = all_files[i]\n",
    "            train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "            train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "            val_data = train_data\n",
    "            val_params = train_params\n",
    "\n",
    "            train_dof = np.unique(train_params[:,-1])\n",
    "            key = np.empty(train_dof.shape)\n",
    "            for key_i in range(len(train_dof)):\n",
    "                key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "            n_dof = int(np.max(key))\n",
    "            \n",
    "            train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "            val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key)\n",
    "\n",
    "            _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "\n",
    "            _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "            del train_data, train_params, val_data, val_params\n",
    "\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, print_b=True)\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=[cnn], n_dof=n_dof, ep=30, dec=False,print_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_clean_cnn)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    x_out[y_clean[:,cl]==1,...] = np.random.normal(np.mean(x_clean_cnn[y_clean[:,cl]==1,...],axis=0), np.std(x_clean_cnn[y_clean[:,cl]==1,...],axis=0),x_clean_cnn[y_clean[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_train_cnn)\n",
    "for cl in range(y_train.shape[1]):\n",
    "    x_out[y_train[:,cl]==1,...] = np.random.normal(np.mean(x_train_cnn[y_train[:,cl]==1,...],axis=0), np.std(x_train_cnn[y_train[:,cl]==1,...],axis=0),x_train_cnn[y_train[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = np.ones((1*y_clean.shape[0],8))\n",
    "x_out,_,_ = cnn.dec(samp1,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),samp=True)\n",
    "# _,x_out,_,_ = cnn(x_clean_cnn,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),dec=True)\n",
    "x_out = x_out.numpy()\n",
    "# for i in range(y_clean.shape[1]):\n",
    "#     adjust1 = np.std(x_clean_cnn[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     rescale = np.mean(adjust1)/np.mean(np.std(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0))\n",
    "#     gmean = np.mean(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     x_out[np.argmax(y_clean,axis=1)==i,...] = (x_out[np.argmax(y_clean,axis=1)==i,...] - gmean)*rescale + gmean\n",
    "# x_out = np.maximum(np.minimum(x_out,1),0)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    ind = np.tile(np.argmax(y_clean,axis=1)==cl,[1])\n",
    "    ind2 = np.argmax(y_clean,axis=1)==cl\n",
    "    x_temp = x_out[ind,...].reshape((np.sum(ind),-1))\n",
    "    x_true = x_clean_cnn[ind2,...].reshape((np.sum(ind2),-1))\n",
    "    # for i in range()\n",
    "    plt.figure()\n",
    "    # for i in range(x_true.shape[0]):\n",
    "    #     plt.plot(x_true[i,...],'k-')\n",
    "        \n",
    "    for i in range(x_temp.shape[0]):\n",
    "        plt.plot(x_temp[i,...],'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lda = x_clean_cnn.reshape(x_clean_cnn.shape[0],-1)\n",
    "y_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "y_train_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_lda,y_lda)\n",
    "y_out = dlda.predict(x_lda, w, c)\n",
    "print(dlda.eval_lda(w, c, x_lda, y_lda))\n",
    "x_out_lda = x_out.reshape(x_out.shape[0],-1)\n",
    "print(dlda.eval_lda(w,c, x_out_lda,np.tile(y_train_lda,[1,1])))\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_out_lda,np.tile(y_train_lda,[1,1]))\n",
    "print(dlda.eval_lda(w, c, x_out_lda,y_train_lda))\n",
    "print(dlda.eval_lda(w, c, x_lda, np.argmax(y_clean,axis=1)[...,np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn1,test_accuracy)\n",
    "print(lp.test_models(x_out, y_clean, None, None, cnn=cnn1, test_mod=test_mod, test_accuracy=test_accuracy))\n",
    "\n",
    "cnn2, _ = lp.train_models(traincnn=x_out,y_train=y_clean, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn2,test_accuracy)\n",
    "print(lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn2, test_mod=test_mod, test_accuracy=test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "fig,ax = plt.subplots(1,5,figsize=(30,4))\n",
    "for sub in range(2,3):#,5):\n",
    "    with open(subs[sub] + '_0_r_accs.p','rb') as f:\n",
    "        acc_all, recal_all, cur_all, prev_all, val_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "    # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "    colors =  cm.get_cmap('tab20c')\n",
    "    c = np.empty((20,4))\n",
    "    for i in range(20):\n",
    "        c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "    nn_c[0,-1] = 1\n",
    "    all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "    pt_m = ['ko-','o-','o-','o-','s','s','s','s','D']\n",
    "    nn_c = np.vstack((np.array([0,0,0,1]), c[0,:],c[1,:],c[2,:],c[3,:],c[4,:],c[5,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "    # nn_c[0,-1] = 1\n",
    "\n",
    "    labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "    labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "    # labels = mod_tot\n",
    "\n",
    "    ax_ind = sub\n",
    "    it = 0\n",
    "    for v in [1,2]: \n",
    "        i = mod_tot.index(mod_all[v])\n",
    "        acc_temp = acc_all[1:-1,i]\n",
    "        if not np.isnan(acc_temp).all():\n",
    "            x = np.arange(len(acc_temp))\n",
    "            recal_i = (acc_temp < 0)\n",
    "            ax[ax_ind].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v],color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "            it+=1\n",
    "\n",
    "    for i in range(5):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        \n",
    "        ax[i].set_ylim([0,100])\n",
    "        ax[i].set_title('TR' + str(i+1))\n",
    "    ax[0].legend()\n",
    "    ax[2].set_xlabel('Calibration Set')\n",
    "    ax[0].set_ylabel('Accuracy (%)')\n",
    "    plt.rc('font', size=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "for sub in range(2,3):#,5):\n",
    "    fig,ax = plt.subplots(1,4,figsize=(20,4))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, val_all,mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [1,0,0,1,2,2,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[it]+': ' + str(int(recal_all[i,0])),color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "                it+=1\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "cv_iter = 1\n",
    "for sub in range(0,5):\n",
    "    fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','o','*','o','s','D','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','f-cnn-5','f-cnn-3','f-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in [0, 3, 5, 4, 6, 7]: #range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v]+': ' + str(int(recal_all[i,0])),color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                it+=1\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "        \n",
    "\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e4d54467b05e62951c9fd7929782b99429e3b62c1a3b146d4f3dbf79f907e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('adapt_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
