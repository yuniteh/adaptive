{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpu import set_gpu\n",
    "import numpy as np\n",
    "import os\n",
    "import adapt.utils.data_utils as prd\n",
    "import adapt.loop as lp\n",
    "import adapt.ml.lda as dlda\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import adapt.ml.dl_subclass as dl\n",
    "import copy as cp\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "set_gpu()\n",
    "policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 3\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "path += subs[sub] + '/DATA/MAT/'\n",
    "all_files = os.listdir(path)\n",
    "if 'skip' in all_files:\n",
    "    all_files = np.delete(all_files,all_files.index('skip'))\n",
    "print(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = np.delete(all_files,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR62\n",
      "setting CNN weights\n",
      "Set: 20180515_060621, Test: 20180521_090336, Accuracy: 70.17 , Val: 93.86 , Prev: 0.00 , Train: 93.13\n",
      "Set: 20180515_060621, Test: 20180524_161811, Accuracy: 76.52 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180515_060621, Test: 20180525_084201, Accuracy: 76.21 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "Set: 20180515_060621, Test: 20180531_073149, Accuracy: 69.34 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20180531_073149\n",
      "Initial val acc 0: 0.9386, val acc 1: 0.7014\n",
      "<dtype: 'float32'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    c:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\ml\\dl_subclass.py:343 train_step  *\n        f_loss_orig = tf.reduce_sum(tf.multiply(mod.F_accum[v].astype(mod.trainable_weights[v].dtype),tf.square(mod.trainable_weights[v] - mod.star_vars[v])))\n    C:\\ProgramData\\Anaconda3\\envs\\adapt_env\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py:416 converted_call\n        return py_builtins.overload_of(f)(*args)\n\n    TypeError: Cannot interpret 'tf.float32' as a data type\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-93be26e65179>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    221\u001b[0m                                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mewc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx_val_cnn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw_c\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_c\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnnlda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcnnlda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                             \u001b[1;32melif\u001b[0m \u001b[1;34m'acewc'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m                                 \u001b[0mw_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_c\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_task\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mewc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mprev_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_val_cnn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprev_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnnlda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcnnlda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;34m'l'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\loop.py\u001b[0m in \u001b[0;36mtrain_task\u001b[1;34m(model, num_iter, disp_freq, x_train, y_train, x_test, y_test, lams, plot_loss, bat, clda, cnnlda)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_in\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[0mtrain_ewc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfish_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlam_in\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclda\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mf_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 615\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    616\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    495\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m    496\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[1;32m--> 497\u001b[1;33m             *args, **kwds))\n\u001b[0m\u001b[0;32m    498\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: in converted code:\n\n    c:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\ml\\dl_subclass.py:343 train_step  *\n        f_loss_orig = tf.reduce_sum(tf.multiply(mod.F_accum[v].astype(mod.trainable_weights[v].dtype),tf.square(mod.trainable_weights[v] - mod.star_vars[v])))\n    C:\\ProgramData\\Anaconda3\\envs\\adapt_env\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py:416 converted_call\n        return py_builtins.overload_of(f)(*args)\n\n    TypeError: Cannot interpret 'tf.float32' as a data type\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x504 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = True\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn','acnn03','acnn30','acewc00','acewc30', 'adcnn30', 'bcnnl', 'cnnl', 'crcnnl', 'acnnl03','acewcl','acnnlm','acewclm','xtra','xtra1','xtra2']\n",
    "ft = 'feat'\n",
    "iter = 1\n",
    "\n",
    "for sub in range(4,5):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    for it in range(0,iter):\n",
    "        # first iteration, includes LDA; others exclude LDA\n",
    "        if it == 0:\n",
    "            mod_all = ['acewc30']\n",
    "        else:\n",
    "            mod_all = ['cnnl','acnnl3','acnnl5','acnnl30','acewcl']\n",
    "\n",
    "        # load or initialize cnn weights\n",
    "        if load_mod:\n",
    "            with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                all_acc, all_recal, all_val, all_prev, all_train, _, _, c_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        else:\n",
    "            c_weights = None\n",
    "            cl_wc = None\n",
    "            all_recal = np.empty((len(mod_tot),1))\n",
    "            all_recal[:] = np.nan\n",
    "            all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "        mod_i = 0\n",
    "        for mod in mod_all:\n",
    "            acc = np.zeros((len(all_files),5))\n",
    "            acc_val = np.zeros((len(all_files),5))\n",
    "            acc_prev = np.zeros((len(all_files),5))\n",
    "            acc_train = np.zeros((len(all_files),5))\n",
    "\n",
    "            if 'cnn' in mod:\n",
    "                acc_i = 2\n",
    "            elif 'cewc' in mod:\n",
    "                acc_i = 4\n",
    "            elif 'lda' in mod:\n",
    "                acc_i = 0\n",
    "\n",
    "            cnn = None\n",
    "            ewc = None\n",
    "\n",
    "            ep = 30\n",
    "            recal = 0\n",
    "            skip = False\n",
    "\n",
    "            # Loop through files\n",
    "            for i in range(1,len(all_files)-1):\n",
    "                # Check if need to recalibrate\n",
    "                if i > 1:\n",
    "                    if 'b' in mod:\n",
    "                        skip = True\n",
    "                    elif acc[i,acc_i] < 70:\n",
    "                        skip = False\n",
    "                    else:\n",
    "                        skip = True\n",
    "                    \n",
    "                    if not skip:\n",
    "                        train_file = all_files[i]\n",
    "                        train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                            \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "\n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "                        _, _, _, _, _, _, x_tr, y_tr, emg_scale_tr, _, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, ft=ft, noise=False, split=False)\n",
    "\n",
    "                        _, _, _, _, _, _, x_val, y_val, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale_tr, prop_b=False, ft=ft, noise=False, split=False)\n",
    "\n",
    "                        w, c, _, _, _, _, _ = dlda.train_lda(x_tr, y_tr)\n",
    "                        skip_test = lp.test_models(None, None, x_val, None, y_val, lda=[w,c])[0]\n",
    "\n",
    "                        if skip_test > 70:\n",
    "                            skip = False\n",
    "                            recal += 1\n",
    "                            print('recal: ' + str(recal) + ' ' + all_files[i])\n",
    "                            acc[i,acc_i] *= -1\n",
    "                        else:\n",
    "                            skip = True\n",
    "                            print('skip bad set: ' + all_files[i] + ', ' + f'accuracy: {skip_test:.2f}')\n",
    "                        \n",
    "                        del skip_test, train_data, train_params, val_data, val_params, x_tr, y_tr, x_val, y_val, emg_scale_tr, train_temp, params_temp, tr_i, te_i\n",
    "\n",
    "                if not skip:\n",
    "                    # load training file\n",
    "                    train_file = all_files[i]\n",
    "                    train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "                    # if first train, use two train files\n",
    "                    if i == 1:\n",
    "                        train_data2, train_params2 = prd.load_caps_train(sub_path + all_files[i-1] + '/traindata.mat')\n",
    "                        train_data = np.vstack((train_data,train_data2))\n",
    "                        train_params = np.vstack((train_params,train_params2))\n",
    "                        del train_data2, train_params2\n",
    "\n",
    "                        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "                        val_data = train_data\n",
    "                        val_params = train_params\n",
    "                    else:\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                    \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "\n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "\n",
    "                        del train_temp, params_temp, tr_i, te_i\n",
    "\n",
    "                    if (i == 1 and mod[0] == 'a') or (mod[0] != 'a'):\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        key = np.empty(train_dof.shape)\n",
    "                        for key_i in range(len(train_dof)):\n",
    "                            key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "                        n_dof = int(np.max(key))\n",
    "                    \n",
    "                    train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "                    val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key)\n",
    "\n",
    "                    # if combining, save current training data\n",
    "                    if 'cr' in mod:\n",
    "                        # combine old and new training data\n",
    "                        if i > 1:\n",
    "                            train_data = np.vstack((train_data_0,train_data))\n",
    "                            train_params = np.vstack((train_params_0,train_params))\n",
    "\n",
    "                        train_data_0 = cp.deepcopy(train_data)\n",
    "                        train_params_0 = cp.deepcopy(train_params)\n",
    "\n",
    "                    if (mod[0] == 'a' and i > 1) or ('cr' in mod and i > 1):\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, _, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale, scaler=scaler, num_classes=n_dof, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False)\n",
    "                    else:\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "                        if (i == 1) and (c_weights is not None):\n",
    "                            scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                    _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "                    del train_data, train_params, val_data, val_params\n",
    "\n",
    "                    if 'lda' not in mod:\n",
    "                        cnnlda = 'l' in mod\n",
    "                        if i == 1:\n",
    "                            if c_weights is None:\n",
    "                                cnn, w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                                c_weights = cp.deepcopy(cnn.get_weights())\n",
    "                                scaler_0 = cp.deepcopy(scaler)    \n",
    "                                cl_wc = cp.deepcopy([w_c,c_c])\n",
    "                            else:\n",
    "                                print('setting CNN weights')\n",
    "                                cnn = dl.CNN(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.set_weights(c_weights)\n",
    "                                if cnnlda:\n",
    "                                    print('setting LDA weights')\n",
    "                                    w_c = cp.deepcopy(cl_wc[0].astype('float32'))\n",
    "                                    c_c = cp.deepcopy(cl_wc[1].astype('float32'))\n",
    "                            \n",
    "                            if 'ewc' in mod:\n",
    "                                ewc = dl.EWC(mod='CNN', n_class=n_dof)\n",
    "                                ewc(x_train_cnn[:1,...])\n",
    "                                ewc.set_weights(c_weights)\n",
    "                            if 'ad' in mod:\n",
    "                                cnn = dl.CNN(n_class=n_dof,adapt=True)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.set_weights(c_weights)\n",
    "\n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                            else:\n",
    "                                clda = None\n",
    "                        else:\n",
    "                            if mod =='acnnlm': # update CNN encoder using lda for loss\n",
    "                                ep = 5\n",
    "                                cnn, _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[[cnn,w_c,c_c]],cnnlda=cnnlda)\n",
    "                            elif 'adcnn' in mod: # adapt first layer only\n",
    "                                # cnn.base.trainable=False\n",
    "                                cnn.clf.trainable=False\n",
    "                                ep = int(mod[-2:])\n",
    "                                cnn, _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], adapt=True, cnnlda=cnnlda, lr=0.00001)\n",
    "                            elif 'acnn' in mod: # update whole CNN and lda weights\n",
    "                                ep = int(mod[-2:])\n",
    "                                cnn, w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], cnnlda=cnnlda, lr=0.00001)\n",
    "                            elif mod == 'afcnnl': # update lda only \n",
    "                                w_c, c_c = lp.train_models(x_train_lda=cnn.enc(x_train_cnn).numpy(), y_train_lda=np.argmax(y_train,axis=1)[...,np.newaxis], mod=['lda'])\n",
    "                            elif 'cnn' in mod: # recalibrate cnnlda\n",
    "                                cnn, w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                            elif mod == 'acewclm':\n",
    "                                _, _ = lp.train_task(ewc, ep, 1, x_train_cnn, y_train, [x_val_cnn], [y_val], lams=[int(mod[-2:])], bat=bat, clda=[w_c,c_c], cnnlda=cnnlda)\n",
    "                            elif 'acewc' in mod:\n",
    "                                w_c, c_c = lp.train_task(ewc, ep, 1, x_train_cnn, y_train, [prev_x, x_val_cnn],[prev_y, y_val], lams=[0,int(mod[-2:])], bat=bat, cnnlda=cnnlda)\n",
    "                            \n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                            else:\n",
    "                                clda = None\n",
    "\n",
    "                            if 'cnn' in mod:\n",
    "                                acc_prev[i,:] = lp.test_models(prev_x, None, None, prev_y, None, cnn=cnn, clda=clda)\n",
    "                            elif 'ewc' in mod: \n",
    "                                acc_prev[i,:] = lp.test_models(prev_x, None, None, prev_y, None, ewc_cnn=ewc, clda=clda)\n",
    "\n",
    "                        if 'cnn' in mod:\n",
    "                            acc_val[i,:] = lp.test_models(x_val_cnn, None, None, y_val, None, cnn=cnn, clda=clda)\n",
    "                            acc_train[i,:] = lp.test_models(x_clean_cnn, None, None, y_clean, None, cnn=cnn, clda=clda)\n",
    "                        elif 'ewc' in mod:\n",
    "                            acc_val[i,:] = lp.test_models(x_val_cnn, None, None, y_val, None, ewc_cnn=ewc, clda=clda)\n",
    "                            acc_train[i,:] = lp.test_models(x_clean_cnn, None, None, y_clean, None, ewc_cnn=ewc, clda=clda)\n",
    "                            \n",
    "                            ewc.compute_fisher(x_clean_cnn, y_clean, num_samples=200, plot_diffs=False) \n",
    "                            ewc.star()\n",
    "                    else:\n",
    "                        if mod[0] != 'a' or (i == 1 and mod[0] == 'a'):\n",
    "                            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda)\n",
    "                        else:\n",
    "                            w, c, mu_class, cov_class, N = dlda.update_lda(x_train_lda, y_train_lda, N, mu_class, cov_class)\n",
    "\n",
    "                        acc_val[i,:] = lp.test_models(None, None, x_val_lda, None, y_val_lda, lda=[w,c])\n",
    "                        acc_train[i,:] = lp.test_models(None, None, x_train_lda, None, y_train_lda, lda=[w,c])\n",
    "                        if i > 1:\n",
    "                            acc_prev[i,:] = lp.test_models(None, None, prev_x_lda, None, prev_y_lda, lda=[w,c])\n",
    "                        del x_train_lda, y_train_lda\n",
    "                    \n",
    "                    prev_x = cp.deepcopy(x_val_cnn)\n",
    "                    prev_y = cp.deepcopy(y_val)\n",
    "                    prev_x_lda = cp.deepcopy(x_val_lda)\n",
    "                    prev_y_lda = cp.deepcopy(y_val_lda)\n",
    "                    \n",
    "                    del x_train_cnn, y_train, x_val_cnn, y_val, x_val_lda, y_val_lda, x_clean_cnn, y_clean\n",
    "                \n",
    "                # load data\n",
    "                test_file = all_files[i+1]\n",
    "                test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "                \n",
    "                # check class labels\n",
    "                test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "                test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key)\n",
    "\n",
    "                # test \n",
    "                y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "\n",
    "                # test \n",
    "                if 'cnn' in mod:\n",
    "                    if 'cnnl' in mod:\n",
    "                        acc[i+1,:] = lp.test_models(x_test_cnn, None, x_test_lda, y_test, y_test_lda, cnn=cnn, clda=[w_c,c_c])\n",
    "                    else:\n",
    "                        acc[i+1,:] = lp.test_models(x_test_cnn, None, x_test_lda, y_test, y_test_lda, cnn)\n",
    "                elif 'cewc' in mod:\n",
    "                    if 'cewcl' in mod:\n",
    "                        acc[i+1,:] = lp.test_models(x_test_cnn, None, x_test_lda, y_test, y_test_lda, ewc_cnn=ewc, clda=[w_c,c_c])\n",
    "                    else:\n",
    "                        acc[i+1,:] = lp.test_models(x_test_cnn, None, x_test_lda, y_test, y_test_lda, ewc_cnn=ewc)\n",
    "                elif 'lda' in mod:\n",
    "                    acc[i+1,:] = lp.test_models(x_test_cnn, None, x_test_lda, y_test, y_test_lda, lda=[w,c])\n",
    "\n",
    "                print ('Set: ' + train_file + ', Test: ' + test_file + ',', f'Accuracy: {acc[i+1,acc_i]:.2f}', f', Val: {acc_val[i,acc_i]:.2f}', f', Prev: {acc_prev[i,acc_i]:.2f}', f', Train: {acc_train[i,acc_i]:.2f}')\n",
    "                del y_test, x_test_cnn, x_test_lda, y_test_lda, test_data, test_params\n",
    "\n",
    "            all_acc[:,mod_tot.index(mod)] = acc[:,acc_i]\n",
    "            all_recal[mod_tot.index(mod)] = recal\n",
    "            all_val[:,mod_tot.index(mod)] = acc_val[:,acc_i]\n",
    "            all_prev[:,mod_tot.index(mod)] = acc_prev[:,acc_i]\n",
    "            all_train[:,mod_tot.index(mod)] = acc_train[:,acc_i]\n",
    "            print(mod + ' ' + str(recal))\n",
    "            mod_i += 1\n",
    "\n",
    "            if 'cr' in mod:\n",
    "                del train_data_0, train_params_0\n",
    "\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','wb') as f:\n",
    "            pickle.dump([all_acc, all_recal, all_val, all_prev, all_train, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "fig,ax = plt.subplots(1,5,figsize=(30,4))\n",
    "for sub in range(2,3):#,5):\n",
    "    with open(subs[sub] + '_0_r_accs.p','rb') as f:\n",
    "        acc_all, recal_all, cur_all, prev_all, val_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "    # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "    colors =  cm.get_cmap('tab20c')\n",
    "    c = np.empty((20,4))\n",
    "    for i in range(20):\n",
    "        c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "    nn_c[0,-1] = 1\n",
    "    all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "    pt_m = ['ko-','o-','o-','o-','s','s','s','s','D']\n",
    "    nn_c = np.vstack((np.array([0,0,0,1]), c[0,:],c[1,:],c[2,:],c[3,:],c[4,:],c[5,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "    # nn_c[0,-1] = 1\n",
    "\n",
    "    labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "    labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "    # labels = mod_tot\n",
    "\n",
    "    ax_ind = sub\n",
    "    it = 0\n",
    "    for v in [1,2]: \n",
    "        i = mod_tot.index(mod_all[v])\n",
    "        acc_temp = acc_all[1:-1,i]\n",
    "        if not np.isnan(acc_temp).all():\n",
    "            x = np.arange(len(acc_temp))\n",
    "            recal_i = (acc_temp < 0)\n",
    "            ax[ax_ind].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v],color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "            it+=1\n",
    "\n",
    "    for i in range(5):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        \n",
    "        ax[i].set_ylim([0,100])\n",
    "        ax[i].set_title('TR' + str(i+1))\n",
    "    ax[0].legend()\n",
    "    ax[2].set_xlabel('Calibration Set')\n",
    "    ax[0].set_ylabel('Accuracy (%)')\n",
    "    plt.rc('font', size=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "for sub in range(2,3):#,5):\n",
    "    fig,ax = plt.subplots(1,4,figsize=(20,4))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, val_all,mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [1,0,0,1,2,2,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[it]+': ' + str(int(recal_all[i,0])),color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "                it+=1\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "cv_iter = 1\n",
    "for sub in range(0,5):\n",
    "    fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','o','*','o','s','D','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','f-cnn-5','f-cnn-3','f-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in [0, 3, 5, 4, 6, 7]: #range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v]+': ' + str(int(recal_all[i,0])),color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                it+=1\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "        \n",
    "\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e4d54467b05e62951c9fd7929782b99429e3b62c1a3b146d4f3dbf79f907e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('adapt_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
