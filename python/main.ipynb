{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpu import set_gpu\n",
    "import numpy as np\n",
    "import os\n",
    "import adapt.utils.data_utils as prd\n",
    "import adapt.loop as lp\n",
    "import adapt.ml.lda as dlda\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import adapt.ml.dl_subclass as dl\n",
    "import copy as cp\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython import display\n",
    "import gc as gc\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR62\n",
      "train dof: [ 1  6 16 19 48 90], key: [0 1 2 3 4 5]\n",
      "setting CNN weights\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180524_161811, Accuracy: 98.85 , Val: 99.12 , Prev: 0.00 , Train: 99.56\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180525_084201, Accuracy: 84.07 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180531_073149, Accuracy: 86.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180602_105936, Accuracy: 79.33 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180604_090437, Accuracy: 71.42 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20180604_090437\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.0814337730407715\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180604_214053, Accuracy: 81.26 , Val: 81.27 , Prev: 81.88 , Train: 99.79\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180605_080547, Accuracy: 78.60 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180606_174322, Accuracy: 63.25 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20180606_174322\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.9228277206420898\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180606_174322, Test: 20180610_081839, Accuracy: 75.90 , Val: 90.01 , Prev: 74.71 , Train: 99.90\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180606_174322, Test: 20180612_085047, Accuracy: 64.60 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 3 20180612_085047\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.7827818393707275\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180612_085047, Test: 20180614_081624, Accuracy: 78.29 , Val: 82.93 , Prev: 79.92 , Train: 100.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180612_085047, Test: 20180616_182930, Accuracy: 71.00 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 4 20180616_182930\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.8284540176391602\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180616_182930, Test: 20180617_184248, Accuracy: 62.21 , Val: 86.89 , Prev: 74.19 , Train: 98.96\n",
      "recal: 5 20180617_184248\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.829103708267212\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180619_193541, Accuracy: 86.62 , Val: 52.70 , Prev: 83.04 , Train: 99.58\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073139, Accuracy: 77.72 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073838, Accuracy: 73.97 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 6 20180620_073838\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.9383471012115479\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_073838, Test: 20180620_083903, Accuracy: 68.14 , Val: 86.99 , Prev: 66.39 , Train: 100.00\n",
      "recal: 7 20180620_083903\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.9615373611450195\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180621_094013, Accuracy: 85.22 , Val: 86.06 , Prev: 88.35 , Train: 98.75\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180623_200107, Accuracy: 72.20 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 8 20180623_200107\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.1025426387786865\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180703_072425, Accuracy: 78.08 , Val: 79.08 , Prev: 76.59 , Train: 100.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180711_074144, Accuracy: 74.49 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 9 20180711_074144\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.9686119556427002\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_074144, Test: 20180711_113445, Accuracy: 70.48 , Val: 79.19 , Prev: 86.37 , Train: 100.00\n",
      "recal: 10 20180711_113445\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.0106735229492188\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180713_102029, Accuracy: 74.44 , Val: 83.56 , Prev: 61.60 , Train: 98.65\n",
      "recal: 11 20180713_102029\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.9874906539916992\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180713_102029, Test: 20180717_073851, Accuracy: 88.70 , Val: 94.49 , Prev: 70.55 , Train: 99.69\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180713_102029, Test: 20180717_112511, Accuracy: 78.81 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180713_102029, Test: 20190321_073718, Accuracy: 50.07 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 12 20190321_073718\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.1838252544403076\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_073718, Test: 20190321_095657, Accuracy: 67.96 , Val: 78.03 , Prev: 88.25 , Train: 99.07\n",
      "recal: 13 20190321_095657\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.207017183303833\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_095657, Test: 20190325_084612, Accuracy: 60.22 , Val: 66.48 , Prev: 70.02 , Train: 99.35\n",
      "recal: 14 20190325_084612\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.0757339000701904\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190325_084612, Test: 20190326_073702, Accuracy: 60.39 , Val: 73.00 , Prev: 54.28 , Train: 100.00\n",
      "recal: 15 20190326_073702\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2789015769958496\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190328_125619, Accuracy: 78.50 , Val: 82.16 , Prev: 81.75 , Train: 99.07\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190331_112350, Accuracy: 88.17 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190402_073436, Accuracy: 78.71 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190402_142249, Accuracy: 74.99 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 16 20190402_142249\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2358946800231934\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190403_050414, Accuracy: 75.50 , Val: 81.94 , Prev: 80.58 , Train: 98.70\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190403_052115, Accuracy: 86.59 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190404_075938, Accuracy: 73.22 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 17 20190404_075938\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.6451573371887207\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190404_085916, Accuracy: 80.18 , Val: 74.49 , Prev: 80.45 , Train: 100.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190406_091945, Accuracy: 71.91 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 18 20190406_091945\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2488417625427246\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190410_102236, Accuracy: 79.18 , Val: 84.95 , Prev: 82.96 , Train: 99.64\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190410_102641, Accuracy: 80.07 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190415_073441, Accuracy: 85.42 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190415_102702, Accuracy: 82.26 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190423_063337, Accuracy: 79.32 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190424_045927, Accuracy: 79.60 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190502_102725, Accuracy: 83.19 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190505_082637, Accuracy: 82.53 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190505_083227, Accuracy: 73.68 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 19 20190505_083227\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.1159605979919434\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190505_083227, Test: 20190518_142954, Accuracy: 72.01 , Val: 74.30 , Prev: 85.23 , Train: 99.63\n",
      "------------------------acnn05 14 - 5 -- 0-------------\n",
      "train dof: [ 1  6 16 19 48 90], key: [0 1 2 3 4 5]\n",
      "setting CNN weights\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180524_161811, Accuracy: 98.85 , Val: 99.12 , Prev: 0.00 , Train: 99.32\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180525_084201, Accuracy: 84.07 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180531_073149, Accuracy: 86.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180602_105936, Accuracy: 79.33 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180604_090437, Accuracy: 71.42 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20180604_090437\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.9391202926635742\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180604_214053, Accuracy: 81.78 , Val: 82.93 , Prev: 83.78 , Train: 99.79\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180605_080547, Accuracy: 77.82 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180606_174322, Accuracy: 66.48 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20180606_174322\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.778130054473877\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180606_174322, Test: 20180610_081839, Accuracy: 65.33 , Val: 89.80 , Prev: 69.09 , Train: 98.96\n",
      "recal: 3 20180610_081839\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.8459982872009277\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180612_085047, Accuracy: 74.13 , Val: 81.06 , Prev: 85.02 , Train: 99.69\n",
      "recal: 4 20180612_085047\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.898493766784668\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180612_085047, Test: 20180614_081624, Accuracy: 74.80 , Val: 82.10 , Prev: 78.56 , Train: 99.90\n",
      "recal: 5 20180614_081624\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.844977855682373\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180614_081624, Test: 20180616_182930, Accuracy: 63.40 , Val: 89.28 , Prev: 77.21 , Train: 99.90\n",
      "recal: 6 20180616_182930\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.048922538757324\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180616_182930, Test: 20180617_184248, Accuracy: 62.00 , Val: 87.62 , Prev: 78.04 , Train: 98.75\n",
      "recal: 7 20180617_184248\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.318779706954956\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180619_193541, Accuracy: 86.62 , Val: 53.74 , Prev: 80.96 , Train: 98.02\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073139, Accuracy: 80.17 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073838, Accuracy: 74.23 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 8 20180620_073838\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.8520984649658203\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_073838, Test: 20180620_083903, Accuracy: 65.23 , Val: 83.66 , Prev: 66.60 , Train: 100.00\n",
      "recal: 9 20180620_083903\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.9960696697235107\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180621_094013, Accuracy: 85.74 , Val: 86.58 , Prev: 90.22 , Train: 96.67\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180623_200107, Accuracy: 68.56 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 10 20180623_200107\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 1.8737249374389648\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180703_072425, Accuracy: 79.18 , Val: 79.81 , Prev: 73.26 , Train: 96.25\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180711_074144, Accuracy: 73.92 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 11 20180711_074144\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.0766260623931885\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_074144, Test: 20180711_113445, Accuracy: 66.42 , Val: 74.71 , Prev: 84.50 , Train: 98.75\n",
      "recal: 12 20180711_113445\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.219785451889038\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180713_102029, Accuracy: 74.44 , Val: 87.62 , Prev: 67.22 , Train: 96.67\n",
      "recal: 13 20180713_102029\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.518726110458374\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180713_102029, Test: 20180717_073851, Accuracy: 85.37 , Val: 95.32 , Prev: 68.89 , Train: 98.96\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180713_102029, Test: 20180717_112511, Accuracy: 80.01 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180713_102029, Test: 20190321_073718, Accuracy: 56.26 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 14 20190321_073718\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.3188445568084717\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_073718, Test: 20190321_095657, Accuracy: 69.45 , Val: 76.82 , Prev: 87.63 , Train: 97.39\n",
      "recal: 15 20190321_095657\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.3438971042633057\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_095657, Test: 20190325_084612, Accuracy: 56.92 , Val: 65.83 , Prev: 76.26 , Train: 98.32\n",
      "recal: 16 20190325_084612\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.933122158050537\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190325_084612, Test: 20190326_073702, Accuracy: 55.79 , Val: 63.22 , Prev: 46.18 , Train: 98.79\n",
      "recal: 17 20190326_073702\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.3601343631744385\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190328_125619, Accuracy: 76.54 , Val: 81.78 , Prev: 83.24 , Train: 95.91\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190331_112350, Accuracy: 87.66 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190402_073436, Accuracy: 75.69 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190402_142249, Accuracy: 75.87 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190403_050414, Accuracy: 75.59 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190403_052115, Accuracy: 82.49 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190404_075938, Accuracy: 75.64 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190404_085916, Accuracy: 84.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190406_091945, Accuracy: 77.23 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190410_102236, Accuracy: 73.03 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 18 20190410_102236\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.35453200340271\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190410_102236, Test: 20190410_102641, Accuracy: 77.41 , Val: 88.64 , Prev: 75.74 , Train: 99.16\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190410_102236, Test: 20190415_073441, Accuracy: 88.54 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190410_102236, Test: 20190415_102702, Accuracy: 71.00 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 19 20190415_102702\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.262009859085083\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190415_102702, Test: 20190423_063337, Accuracy: 79.13 , Val: 85.31 , Prev: 82.40 , Train: 95.95\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190415_102702, Test: 20190424_045927, Accuracy: 77.50 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190415_102702, Test: 20190502_102725, Accuracy: 75.56 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190415_102702, Test: 20190505_082637, Accuracy: 69.03 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 20 20190505_082637\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.44619083404541\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190505_082637, Test: 20190505_083227, Accuracy: 65.67 , Val: 79.61 , Prev: 66.85 , Train: 100.00\n",
      "recal: 21 20190505_083227\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.274374008178711\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190505_083227, Test: 20190518_142954, Accuracy: 72.43 , Val: 75.79 , Prev: 79.80 , Train: 98.51\n",
      "------------------------avcnn 17 - 4 -- 0-------------\n"
     ]
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = True\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn2','acnn05','acnn30','acewc30','acewc15', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn', 'avcnn15', 'acnnl03','crvcnn','acewclm','crcnn','acewc00','xtra2']\n",
    "ft = 'tdar'\n",
    "iter = 10\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_mod = 0\n",
    "\n",
    "for it in range(1):#iter):\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    if it == 0:\n",
    "        mod_all = ['blda','lda','alda','cnn','acnn05','avcnn']#,'crcnn']\n",
    "        mod_all = ['lda','alda','acnn05','avcnn']\n",
    "        mod_all = ['acnn05','avcnn']\n",
    "    else:\n",
    "        mod_all = ['bcnn','cnn','acnn05','avcnn']\n",
    "\n",
    "    for sub in range(4,5):\n",
    "        print(subs[sub])\n",
    "        sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "        all_files = os.listdir(sub_path)\n",
    "        if 'skip' in all_files:\n",
    "            all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "        # load or initialize cnn weights\n",
    "        if load_mod:\n",
    "            with open(subs[sub] + '_' + str(it) + '_r_accs_d.p','rb') as f:\n",
    "                all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof,_, _, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        else:\n",
    "            c_weights = None\n",
    "            v_weights = None\n",
    "            v_wc = None\n",
    "            cl_wc = None\n",
    "            scaler_0 = None\n",
    "            all_recal = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "            rows, cols = (len(all_files), len(mod_tot))\n",
    "            all_dof = [[0]*cols]*rows\n",
    "\n",
    "        mod_i = 0\n",
    "        for mod in mod_all:\n",
    "            acc = np.zeros((len(all_files),2))\n",
    "            acc_val = np.zeros((len(all_files),2))\n",
    "            acc_prev = np.zeros((len(all_files),2))\n",
    "            acc_train = np.zeros((len(all_files),2))\n",
    "            mod_recal = np.zeros((len(all_files),))\n",
    "\n",
    "            if 'lda' in mod:\n",
    "                acc_i = 0\n",
    "            else:\n",
    "                acc_i = 1\n",
    "\n",
    "            cnn = None\n",
    "            ewc = None\n",
    "            clda = None\n",
    "\n",
    "            ep = 30\n",
    "            recal = 0\n",
    "            skip_recal = 0\n",
    "            skip = False\n",
    "\n",
    "            # Loop through files\n",
    "            for i in range(0,len(all_files)-1):\n",
    "                # Check if need to recalibrate\n",
    "                if i > 0:\n",
    "                    if acc[i,acc_i] < 75:\n",
    "                        skip = False\n",
    "                        recal += 1\n",
    "                        print('recal: ' + str(recal) + ' ' + all_files[i])\n",
    "                        acc[i,acc_i] *= -1\n",
    "                        mod_recal[i] = 1\n",
    "                    else:\n",
    "                        skip = True\n",
    "                    \n",
    "                    if 'b' in mod:\n",
    "                        skip = True\n",
    "                        \n",
    "                if not skip:\n",
    "                    # load training file\n",
    "                    train_file = all_files[i]\n",
    "                    train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "                    # if first train, use two train files\n",
    "                    if i == 0:\n",
    "                        # load training file\n",
    "                        train_file2 = all_files[i+1]\n",
    "                        train_data2, train_params2 = prd.load_caps_train(sub_path + train_file2 + '/traindata.mat')\n",
    "                        train_data = np.vstack((train_data,train_data2))\n",
    "                        train_params = np.vstack((train_params,train_params2))\n",
    "                        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "                        val_data, val_params = train_data, train_params\n",
    "                    else:\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                    \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "                        \n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "                        del train_temp, params_temp, tr_i, te_i\n",
    "                        \n",
    "                    # if combining, save current training data\n",
    "                    # if 'cr' in mod:\n",
    "                    #     # combine old and new training data\n",
    "                    #     if i > 1:\n",
    "                    #         train_data = np.vstack((train_data_0,train_data))\n",
    "                    #         train_params = np.vstack((train_params_0,train_params))\n",
    "\n",
    "                    #     train_data_0 = cp.deepcopy(train_data)\n",
    "                    #     train_params_0 = cp.deepcopy(train_params)\n",
    "\n",
    "                    # if (i == 1 and mod[0] == 'a') or (i == 1 and mod[:2] == 'cr') or (mod[0] != 'a' and mod[:2] != 'cr'):\n",
    "                        \n",
    "                    if i > 0:\n",
    "                        # get previous dofs\n",
    "                        prev_ndof = [n_dof, key, train_dof]\n",
    "                        if mod[0] == 'a':\n",
    "                            print('prev: ' + str(train_dof))\n",
    "                            # get current dofs and create key\n",
    "                            train_dof = np.unique(train_params[:,-1])\n",
    "                            print('cur: ' + str(train_dof))\n",
    "                            key = np.zeros((len(train_dof),))\n",
    "                            # check if current dofs are all in old dof list\n",
    "                            dof_ovlp = np.isin(train_dof,prev_ndof[2],assume_unique=True)\n",
    "                            temp_dof = cp.deepcopy(train_dof)\n",
    "                            # loop through dofs that are in previous dofs, set the keys\n",
    "                            for dof in train_dof[dof_ovlp]:\n",
    "                                key[train_dof==dof] = prev_ndof[1][prev_ndof[2]==dof]\n",
    "                                temp_dof[train_dof==dof] = prev_ndof[2][prev_ndof[2]==dof]\n",
    "\n",
    "                            # check if previous dofs has classes not in this set\n",
    "                            dof_xtra = ~np.isin(prev_ndof[2],temp_dof,assume_unique=True)\n",
    "                            temp_dof = np.hstack((temp_dof,prev_ndof[2][dof_xtra]))\n",
    "                            key = np.hstack((key,prev_ndof[1][dof_xtra]))\n",
    "\n",
    "                            # loop through dofs that are not in previous dofs (ie new classes), add keys\n",
    "                            key_i = 1\n",
    "                            xtra = False\n",
    "                            for dof in train_dof[~dof_ovlp]:\n",
    "                                # if 'lda' in mod:\n",
    "                                # xtra = True\n",
    "                                # key[temp_dof==dof] = np.max(key) + 1\n",
    "                                    # key_i += 1\n",
    "                                # else:\n",
    "                                    # remove extras\n",
    "                                print('removing train ' + str(dof))\n",
    "                                ind = train_params[:,-1] == dof\n",
    "                                train_params = train_params[~ind,...]\n",
    "                                train_data = train_data[~ind,...]\n",
    "                                ind = val_params[:,-1] == dof\n",
    "                                val_params = val_params[~ind,...]\n",
    "                                val_data = val_data[~ind,...]\n",
    "                                key = np.delete(key,temp_dof==dof)\n",
    "                                temp_dof = np.delete(temp_dof,temp_dof==dof)\n",
    "\n",
    "                            train_dof = cp.deepcopy(temp_dof)\n",
    "                        else:\n",
    "                            # get current dofs and create key\n",
    "                            train_dof = np.unique(train_params[:,-1])\n",
    "                            key = np.arange(len(train_dof))\n",
    "                    else:\n",
    "                        # get current dofs and create key\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        key = np.arange(len(train_dof))\n",
    "\n",
    "                    n_dof = len(train_dof)\n",
    "                    all_dof[i][mod_tot.index(mod)] = train_dof\n",
    "\n",
    "                    train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key,False)\n",
    "                    val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key,False)\n",
    "\n",
    "                    print('train dof: ' + str(train_dof) + ', key: ' + str(key))\n",
    "\n",
    "                    if (mod[0] == 'a' and i > 0) or ('cr' in mod and i > 0) or (mod == 'vcnn' and i > 0):\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale, scaler=scaler, num_classes=n_dof, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False)\n",
    "                    else:\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "                        if ((i == 0) and (c_weights is not None)) or ((i == 0) and (v_weights is not None)):\n",
    "                            scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                    _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "                    if 'cr' in mod:\n",
    "                        # combine old and new training data\n",
    "                        if i > 0:\n",
    "                            x_clean_cnn = np.vstack((clean_data_0,x_clean_cnn))\n",
    "                            y_clean = np.vstack((clean_params_0,y_clean))\n",
    "                            x_train_cnn = np.vstack((x_clean_cnn,x_train_cnn))\n",
    "                            y_train = np.vstack((y_clean,y_train))\n",
    "\n",
    "                    del train_data, train_params, val_data, val_params\n",
    "\n",
    "                    if 'lda' not in mod:\n",
    "                        cnnlda = 'l' in mod\n",
    "                        if i == 0:\n",
    "                            if c_weights is None:\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda, bn_trainable=True,bn_training=True, prog_train=True)\n",
    "                                c_weights = cp.deepcopy([cnn.enc.get_weights(),cnn.clf.get_weights()])\n",
    "                                scaler_0 = cp.deepcopy(scaler)    \n",
    "                            else:\n",
    "                                print('setting CNN weights')\n",
    "                                cnn = dl.CNN(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                                if cnnlda:\n",
    "                                    print('setting LDA weights')\n",
    "                                    w_c = cp.deepcopy(cl_wc[0].astype('float32'))\n",
    "                                    c_c = cp.deepcopy(cl_wc[1].astype('float32'))\n",
    "                            if 'ewc' in mod:\n",
    "                                cnn = dl.EWC(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                            if 'ad' in mod:\n",
    "                                cnn = dl.CNN(n_class=n_dof,adapt=True)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                            if 'av' in mod:\n",
    "                                mu_class, std_class, N = prd.set_mean(x_clean_cnn,y_clean)\n",
    "\n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                            \n",
    "                            if scaler_0 is None:\n",
    "                                scaler_0 = cp.deepcopy(scaler)\n",
    "                            else:\n",
    "                                scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                        else:\n",
    "                            prev_w = cnn.get_weights()\n",
    "                            if xtra and mod[0] == 'a':\n",
    "                                print('add neuron')\n",
    "                                cnn = dl.CNN(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                w_temp = cnn.get_weights()\n",
    "                                w_temp[:-2] = prev_w[:-2]\n",
    "                                w_temp[-2][:,:-1] = prev_w[-2]\n",
    "                                w_temp[-1][:-1] = prev_w[-1]\n",
    "                                cnn.set_weights(w_temp)\n",
    "                            if 'adcnn' in mod: # adapt first layer only\n",
    "                                # cnn.base.trainable=False\n",
    "                                cnn.clf.trainable=False\n",
    "                                ep = int(mod[-2:])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], adapt=True, cnnlda=cnnlda, lr=0.00001)\n",
    "                            elif 'avcnn' in mod:\n",
    "                                # prev_w = cnn.get_weights()\n",
    "                                # prev_w = cnn.get_weights()\n",
    "                                prev_mu = [mu_class, std_class, N]\n",
    "                                # generate old training data, same size as clean data\n",
    "                                # x_out = cp.deepcopy(x_clean_cnn)\n",
    "                                # num_y = x_out.shape[0]//prev_ndof[0]\n",
    "                                # y_gen = np.zeros((x_out.shape[0],n_dof))\n",
    "                                # for cl in prev_ndof[1]:\n",
    "                                #     cl_i = prev_ndof[1]==cl\n",
    "                                #     y_gen[int(cl*num_y):int((cl+1)*num_y),np.where(prev_ndof[1]==cl)] = 1\n",
    "                                #     x_out[int(cl*num_y):int((cl+1)*num_y),...] = np.random.normal(mu_class[cl_i], std_class[cl_i],x_clean_cnn[:num_y,...].shape)\n",
    "                                \n",
    "                                x_out = cp.deepcopy(x_clean_cnn)\n",
    "                                num_y = x_out.shape[0]//prev_ndof[0]\n",
    "                                y_gen = np.zeros((x_out.shape[0],n_dof))\n",
    "                                y_xtra = 0\n",
    "                                x_xtra = 0\n",
    "                                for cl in prev_ndof[1]:\n",
    "                                    cl_i = np.where(prev_ndof[1]==cl)\n",
    "                                    x_ind = np.squeeze(y_clean[:,cl_i]==1)\n",
    "                                    if np.sum(x_ind) > 0:\n",
    "                                        y_gen[x_ind,np.where(prev_ndof[1]==cl)] = 1\n",
    "                                        x_out[x_ind,...] = np.random.normal(mu_class[cl_i[0],...], std_class[cl_i[0],...],x_clean_cnn[x_ind,...].shape)\n",
    "                                    else:\n",
    "                                        temp = np.zeros((num_y,n_dof))\n",
    "                                        temp[:,cl_i] = 1\n",
    "                                        if isinstance(y_xtra,np.ndarray):\n",
    "                                            y_xtra = np.vstack((y_xtra,temp))\n",
    "                                            x_xtra = np.vstack((x_xtra,np.random.normal(mu_class[cl_i[0],...], std_class[cl_i[0],...],x_clean_cnn[:num_y,...].shape)))\n",
    "                                        else:\n",
    "                                            y_xtra = cp.deepcopy(temp)\n",
    "                                            x_xtra = np.random.normal(mu_class[cl_i[0],...], std_class[cl_i[0],...],x_clean_cnn[:num_y,...].shape)\n",
    "                                \n",
    "                                # for cl in prev_ndof[1].astype(int):\n",
    "                                #     cl_i = np.where(prev_ndof[1]==cl)\n",
    "                                #     x_ind = np.squeeze(y_clean[:,cl]==1)\n",
    "                                #     if np.sum(x_ind) > 0:\n",
    "                                #         y_gen[x_ind,np.where(prev_ndof[1]==cl)] = 1\n",
    "                                #         x_out[x_ind,...] = np.random.normal(mu_class[cl], std_class[cl],x_clean_cnn[x_ind,...].shape)\n",
    "                                #     else:\n",
    "                                #         temp = np.zeros((num_y,n_dof))\n",
    "                                #         temp[:,cl] = 1\n",
    "                                #         if isinstance(y_xtra,np.ndarray):\n",
    "                                #             y_xtra = np.vstack((y_xtra,temp))\n",
    "                                #             x_xtra = np.vstack((x_xtra,np.random.normal(mu_class[cl], std_class[cl],x_clean_cnn[:num_y,...].shape)))\n",
    "                                #         else:\n",
    "                                #             y_xtra = cp.deepcopy(temp)\n",
    "                                #             x_xtra = np.random.normal(mu_class[cl_i], std_class[cl],x_clean_cnn[:num_y,...].shape)\n",
    "\n",
    "                                if isinstance(y_xtra,np.ndarray):\n",
    "                                    x_out = np.vstack((x_out,x_xtra))\n",
    "                                    y_gen = np.vstack((y_gen,y_xtra))\n",
    "\n",
    "                                x_train_aug = np.vstack((x_out,x_train_cnn))\n",
    "                                y_train_aug = np.vstack((y_gen,y_train))\n",
    "\n",
    "                                x_train_m = np.vstack((x_out,x_clean_cnn))\n",
    "                                y_train_m = np.vstack((y_gen,y_clean))\n",
    "\n",
    "                                # mu_class = np.zeros((n_dof))\n",
    "                                for cl in key.astype(int):\n",
    "                                    cl_i = np.where(key==cl)\n",
    "                                    x_ind = np.squeeze(y_train_m[:,cl_i]==1)\n",
    "                                    mu_class[cl_i[0],...] = np.nanmean(x_train_m[x_ind,...])\n",
    "                                    std_class[cl_i[0],...] = np.nanstd(x_train_m[x_ind,...])\n",
    "                                # mu_class, std_class, N = prd.set_mean(x_clean_cnn,y_clean)\n",
    "\n",
    "                                # mu_class, std_class, N = prd.update_mean(x_clean_cnn,y_clean,N,mu_class,std_class,key,prev_ndof[1])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)],_,_ = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=5, dec=False, lr=0.00001, bn_training=False, bn_trainable=False, prog_train=False)\n",
    "                            elif 'acnn' in mod: # update whole CNN and lda weights\n",
    "                                ep = int(mod[-2:])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], cnnlda=cnnlda, lr=0.00001, bn_training=False, bn_trainable=False, prog_train=False)\n",
    "                            elif 'cnn' in mod: # recalibrate cnnlda\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda, bn_training=True, prog_train=True)\n",
    "                            elif 'acewc' in mod:\n",
    "                                w_c, c_c, all_times[i,mod_tot.index(mod)] = lp.train_task(cnn, 15, 1, x_train_cnn, y_train, [prev_x, x_val_cnn],[prev_y, y_val], lams=[int(mod[-2:])], bat=bat, cnnlda=cnnlda)\n",
    "                            \n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                        del test_mod\n",
    "                        test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                        if i > 0:\n",
    "                            acc_prev[i,:] = lp.test_models(prev_x, prev_y, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_val[i,:] = lp.test_models(x_val_cnn, y_val, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_train[i,:] = lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        \n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            mod_recal[i] = -1\n",
    "                            print('bad recal')\n",
    "                            # if acc_val[i,acc_i] < 75:\n",
    "                            #     if i > 0:\n",
    "                            #         n_dof, key, train_dof = prev_ndof\n",
    "                            #         if 'vcnn' in mod:\n",
    "                            #             mu_class, std_class, N = prev_mu\n",
    "                            #         cnn = dl.CNN(n_class = n_dof)\n",
    "                            #         cnn(x_train_cnn[:1,...])\n",
    "                            #         cnn.set_weights(prev_w)\n",
    "                            #         del test_mod\n",
    "                            #         test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                            # else:\n",
    "                            #     print('keeping new model')\n",
    "                            #     mod_recal[i] = -2\n",
    "                        elif 'cr' in mod:\n",
    "                            clean_data_0 = cp.deepcopy(x_clean_cnn)\n",
    "                            clean_params_0 = cp.deepcopy(y_clean)\n",
    "                        if 'ewc' in mod: \n",
    "                            cnn.compute_fisher(x_train_cnn, y_train, num_samples=200, plot_diffs=False) \n",
    "                            cnn.star()\n",
    "                    else:\n",
    "                        if i == 0:\n",
    "                            N = np.zeros((n_dof),)\n",
    "                            cov_class = np.zeros([x_train_lda.shape[1],x_train_lda.shape[1]])\n",
    "                            mu_class = np.zeros([n_dof,x_train_lda.shape[1]])\n",
    "                        prev_lda = [mu_class,cov_class,N]\n",
    "                        start_time = time.time()\n",
    "                        if mod[0] != 'a' or (i == 0 and mod[0] == 'a'):\n",
    "                            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda, key)\n",
    "                        else:\n",
    "                            w, c, mu_class, cov_class, N = dlda.update_lda(x_train_lda, y_train_lda, N, mu_class, cov_class, key, prev_ndof[1])\n",
    "                        all_times[i,mod_tot.index(mod)] = time.time() - start_time\n",
    "\n",
    "                        acc_val[i,:],out = lp.test_models(None, None, x_val_lda, y_val_lda, lda=[w,c])\n",
    "                        acc_train[i,:],out = lp.test_models(None, None, x_train_lda, y_train_lda, lda=[w,c])\n",
    "                        if i > 0:\n",
    "                            acc_prev[i,:],out = lp.test_models(None, None, prev_x_lda, prev_y_lda, lda=[w,c])\n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            mod_recal[i] = -1\n",
    "                            print('bad recal')\n",
    "                            # if acc_val[i,acc_i] < 75:\n",
    "                            #     if i > 0:\n",
    "                            #         mu_class, cov_class, N = prev_lda\n",
    "                            #         n_dof, key, train_dof = prev_ndof\n",
    "                            # else:\n",
    "                            #     print('keeping new model')\n",
    "                            #     mod_recal[i] = -2\n",
    "                        del x_train_lda, y_train_lda\n",
    "                    \n",
    "                    if mod_recal[i] != -1 or i == 0 :\n",
    "                        prev_x = cp.deepcopy(x_val_cnn)\n",
    "                        prev_y = cp.deepcopy(y_val)\n",
    "                        prev_x_lda = cp.deepcopy(x_val_lda)\n",
    "                        prev_y_lda = cp.deepcopy(y_val_lda)\n",
    "                    \n",
    "                    del x_train_cnn, y_train, x_val_cnn, y_val, x_val_lda, y_val_lda, x_clean_cnn, y_clean\n",
    "                \n",
    "                # load data\n",
    "                test_file = all_files[i+1]\n",
    "                test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "                \n",
    "                # check class labels\n",
    "                test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "                test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key,True)\n",
    "\n",
    "                # test \n",
    "                y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "                \n",
    "                # test \n",
    "                if 'lda' in mod:\n",
    "                    acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "                else:\n",
    "                    acc[i+1,:] = lp.test_models(x_test_cnn, y_test, x_test_lda, y_test_lda, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "\n",
    "                print ('Set: ' + train_file + ', Test: ' + test_file + ',', f'Accuracy: {acc[i+1,acc_i]:.2f}', f', Val: {acc_val[i,acc_i]:.2f}', f', Prev: {acc_prev[i,acc_i]:.2f}', f', Train: {acc_train[i,acc_i]:.2f}')\n",
    "                del y_test, x_test_cnn, x_test_lda, y_test_lda#, test_data, test_params\n",
    "\n",
    "            all_acc[:,mod_tot.index(mod)] = acc[:,acc_i]\n",
    "            all_val[:,mod_tot.index(mod)] = acc_val[:,acc_i]\n",
    "            all_prev[:,mod_tot.index(mod)] = acc_prev[:,acc_i]\n",
    "            all_train[:,mod_tot.index(mod)] = acc_train[:,acc_i]\n",
    "            all_recal[:,mod_tot.index(mod)] = mod_recal\n",
    "\n",
    "            print('------------------------' + mod + ' ' + str(np.sum(mod_recal==1)) + ' - ' + str(np.sum(mod_recal==-1)) + ' -- ' + str(np.sum(mod_recal==-2)) + '-------------')\n",
    "            mod_i += 1\n",
    "\n",
    "            # if 'cr' in mod:\n",
    "            #     del train_data_0, train_params_0\n",
    "\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','wb') as f:\n",
    "            pickle.dump([all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof, mod_all, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale],f)\n",
    "        \n",
    "        gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:69: RuntimeWarning: Mean of empty slice\n",
      "  ave_acc2 = np.nanmean(np.abs(np.array(it_acc2)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:70: RuntimeWarning: Mean of empty slice\n",
      "  ave_acc = np.nanmean(np.abs(np.array(it_acc)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:71: RuntimeWarning: Mean of empty slice\n",
      "  ave_val = np.nanmean(np.abs(np.array(it_val)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:72: RuntimeWarning: Mean of empty slice\n",
      "  ave_prev = np.nanmean(np.abs(np.array(it_prev)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:73: RuntimeWarning: Mean of empty slice\n",
      "  ave_train = np.nanmean(np.abs(np.array(it_train)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_86204\\2889983251.py:74: RuntimeWarning: Mean of empty slice\n",
      "  ave_times = np.nanmean(np.abs(np.array(it_times)),axis=0)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1670: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "ft = 'tdar'\n",
    "iter = 6\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "plot_mod = ['lda','alda','cnn','acnn03','avcnn','crcnn']\n",
    "plot_mod = ['lda','avcnn']\n",
    "plot_mod = ['acnn05','avcnn']\n",
    "for plot_i in range(1):\n",
    "    for sub in range(4,5):    \n",
    "        it_acc = []\n",
    "        it_recal = []\n",
    "        it_fail = []\n",
    "        it_val = []\n",
    "        it_prev = []\n",
    "        it_train = []\n",
    "        it_times = []\n",
    "        it_replaced = []\n",
    "        for it in range(1):#iter):\n",
    "        \n",
    "            # load or initialize cnn weights\n",
    "            if plot_i == 1:\n",
    "                with open('0323 full run pre and post/' + subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            else:\n",
    "                with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            \n",
    "            lda_ind = mod_tot.index('alda') + 1\n",
    "            all_acc[all_acc==0] = np.nan\n",
    "            all_val[all_val==0] = np.nan\n",
    "            all_prev[all_prev==0] = np.nan\n",
    "            all_train[all_train==0] = np.nan\n",
    "            all_times[all_times==0] = np.nan\n",
    "\n",
    "            it_acc.append(all_acc)\n",
    "            it_recal.append(np.sum(all_recal==1,axis=0))\n",
    "            it_fail.append(np.sum(all_recal==-1,axis=0))\n",
    "            it_replaced.append(np.sum(all_recal==-2,axis=0))\n",
    "            it_val.append(all_val)\n",
    "            it_prev.append(all_prev)\n",
    "            it_train.append(all_train)\n",
    "            it_times.append(all_times)\n",
    "\n",
    "            it_acc[it][:,:lda_ind] = it_acc[0][:,:lda_ind]\n",
    "            it_recal[it][:lda_ind] = it_recal[0][:lda_ind]\n",
    "            it_fail[it][:lda_ind] = it_fail[0][:lda_ind]\n",
    "            it_replaced[it][:lda_ind] = it_replaced[0][:lda_ind]\n",
    "            it_val[it][:,:lda_ind] = it_val[0][:,:lda_ind]\n",
    "            it_prev[it][:,:lda_ind] = it_prev[0][:,:lda_ind]\n",
    "            it_train[it][:,:lda_ind] = it_train[0][:,:lda_ind]\n",
    "            it_times[it][:,:lda_ind] = it_times[0][:,:lda_ind]\n",
    "\n",
    "\n",
    "        it_acc2 = cp.deepcopy(it_acc)\n",
    "        for i in range(len(it_acc2)):\n",
    "            x = it_val[i] < 0\n",
    "            # print(x.type)\n",
    "            # print(it_acc2[i].shape)\n",
    "            # print(ave_val.shape)\n",
    "            it_acc2[i][(it_acc[i]< 0) & (it_val[i] > 0)]= it_val[i][(it_acc[i]< 0)& (it_val[i] > 0)]\n",
    "            # it_acc2[i][(it_acc[i]< 0)]= it_val[i][(it_acc[i]< 0)]\n",
    "\n",
    "\n",
    "        ave_acc2 = np.nanmean(np.abs(np.array(it_acc2)),axis=0)\n",
    "        ave_acc = np.nanmean(np.abs(np.array(it_acc)),axis=0)\n",
    "        ave_val = np.nanmean(np.abs(np.array(it_val)),axis=0)\n",
    "        ave_prev = np.nanmean(np.abs(np.array(it_prev)),axis=0)\n",
    "        ave_train = np.nanmean(np.abs(np.array(it_train)),axis=0)\n",
    "        ave_times = np.nanmean(np.abs(np.array(it_times)),axis=0)\n",
    "        ave_recal = np.nanmean(np.array(it_recal),axis=0)\n",
    "        ave_fail = np.nanmean(np.array(it_fail),axis=0)\n",
    "        ave_replaced = np.nanmean(np.array(it_replaced),axis=0)\n",
    "\n",
    "        std_acc2 = np.nanstd(np.abs(np.array(it_acc2)),axis=0)/np.sum(~np.isnan(np.array(it_acc2)),axis=0)\n",
    "        std_acc = np.nanstd(np.abs(np.array(it_acc)),axis=0)/np.sum(~np.isnan(np.array(it_acc)),axis=0)\n",
    "        std_val = np.nanstd(np.abs(np.array(it_val)),axis=0)/np.sum(~np.isnan(np.array(it_val)),axis=0)\n",
    "        std_prev = np.nanstd(np.abs(np.array(it_prev)),axis=0)/np.sum(~np.isnan(np.array(it_prev)),axis=0)\n",
    "        std_train = np.nanstd(np.abs(np.array(it_train)),axis=0)/np.sum(~np.isnan(np.array(it_train)),axis=0)\n",
    "        std_times = np.nanstd(np.abs(np.array(it_times)),axis=0)/np.sum(~np.isnan(np.array(it_times)),axis=0)\n",
    "        std_recal = np.nanstd(np.array(it_recal),axis=0)/np.sum(~np.isnan(np.array(it_recal)),axis=0)\n",
    "        std_fail = np.nanstd(np.array(it_fail),axis=0)/np.sum(~np.isnan(np.array(it_fail)),axis=0)\n",
    "        std_replaced = np.nanstd(np.array(it_replaced),axis=0)/np.sum(~np.isnan(np.array(it_replaced)),axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "        for mod in plot_mod:\n",
    "            plot_ind = mod_tot.index(mod)\n",
    "            # ax.plot(ave_acc[2:,plot_ind],'.-',label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            x = ~np.isnan(ave_val[:,plot_ind]) \n",
    "            # ave_acc2 = cp.deepcopy(ave_acc)\n",
    "            # ave_acc[x,plot_ind] = ave_val[x,plot_ind]\n",
    "\n",
    "            ax[0].plot(ave_acc[:,plot_ind],'.-',ms=8,label= mod + ' = '+ \"{:.2f}\".format(ave_recal[plot_ind]) + ' +/- ' + \"{:.2f}\".format(std_recal[plot_ind]) + ', ' + \"{:.2f}\".format(ave_fail[plot_ind]) +' failed'+ ', ' + \"{:.2f}\".format(ave_replaced[plot_ind]) +' replaced')#str(std_recal[plot_ind,0]))\n",
    "            # ax[0].plot(np.squeeze(np.where(x)),ave_acc[x,plot_ind],'kx',ms=12)\n",
    "            ax[1].plot(ave_acc2[:,plot_ind],'.-',ms=8,label= mod + ' = '+ \"{:.2f}\".format(ave_recal[plot_ind]) + ' +/- ' + \"{:.2f}\".format(std_recal[plot_ind])+ ', ' + \"{:.2f}\".format(ave_fail[plot_ind]) + ' failed'+ ', ' + \"{:.2f}\".format(ave_replaced[plot_ind]) +' replaced')#+ str(std_recal[plot_ind,0]))\n",
    "            # ax.plot(np.squeeze(np.where(x)), ave_val[~np.isnan(ave_val[:,plot_ind]),plot_ind],'.-',ms=8,label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            # plt.fill_between(np.arange(ave_acc[2:,plot_ind].shape[0]),ave_acc[2:,plot_ind]-std_acc[2:,plot_ind],ave_acc[2:,plot_ind]+std_acc[2:,plot_ind],alpha=.3)\n",
    "        ax[1].legend()\n",
    "        for i in range(2):\n",
    "            ax[i].axhline(75, ls='--', color='grey')\n",
    "            ax[i].set_ylim([0,100])\n",
    "        # ax[1].axhline(75, ls='--', color='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = False\n",
    "ft = 'tdar'\n",
    "iter = 10\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_mod = 0\n",
    "\n",
    "sub_b = []\n",
    "sub_s = []\n",
    "sub_sep=[]\n",
    "for sub in range(2,7):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    # load or initialize cnn weights\n",
    "    acc = np.zeros((len(all_files),2))\n",
    "    acc_val = np.zeros((len(all_files),2))\n",
    "    acc_prev = np.zeros((len(all_files),2))\n",
    "    acc_train = np.zeros((len(all_files),2))\n",
    "    \n",
    "\n",
    "    acc_i = 0\n",
    "\n",
    "    # Loop through files\n",
    "    for i in range(len(all_files)-1):              \n",
    "        # load training file\n",
    "        train_file = all_files[i]\n",
    "        train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "        # load training file\n",
    "        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "        val_data, val_params = train_data, train_params\n",
    "            \n",
    "        # get current dofs and create key\n",
    "        if i == 0:\n",
    "            train_dof = np.unique(train_params[:,-1])\n",
    "            key = np.arange(len(train_dof))\n",
    "\n",
    "            n_dof = len(train_dof)\n",
    "\n",
    "            h = np.ones((len(all_files),n_dof))\n",
    "            b = np.ones((len(all_files),n_dof))\n",
    "            sep = np.zeros((len(all_files),n_dof))\n",
    "            tot_b = np.ones((len(all_files,)))\n",
    "            h[:] = np.nan\n",
    "            b[:] = np.nan\n",
    "\n",
    "            train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key,False)\n",
    "            val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key,False)\n",
    "\n",
    "            _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=False, split=False,num_classes=n_dof)\n",
    "\n",
    "            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda, key)\n",
    "            \n",
    "            cl_s = np.zeros((n_dof,))\n",
    "            # cl_s[:] = np.nan\n",
    "            m_cl = np.zeros((n_dof,x_train_lda.shape[1]))\n",
    "            s_cl = np.zeros((n_dof,x_train_lda.shape[1],x_train_lda.shape[1]))\n",
    "            for cl in train_dof:\n",
    "                train_ind = np.squeeze(y_train_lda == key[train_dof==cl])\n",
    "                temp_x = x_train_lda[train_ind,...]\n",
    "                m1 = np.nanmean(temp_x,axis=0)\n",
    "                s1 = np.cov(temp_x.T)\n",
    "                m_cl[key[train_dof==cl],...] = np.nanmean(temp_x,axis=0)\n",
    "                s_cl[key[train_dof==cl],...] = np.cov(temp_x.T)\n",
    "                ct = 0\n",
    "                for cl_i in train_dof:\n",
    "                    temp_ind = np.squeeze(y_train_lda == key[train_dof==cl_i])\n",
    "                    temp_x2 = x_train_lda[temp_ind,...]\n",
    "                    m2 = np.nanmean(temp_x2,axis=0)\n",
    "                    s2 = np.cov(temp_x2.T)\n",
    "                    cl_s[key[train_dof==cl]] += np.nanmean(prd.mahal(m1,s1,m2,s2))\n",
    "                    ct+=1\n",
    "                cl_s[key[train_dof==cl]] /= ct\n",
    "\n",
    "        #del x_train_lda, y_train_lda, x_train_cnn, y_train, x_clean_cnn, y_clean\n",
    "        \n",
    "        # load data\n",
    "        test_file = all_files[i+1]\n",
    "        test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "        \n",
    "        # check class labels\n",
    "        test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "        test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key,True)\n",
    "\n",
    "        # test \n",
    "        y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "        \n",
    "        # test \n",
    "        acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "\n",
    "        for cl in train_dof:\n",
    "            test_ind = np.squeeze(y_test_lda == key[train_dof==cl])\n",
    "            if i == 0:\n",
    "                train_ind = np.squeeze(y_train_lda == key[train_dof==cl])\n",
    "                m1 = np.nanmean(x_train_lda[train_ind,:],axis=0)\n",
    "                s1 = np.cov(x_train_lda[train_ind,:].T)\n",
    "            if np.sum(test_ind) > 50:\n",
    "                # print(np.sum(test_ind))\n",
    "                m2 = np.nanmean(x_test_lda[test_ind,:],axis=0)\n",
    "                s2 = np.cov(x_test_lda[test_ind,:].T)\n",
    "                h[i+1,key[train_dof==cl]] = prd.hellinger(m1,s1,m2,s2)\n",
    "                b[i+1,key[train_dof==cl]] = np.nanmean(prd.mahal(m1,s1,m2,s2))\n",
    "                ct = 0\n",
    "                for cl_i in train_dof:\n",
    "                    cl_sep = np.nanmean(prd.mahal(np.squeeze(m_cl[key[train_dof==cl_i],...]),np.squeeze(s_cl[key[train_dof==cl_i],...]),m2,s2))\n",
    "                    if np.isnan(cl_sep):\n",
    "                        print('oops')\n",
    "                    else:\n",
    "                        ct += 1\n",
    "                        sep[i+1,key[train_dof==cl]] += cl_sep\n",
    "                sep[i+1,key[train_dof==cl]] /= ct\n",
    "                # b[i+1,key[train_dof==cl]] = prd.bhatta(m1,s1,m2,s2)\n",
    "        # print(h[i+1,:])\n",
    "        # m1 = np.nanmean(x_train_lda,axis=0)\n",
    "        # s1 = np.cov(x_train_lda.T)\n",
    "        # m2 = np.nanmean(x_test_lda,axis=0)\n",
    "        # s2 = np.cov(x_test_lda.T)\n",
    "        # b[i+1,0] = np.min(h[i+1,:])\n",
    "\n",
    "        print(\"{:.2f}\".format(acc[i+1,0]))\n",
    "        del y_test, x_test_cnn#, x_test_lda, y_test_lda, test_data, test_params\n",
    "\n",
    "    print(np.median(acc,axis=0))\n",
    "    with open(subs[sub] + '_hell.p','wb') as f:\n",
    "        pickle.dump([acc,h,b,cl_s,sep],f)\n",
    "    sub_b.append(b)\n",
    "    sub_s.append(cl_s)\n",
    "    sub_sep.append(sep)\n",
    "    \n",
    "    gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 4\n",
    "h[h==0] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "for sub in range(7):\n",
    "    with open(subs[sub] + '_hell.p','rb') as f:\n",
    "        acc,h,b,cl_s,sep = pickle.load(f)\n",
    "    min_h = np.nanmax(h[1:,:],axis=1)\n",
    "    norm_m = b/np.nanmean(cl_s)\n",
    "    acc_p = acc[1:,0]\n",
    "    fig,ax = plt.subplots()\n",
    "    fig1,ax1 = plt.subplots(1,2,figsize=(15,3))\n",
    "    # ax.plot(-np.nanmean(b[1:,:],axis=1),acc_p, 'x')\n",
    "    ax.plot(np.nanmax(b[1:,:],axis=1),acc_p, 'x')\n",
    "    ax1[0].plot(np.nanmax(b[1:,:],axis=1), 'x')\n",
    "    ax1[1].plot(acc_p, 'x')\n",
    "    ax1[1].set_ylim([0,100])\n",
    "    # ax1[0].set_ylim([0,20])\n",
    "    # ax.set_xlim([0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for sub in range(7):\n",
    "    with open(subs[sub] + '_lda_accs.p','rb') as f:\n",
    "        acc, _ = pickle.load(f)\n",
    "    temp.append(acc[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 128\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn','acnn03','acnn30','acewc00','acewc30', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn03', 'avcnn15', 'acnnl03','crvcnn','acewclm','xtra','xtra1','xtra2']\n",
    "ft = 'feat'\n",
    "iter = 1\n",
    "\n",
    "for sub in range(4,5):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    mod_all = ['vcnn']\n",
    "\n",
    "    # load or initialize cnn weights\n",
    "    if load_mod:\n",
    "        with open(subs[sub] + '_' + str(0) + '_r_accs.p','rb') as f:\n",
    "            all_acc, all_recal, all_val, all_prev, all_train, all_times, _, _, c_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "    else:\n",
    "        c_weights = None\n",
    "        v_weights = None\n",
    "        v_wc = None\n",
    "        cl_wc = None\n",
    "        all_recal = np.empty((len(mod_tot),1))\n",
    "        all_recal[:] = np.nan\n",
    "        all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "    mod_i = 0\n",
    "    for mod in mod_all:\n",
    "        acc = np.zeros((len(all_files),5))\n",
    "        acc_val = np.zeros((len(all_files),5))\n",
    "        acc_prev = np.zeros((len(all_files),5))\n",
    "        acc_train = np.zeros((len(all_files),5))\n",
    "\n",
    "        if 'cnn' in mod:\n",
    "            acc_i = 2\n",
    "        elif 'cewc' in mod:\n",
    "            acc_i = 4\n",
    "        elif 'lda' in mod:\n",
    "            acc_i = 0\n",
    "\n",
    "        cnn = None\n",
    "        ewc = None\n",
    "\n",
    "        ep = 50\n",
    "        recal = 0\n",
    "        skip = False\n",
    "\n",
    "        # Loop through files\n",
    "        for i in range(1,2):#len(all_files)-1):\n",
    "            # load training file\n",
    "            train_file = all_files[i]\n",
    "            train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "            train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "            val_data = train_data\n",
    "            val_params = train_params\n",
    "\n",
    "            train_dof = np.unique(train_params[:,-1])\n",
    "            key = np.empty(train_dof.shape)\n",
    "            for key_i in range(len(train_dof)):\n",
    "                key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "            n_dof = int(np.max(key))\n",
    "            \n",
    "            train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "            val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key)\n",
    "\n",
    "            _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "\n",
    "            _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "            del train_data, train_params, val_data, val_params\n",
    "\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, print_b=True)\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=[cnn], n_dof=n_dof, ep=30, dec=False,print_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_clean_cnn)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    x_out[y_clean[:,cl]==1,...] = np.random.normal(np.mean(x_clean_cnn[y_clean[:,cl]==1,...],axis=0), np.std(x_clean_cnn[y_clean[:,cl]==1,...],axis=0),x_clean_cnn[y_clean[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_train_cnn)\n",
    "for cl in range(y_train.shape[1]):\n",
    "    x_out[y_train[:,cl]==1,...] = np.random.normal(np.mean(x_train_cnn[y_train[:,cl]==1,...],axis=0), np.std(x_train_cnn[y_train[:,cl]==1,...],axis=0),x_train_cnn[y_train[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = np.ones((1*y_clean.shape[0],8))\n",
    "x_out,_,_ = cnn.dec(samp1,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),samp=True)\n",
    "# _,x_out,_,_ = cnn(x_clean_cnn,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),dec=True)\n",
    "x_out = x_out.numpy()\n",
    "# for i in range(y_clean.shape[1]):\n",
    "#     adjust1 = np.std(x_clean_cnn[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     rescale = np.mean(adjust1)/np.mean(np.std(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0))\n",
    "#     gmean = np.mean(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     x_out[np.argmax(y_clean,axis=1)==i,...] = (x_out[np.argmax(y_clean,axis=1)==i,...] - gmean)*rescale + gmean\n",
    "# x_out = np.maximum(np.minimum(x_out,1),0)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    ind = np.tile(np.argmax(y_clean,axis=1)==cl,[1])\n",
    "    ind2 = np.argmax(y_clean,axis=1)==cl\n",
    "    x_temp = x_out[ind,...].reshape((np.sum(ind),-1))\n",
    "    x_true = x_clean_cnn[ind2,...].reshape((np.sum(ind2),-1))\n",
    "    # for i in range()\n",
    "    plt.figure()\n",
    "    # for i in range(x_true.shape[0]):\n",
    "    #     plt.plot(x_true[i,...],'k-')\n",
    "        \n",
    "    for i in range(x_temp.shape[0]):\n",
    "        plt.plot(x_temp[i,...],'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lda = x_clean_cnn.reshape(x_clean_cnn.shape[0],-1)\n",
    "y_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "y_train_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_lda,y_lda)\n",
    "y_out = dlda.predict(x_lda, w, c)\n",
    "print(dlda.eval_lda(w, c, x_lda, y_lda))\n",
    "x_out_lda = x_out.reshape(x_out.shape[0],-1)\n",
    "print(dlda.eval_lda(w,c, x_out_lda,np.tile(y_train_lda,[1,1])))\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_out_lda,np.tile(y_train_lda,[1,1]))\n",
    "print(dlda.eval_lda(w, c, x_out_lda,y_train_lda))\n",
    "print(dlda.eval_lda(w, c, x_lda, np.argmax(y_clean,axis=1)[...,np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn1,test_accuracy)\n",
    "print(lp.test_models(x_out, y_clean, None, None, cnn=cnn1, test_mod=test_mod, test_accuracy=test_accuracy))\n",
    "\n",
    "cnn2, _ = lp.train_models(traincnn=x_out,y_train=y_clean, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn2,test_accuracy)\n",
    "print(lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn2, test_mod=test_mod, test_accuracy=test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "fig,ax = plt.subplots(1,5,figsize=(30,4))\n",
    "for sub in range(2,3):#,5):\n",
    "    with open(subs[sub] + '_0_r_accs.p','rb') as f:\n",
    "        acc_all, recal_all, cur_all, prev_all, val_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "    # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "    colors =  cm.get_cmap('tab20c')\n",
    "    c = np.empty((20,4))\n",
    "    for i in range(20):\n",
    "        c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "    nn_c[0,-1] = 1\n",
    "    all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "    pt_m = ['ko-','o-','o-','o-','s','s','s','s','D']\n",
    "    nn_c = np.vstack((np.array([0,0,0,1]), c[0,:],c[1,:],c[2,:],c[3,:],c[4,:],c[5,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "    # nn_c[0,-1] = 1\n",
    "\n",
    "    labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "    labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "    # labels = mod_tot\n",
    "\n",
    "    ax_ind = sub\n",
    "    it = 0\n",
    "    for v in [1,2]: \n",
    "        i = mod_tot.index(mod_all[v])\n",
    "        acc_temp = acc_all[1:-1,i]\n",
    "        if not np.isnan(acc_temp).all():\n",
    "            x = np.arange(len(acc_temp))\n",
    "            recal_i = (acc_temp < 0)\n",
    "            ax[ax_ind].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v],color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "            it+=1\n",
    "\n",
    "    for i in range(5):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        \n",
    "        ax[i].set_ylim([0,100])\n",
    "        ax[i].set_title('TR' + str(i+1))\n",
    "    ax[0].legend()\n",
    "    ax[2].set_xlabel('Calibration Set')\n",
    "    ax[0].set_ylabel('Accuracy (%)')\n",
    "    plt.rc('font', size=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "for sub in range(2,3):#,5):\n",
    "    fig,ax = plt.subplots(1,4,figsize=(20,4))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, val_all,mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [1,0,0,1,2,2,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[it]+': ' + str(int(recal_all[i,0])),color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "                it+=1\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "cv_iter = 1\n",
    "for sub in range(0,5):\n",
    "    fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','o','*','o','s','D','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','f-cnn-5','f-cnn-3','f-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in [0, 3, 5, 4, 6, 7]: #range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v]+': ' + str(int(recal_all[i,0])),color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                it+=1\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "        \n",
    "\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e4d54467b05e62951c9fd7929782b99429e3b62c1a3b146d4f3dbf79f907e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('adapt_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
