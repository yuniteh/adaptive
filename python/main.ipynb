{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpu import set_gpu\n",
    "import numpy as np\n",
    "import os\n",
    "import adapt.utils.data_utils as prd\n",
    "import adapt.loop as lp\n",
    "import adapt.ml.lda as dlda\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import adapt.ml.dl_subclass as dl\n",
    "import copy as cp\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython import display\n",
    "import gc as gc\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR37\n",
      "train dof: [ 1  6 16 17 19 48], key: [0 1 2 3 4 5]\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20160412_151530, Test: 20160412_151732, Accuracy: 76.21 , Val: 93.30 , Prev: 0.00 , Train: 93.30\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20160412_151530, Test: 20160413_122728, Accuracy: 81.31 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20160412_151530, Test: 20160418_111731, Accuracy: 74.08 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20160418_111731\n",
      "prev: [ 1  6 16 17 19 48]\n",
      "cur: [ 1  6 16 17 19 48]\n",
      "train dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20160418_111731, Test: 20160418_120531, Accuracy: 75.12 , Val: 75.16 , Prev: 85.25 , Train: 97.50\n",
      "init test dof: [ 1  6 16 19 48]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dof: [ 1  6 16 19 48], key: [ 0.  1.  2. nan  4.  5.]\n",
      "Set: 20160418_111731, Test: 20160420_053828, Accuracy: 84.49 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20160418_111731, Test: 20160422_085920, Accuracy: 50.60 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20160422_085920\n",
      "prev: [ 1  6 16 17 19 48]\n",
      "cur: [ 1  6 16 17 19 48]\n",
      "train dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20160422_085920, Test: 20160422_090156, Accuracy: 43.99 , Val: 51.46 , Prev: 57.38 , Train: 84.25\n",
      "recal: 3 20160422_090156\n",
      "prev: [ 1  6 16 17 19 48]\n",
      "cur: [ 1  6 16 17 19 48]\n",
      "train dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20160422_090156, Test: 20160422_092607, Accuracy: 57.99 , Val: 51.51 , Prev: 69.75 , Train: 75.73\n",
      "recal: 4 20160422_092607\n",
      "prev: [ 1  6 16 17 19 48]\n",
      "cur: [ 1  6 16 17 19 48]\n",
      "train dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 19]\n",
      "test_dof: [ 1  6 16 19], key: [ 0.  1.  2. nan  4. nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\core\\_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set: 20160422_092607, Test: 20160507_205135, Accuracy: 82.35 , Val: 51.30 , Prev: 67.36 , Train: 83.96\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20160422_092607, Test: 20160507_210240, Accuracy: 70.02 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 5 20160507_210240\n",
      "prev: [ 1  6 16 17 19 48]\n",
      "cur: [ 1  6 16 17 19 48]\n",
      "train dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20160507_210240, Test: 20160507_210456, Accuracy: 85.22 , Val: 75.57 , Prev: 72.97 , Train: 92.91\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20160507_210240, Test: 20170829_171804, Accuracy: 11.76 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 6 20170829_171804\n",
      "prev: [ 1  6 16 17 19 48]\n",
      "cur: [ 1  6 16 17 19 48]\n",
      "train dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20170829_171804, Test: 20170902_152039, Accuracy: 40.76 , Val: 64.00 , Prev: 19.75 , Train: 94.27\n",
      "recal: 7 20170902_152039\n",
      "prev: [ 1  6 16 17 19 48]\n",
      "cur: [ 1  6 16 17 19 48]\n",
      "train dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20170902_152039, Test: 20171031_122547, Accuracy: 65.75 , Val: 60.98 , Prev: 45.95 , Train: 83.65\n",
      "recal: 8 20171031_122547\n",
      "prev: [ 1  6 16 17 19 48]\n",
      "cur: [ 1  6 16 17 19 48]\n",
      "train dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20171031_122547, Test: 20171107_092257, Accuracy: 73.40 , Val: 76.80 , Prev: 49.79 , Train: 91.04\n",
      "recal: 9 20171107_092257\n",
      "prev: [ 1  6 16 17 19 48]\n",
      "cur: [ 1  6 16 17 19 48]\n",
      "train dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "init test dof: [ 1  6 16 17 19 48]\n",
      "test_dof: [ 1  6 16 17 19 48], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20171107_092257, Test: 20171107_103740, Accuracy: 67.26 , Val: 92.09 , Prev: 73.47 , Train: 97.08\n",
      "alda 9 - 5\n"
     ]
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn2','acnn05','acnn30','acewc30','acewc15', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn', 'avcnn15', 'acnnl03','crvcnn','acewclm','crcnn','acewc00','xtra2']\n",
    "ft = 'tdar'\n",
    "iter = 1\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_mod = 0\n",
    "\n",
    "for it in range(iter):\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    if it == 0:\n",
    "        mod_all = ['alda']#,'alda','cnn','acnn05','avcnn']#,'crcnn']\n",
    "        # mod_all = ['avcnn']\n",
    "    else:\n",
    "        mod_all = ['bcnn','cnn','acnn03','avcnn','crcnn']\n",
    "\n",
    "    for sub in range(1):\n",
    "        print(subs[sub])\n",
    "        sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "        all_files = os.listdir(sub_path)\n",
    "        if 'skip' in all_files:\n",
    "            all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "        # load or initialize cnn weights\n",
    "        if load_mod:\n",
    "            with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                all_acc, all_recal, all_val, all_prev, all_train, all_times, _, _, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        else:\n",
    "            c_weights = None\n",
    "            v_weights = None\n",
    "            v_wc = None\n",
    "            cl_wc = None\n",
    "            scaler_0 = None\n",
    "            all_recal = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "        mod_i = 0\n",
    "        for mod in mod_all:\n",
    "            acc = np.zeros((len(all_files),2))\n",
    "            acc_val = np.zeros((len(all_files),2))\n",
    "            acc_prev = np.zeros((len(all_files),2))\n",
    "            acc_train = np.zeros((len(all_files),2))\n",
    "            mod_recal = np.zeros((len(all_files),))\n",
    "\n",
    "            if 'lda' in mod:\n",
    "                acc_i = 0\n",
    "            else:\n",
    "                acc_i = 1\n",
    "\n",
    "            cnn = None\n",
    "            ewc = None\n",
    "            clda = None\n",
    "\n",
    "            ep = 30\n",
    "            recal = 0\n",
    "            skip_recal = 0\n",
    "            skip = False\n",
    "\n",
    "            # Loop through files\n",
    "            for i in range(1,len(all_files)-1):\n",
    "                # Check if need to recalibrate\n",
    "                if i > 1:\n",
    "                    if acc[i,acc_i] < 75:\n",
    "                        skip = False\n",
    "                        recal += 1\n",
    "                        print('recal: ' + str(recal) + ' ' + all_files[i])\n",
    "                        acc[i,acc_i] *= -1\n",
    "                        mod_recal[i] = 1\n",
    "                    else:\n",
    "                        skip = True\n",
    "                    \n",
    "                    if 'b' in mod:\n",
    "                        skip = True\n",
    "                        \n",
    "                if not skip:\n",
    "                    # load training file\n",
    "                    train_file = all_files[i]\n",
    "                    train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "                    # if first train, use two train files\n",
    "                    if i == 1:\n",
    "                        train_data2, train_params2 = prd.load_caps_train(sub_path + all_files[i-1] + '/traindata.mat')\n",
    "                        train_data = np.vstack((train_data,train_data2))\n",
    "                        train_params = np.vstack((train_params,train_params2))\n",
    "                        del train_data2, train_params2\n",
    "\n",
    "                        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "                        val_data = train_data\n",
    "                        val_params = train_params\n",
    "                    else:\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                    \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "\n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "\n",
    "                        del train_temp, params_temp, tr_i, te_i\n",
    "                        \n",
    "                    # if combining, save current training data\n",
    "                    # if 'cr' in mod:\n",
    "                    #     # combine old and new training data\n",
    "                    #     if i > 1:\n",
    "                    #         train_data = np.vstack((train_data_0,train_data))\n",
    "                    #         train_params = np.vstack((train_params_0,train_params))\n",
    "\n",
    "                    #     train_data_0 = cp.deepcopy(train_data)\n",
    "                    #     train_params_0 = cp.deepcopy(train_params)\n",
    "\n",
    "                    # if (i == 1 and mod[0] == 'a') or (i == 1 and mod[:2] == 'cr') or (mod[0] != 'a' and mod[:2] != 'cr'):\n",
    "                        \n",
    "                    if i > 1:\n",
    "                        # get previous dofs\n",
    "                        prev_ndof = [n_dof, key, train_dof]\n",
    "                        print('prev: ' + str(train_dof))\n",
    "                        # get current dofs and create key\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        print('cur: ' + str(train_dof))\n",
    "                        key = np.zeros((len(train_dof),))\n",
    "                        # check if current dofs are all in old dof list\n",
    "                        dof_ovlp = np.isin(train_dof,prev_ndof[2],assume_unique=True)\n",
    "                        temp_dof = cp.deepcopy(train_dof)\n",
    "                        # loop through dofs that are in previous dofs, set the keys\n",
    "                        for dof in train_dof[dof_ovlp]:\n",
    "                            key[train_dof==dof] = prev_ndof[1][prev_ndof[2]==dof]\n",
    "                            temp_dof[train_dof==dof] = prev_ndof[2][prev_ndof[2]==dof]\n",
    "\n",
    "                        # check if previous dofs has classes not in this set\n",
    "                        dof_xtra = ~np.isin(prev_ndof[2],temp_dof,assume_unique=True)\n",
    "                        temp_dof = np.hstack((temp_dof,prev_ndof[2][dof_xtra]))\n",
    "                        key = np.hstack((key,prev_ndof[1][dof_xtra]))\n",
    "\n",
    "                        # loop through dofs that are not in previous dofs (ie new classes), add keys\n",
    "                        key_i = 1\n",
    "                        xtra = False\n",
    "                        for dof in train_dof[~dof_ovlp]:\n",
    "                            if mod == 'alda':\n",
    "                                xtra = True\n",
    "                                key[temp_dof==dof] = np.max(key) + key_i\n",
    "                                key_i += 1\n",
    "                            else:\n",
    "                                # remove extras\n",
    "                                print('removing train ' + str(dof))\n",
    "                                ind = train_params[:,-1] == dof\n",
    "                                train_params = train_params[~ind,...]\n",
    "                                train_data = train_data[~ind,...]\n",
    "                                ind = val_params[:,-1] == dof\n",
    "                                val_params = val_params[~ind,...]\n",
    "                                val_data = val_data[~ind,...]\n",
    "                                key = np.delete(key,temp_dof==dof)\n",
    "                                temp_dof = np.delete(temp_dof,temp_dof==dof)\n",
    "\n",
    "                        train_dof = cp.deepcopy(temp_dof)\n",
    "                    else:\n",
    "                        # get current dofs and create key\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        key = np.arange(len(train_dof))\n",
    "\n",
    "                    n_dof = len(train_dof)\n",
    "\n",
    "                    train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key,False)\n",
    "                    val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key,False)\n",
    "\n",
    "                    print('train dof: ' + str(train_dof) + ', key: ' + str(key))\n",
    "\n",
    "                    if (mod[0] == 'a' and i > 1) or ('cr' in mod and i > 1) or (mod == 'vcnn' and i > 1):\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale, scaler=scaler, num_classes=n_dof, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False)\n",
    "                    else:\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "                        if ((i == 1) and (c_weights is not None)) or ((i == 1) and (v_weights is not None)):\n",
    "                            scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                    _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "                    if 'cr' in mod:\n",
    "                        # combine old and new training data\n",
    "                        if i > 1:\n",
    "                            x_clean_cnn = np.vstack((clean_data_0,x_clean_cnn))\n",
    "                            y_clean = np.vstack((clean_params_0,y_clean))\n",
    "                            x_train_cnn = np.vstack((x_clean_cnn,x_train_cnn))\n",
    "                            y_train = np.vstack((y_clean,y_train))\n",
    "\n",
    "                    del train_data, train_params, val_data, val_params\n",
    "\n",
    "                    if 'lda' not in mod:\n",
    "                        cnnlda = 'l' in mod\n",
    "                        if 'vcnn' in mod:\n",
    "                            if i == 1:\n",
    "                                # calculate normalization scale\n",
    "                                old_y = cp.deepcopy(y_clean)\n",
    "\n",
    "                                if v_weights is None:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_clean_cnn,y_train=y_clean, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, trainable=True)\n",
    "                                    # save current autoencoder weights\n",
    "                                    b_enc_w = cp.deepcopy(cnn.enc.get_weights())\n",
    "                                    dec_w = cp.deepcopy(cnn.dec.get_weights())\n",
    "                                    v_weights = cp.deepcopy(cnn.dec.get_weights())\n",
    "                                else:\n",
    "                                    cnn = dl.VCNN(n_class = n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.add_dec(x_train_cnn[:1,...])\n",
    "                                    cnn(x_train_cnn[:2,...],np.ones((2,)),dec=True) \n",
    "                                    cnn.dec.set_weights(v_weights)\n",
    "\n",
    "                                if c_weights is None:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=[cnn], n_dof=n_dof, ep=30, dec=False, trainable=True)\n",
    "                                    c_weights = [cnn.enc.get_weights(),cnn.clf.get_weights()]\n",
    "                                else:\n",
    "                                    print('setting CNN weights')\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "\n",
    "                                if scaler_0 is None:\n",
    "                                    scaler_0 = cp.deepcopy(scaler)\n",
    "                                else:\n",
    "                                    scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                                mu_class, std_class, N = prd.update_mean(x_clean_cnn,y_clean)\n",
    "                            else:\n",
    "                                prev_w = cnn.get_weights()\n",
    "                                prev_mu = [mu_class, std_class, N]\n",
    "                                # generate old training data, same size as clean data\n",
    "\n",
    "                                x_out = cp.deepcopy(x_clean_cnn)\n",
    "                                for cl in range((y_clean).shape[1]):\n",
    "                                    x_ind = y_clean[:,cl]==1\n",
    "                                    x_out[x_ind,...] = np.random.normal(mu_class[cl], std_class[cl],x_clean_cnn[x_ind,...].shape)\n",
    "\n",
    "                                x_train_aug = np.vstack((x_out,x_train_cnn))\n",
    "                                y_train_aug = np.vstack((y_clean,y_train))\n",
    "\n",
    "                                x_clean_aug = np.vstack((x_out,x_clean_cnn))\n",
    "                                y_clean_aug = np.vstack((y_clean,y_clean))\n",
    "\n",
    "                                old_y = cp.deepcopy(y_clean_aug)\n",
    "                                mu_class, std_class, N = prd.update_mean(x_clean_cnn,y_clean,N,mu_class,std_class)\n",
    "\n",
    "                                if 'avcnn' in mod: # update whole CNN and lda weights\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=5, dec=False, lr=0.00001, trainable=False)\n",
    "                                else:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=30, dec=False, trainable=True)\n",
    "\n",
    "                                    w, c, _, _, _, _, _ = dlda.train_lda(x_train_aug.reshape((x_train_aug.shape[0],-1)), np.argmax(y_train_aug,axis=1)[...,np.newaxis])\n",
    "                        else:\n",
    "                            if i == 1:\n",
    "                                if c_weights is None:\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                                    c_weights = cp.deepcopy([cnn.enc.get_weights(),cnn.clf.get_weights()])\n",
    "                                    scaler_0 = cp.deepcopy(scaler)    \n",
    "                                else:\n",
    "                                    print('setting CNN weights')\n",
    "                                    cnn = dl.CNN(n_class=n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "                                    if cnnlda:\n",
    "                                        print('setting LDA weights')\n",
    "                                        w_c = cp.deepcopy(cl_wc[0].astype('float32'))\n",
    "                                        c_c = cp.deepcopy(cl_wc[1].astype('float32'))\n",
    "                                if 'ewc' in mod:\n",
    "                                    cnn = dl.EWC(n_class=n_dof)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "                                if 'ad' in mod:\n",
    "                                    cnn = dl.CNN(n_class=n_dof,adapt=True)\n",
    "                                    cnn(x_train_cnn[:1,...])\n",
    "                                    cnn.enc.set_weights(c_weights[0])\n",
    "                                    cnn.clf.set_weights(c_weights[1])\n",
    "\n",
    "                                if 'l' in mod:\n",
    "                                    clda = [w_c, c_c]\n",
    "                                \n",
    "                                if scaler_0 is None:\n",
    "                                    scaler_0 = cp.deepcopy(scaler)\n",
    "                                else:\n",
    "                                    scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                            else:\n",
    "                                prev_w = cnn.get_weights()\n",
    "                                if 'adcnn' in mod: # adapt first layer only\n",
    "                                    # cnn.base.trainable=False\n",
    "                                    cnn.clf.trainable=False\n",
    "                                    ep = int(mod[-2:])\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], adapt=True, cnnlda=cnnlda, lr=0.00001)\n",
    "                                elif 'acnn' in mod: # update whole CNN and lda weights\n",
    "                                    ep = int(mod[-2:])\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], cnnlda=cnnlda, lr=0.00001)\n",
    "                                elif 'cnn' in mod: # recalibrate cnnlda\n",
    "                                    cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda)\n",
    "                                elif 'acewc' in mod:\n",
    "                                    w_c, c_c, all_times[i,mod_tot.index(mod)] = lp.train_task(cnn, 15, 1, x_train_cnn, y_train, [prev_x, x_val_cnn],[prev_y, y_val], lams=[int(mod[-2:])], bat=bat, cnnlda=cnnlda)\n",
    "                                \n",
    "                                if 'l' in mod:\n",
    "                                    clda = [w_c, c_c]\n",
    "                        del test_mod\n",
    "                        test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                        if i > 1:\n",
    "                            acc_prev[i,:] = lp.test_models(prev_x, prev_y, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_val[i,:] = lp.test_models(x_val_cnn, y_val, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_train[i,:] = lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        \n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            n_dof, key, train_dof = prev_ndof\n",
    "                            if 'vcnn' in mod:\n",
    "                                mu_class, std_class, N = prev_mu\n",
    "                                cnn = dl.VCNN(n_class = n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.add_dec(x_train_cnn[:1,...])\n",
    "                                cnn(x_train_cnn[:2,...],np.ones((2,)),dec=True) \n",
    "                            elif 'cnn' in mod:\n",
    "                                cnn = dl.CNN(n_class = n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                            cnn.set_weights(prev_w)\n",
    "                            del test_mod\n",
    "                            test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                            \n",
    "                            mod_recal[i] = -1\n",
    "                            print('bad recal')\n",
    "                        elif 'cr' in mod:\n",
    "                            clean_data_0 = cp.deepcopy(x_clean_cnn)\n",
    "                            clean_params_0 = cp.deepcopy(y_clean)\n",
    "                        if 'ewc' in mod: \n",
    "                            cnn.compute_fisher(x_train_cnn, y_train, num_samples=200, plot_diffs=False) \n",
    "                            cnn.star()\n",
    "                    else:\n",
    "                        if i == 1:\n",
    "                            N = np.zeros((n_dof),)\n",
    "                            cov_class = np.zeros([x_train_lda.shape[1],x_train_lda.shape[1]])\n",
    "                            mu_class = np.zeros([n_dof,x_train_lda.shape[1]])\n",
    "                        prev_lda = [mu_class,cov_class,N]\n",
    "                        start_time = time.time()\n",
    "                        if mod[0] != 'a' or (i == 1 and mod[0] == 'a'):\n",
    "                            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda)\n",
    "                        else:\n",
    "                            w, c, mu_class, cov_class, N = dlda.update_lda(x_train_lda, y_train_lda, N, mu_class, cov_class, key, prev_ndof[1])\n",
    "                        all_times[i,mod_tot.index(mod)] = time.time() - start_time\n",
    "\n",
    "                        acc_val[i,:] = lp.test_models(None, None, x_val_lda, y_val_lda, lda=[w,c])\n",
    "                        acc_train[i,:] = lp.test_models(None, None, x_train_lda, y_train_lda, lda=[w,c])\n",
    "                        if i > 1:\n",
    "                            acc_prev[i,:] = lp.test_models(None, None, prev_x_lda, prev_y_lda, lda=[w,c])\n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            mod_recal[i] = -1\n",
    "                            mu_class, cov_class, N = prev_lda\n",
    "                            n_dof, key, train_dof = prev_ndof\n",
    "                            print('bad recal')\n",
    "                        del x_train_lda, y_train_lda\n",
    "                    \n",
    "                    if mod_recal[i] != -1:\n",
    "                        prev_x = cp.deepcopy(x_val_cnn)\n",
    "                        prev_y = cp.deepcopy(y_val)\n",
    "                        prev_x_lda = cp.deepcopy(x_val_lda)\n",
    "                        prev_y_lda = cp.deepcopy(y_val_lda)\n",
    "                    \n",
    "                    del x_train_cnn, y_train, x_val_cnn, y_val, x_val_lda, y_val_lda, x_clean_cnn, y_clean\n",
    "                \n",
    "                # load data\n",
    "                test_file = all_files[i+1]\n",
    "                test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "                \n",
    "                # check class labels\n",
    "                test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "                test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key,True)\n",
    "\n",
    "                # for cl in train_dof:\n",
    "                #     test_params[test_params[:,-1] == cl,0] = key[train_dof==cl]\n",
    "\n",
    "                # test \n",
    "                y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "                \n",
    "                # test \n",
    "                if 'lda' in mod:\n",
    "                    acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "                else:\n",
    "                    acc[i+1,:] = lp.test_models(x_test_cnn, y_test, x_test_lda, y_test_lda, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "\n",
    "                print ('Set: ' + train_file + ', Test: ' + test_file + ',', f'Accuracy: {acc[i+1,acc_i]:.2f}', f', Val: {acc_val[i,acc_i]:.2f}', f', Prev: {acc_prev[i,acc_i]:.2f}', f', Train: {acc_train[i,acc_i]:.2f}')\n",
    "                del y_test, x_test_cnn, x_test_lda, y_test_lda test_data, test_params\n",
    "\n",
    "            all_acc[:,mod_tot.index(mod)] = acc[:,acc_i]\n",
    "            all_val[:,mod_tot.index(mod)] = acc_val[:,acc_i]\n",
    "            all_prev[:,mod_tot.index(mod)] = acc_prev[:,acc_i]\n",
    "            all_train[:,mod_tot.index(mod)] = acc_train[:,acc_i]\n",
    "            all_recal[:,mod_tot.index(mod)] = mod_recal\n",
    "\n",
    "            print(mod + ' ' + str(recal) + ' - ' + str(np.sum(mod_recal==-1)))\n",
    "            mod_i += 1\n",
    "\n",
    "            # if 'cr' in mod:\n",
    "            #     del train_data_0, train_params_0\n",
    "\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','wb') as f:\n",
    "            pickle.dump([all_acc, all_recal, all_val, all_prev, all_train, all_times, mod_all, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale],f)\n",
    "        \n",
    "        gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_94800\\2652980509.py:62: RuntimeWarning: Mean of empty slice\n",
      "  ave_acc2 = np.nanmean(np.abs(np.array(it_acc2)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_94800\\2652980509.py:63: RuntimeWarning: Mean of empty slice\n",
      "  ave_acc = np.nanmean(np.abs(np.array(it_acc)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_94800\\2652980509.py:64: RuntimeWarning: Mean of empty slice\n",
      "  ave_val = np.nanmean(np.abs(np.array(it_val)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_94800\\2652980509.py:65: RuntimeWarning: Mean of empty slice\n",
      "  ave_prev = np.nanmean(np.abs(np.array(it_prev)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_94800\\2652980509.py:66: RuntimeWarning: Mean of empty slice\n",
      "  ave_train = np.nanmean(np.abs(np.array(it_train)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_94800\\2652980509.py:67: RuntimeWarning: Mean of empty slice\n",
      "  ave_times = np.nanmean(np.abs(np.array(it_times)),axis=0)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1670: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "ft = 'tdar'\n",
    "iter = 1\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "plot_mod = ['lda','alda','cnn','acnn03','avcnn','crcnn']\n",
    "plot_mod = ['lda','alda','cnn','avcnn']\n",
    "plot_mod = ['alda']\n",
    "for plot_i in range(1):\n",
    "    for sub in range(1):    \n",
    "        it_acc = []\n",
    "        it_recal = []\n",
    "        it_fail = []\n",
    "        it_val = []\n",
    "        it_prev = []\n",
    "        it_train = []\n",
    "        it_times = []\n",
    "        for it in range(iter):\n",
    "            # load or initialize cnn weights\n",
    "            if plot_i == 1:\n",
    "                with open('0323 full run pre and post/' + subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            else:\n",
    "                with open('0330 run/' + subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            \n",
    "            all_acc[all_acc==0] = np.nan\n",
    "            all_val[all_val==0] = np.nan\n",
    "            all_prev[all_prev==0] = np.nan\n",
    "            all_train[all_train==0] = np.nan\n",
    "            all_times[all_times==0] = np.nan\n",
    "\n",
    "            it_acc.append(all_acc)\n",
    "            it_recal.append(np.sum(all_recal==1,axis=0))\n",
    "            it_fail.append(np.sum(all_recal==-1,axis=0))\n",
    "            it_val.append(all_val)\n",
    "            it_prev.append(all_prev)\n",
    "            it_train.append(all_train)\n",
    "            it_times.append(all_times)\n",
    "\n",
    "            it_acc[it][:,:2] = it_acc[0][:,:2]\n",
    "            it_recal[it][:2] = it_recal[0][:2]\n",
    "            it_fail[it][:2] = it_fail[0][:2]\n",
    "            it_val[it][:,:2] = it_val[0][:,:2]\n",
    "            it_prev[it][:,:2] = it_prev[0][:,:2]\n",
    "            it_train[it][:,:2] = it_train[0][:,:2]\n",
    "            it_times[it][:,:2] = it_times[0][:,:2]\n",
    "\n",
    "\n",
    "        it_acc2 = cp.deepcopy(it_acc)\n",
    "        for i in range(len(it_acc2)):\n",
    "            x = it_val[i] < 0\n",
    "            # print(x.type)\n",
    "            # print(it_acc2[i].shape)\n",
    "            # print(ave_val.shape)\n",
    "            it_acc2[i][(it_acc[i]< 0) & (it_val[i] > 75)]= it_val[i][(it_acc[i]< 0)& (it_val[i] > 75)]\n",
    "\n",
    "        ave_acc2 = np.nanmean(np.abs(np.array(it_acc2)),axis=0)\n",
    "        ave_acc = np.nanmean(np.abs(np.array(it_acc)),axis=0)\n",
    "        ave_val = np.nanmean(np.abs(np.array(it_val)),axis=0)\n",
    "        ave_prev = np.nanmean(np.abs(np.array(it_prev)),axis=0)\n",
    "        ave_train = np.nanmean(np.abs(np.array(it_train)),axis=0)\n",
    "        ave_times = np.nanmean(np.abs(np.array(it_times)),axis=0)\n",
    "        ave_recal = np.nanmean(np.array(it_recal),axis=0)\n",
    "        ave_fail = np.nanmean(np.array(it_fail),axis=0)\n",
    "\n",
    "        std_acc2 = np.nanstd(np.abs(np.array(it_acc2)),axis=0)/np.sum(~np.isnan(np.array(it_acc2)),axis=0)\n",
    "        std_acc = np.nanstd(np.abs(np.array(it_acc)),axis=0)/np.sum(~np.isnan(np.array(it_acc)),axis=0)\n",
    "        std_val = np.nanstd(np.abs(np.array(it_val)),axis=0)/np.sum(~np.isnan(np.array(it_val)),axis=0)\n",
    "        std_prev = np.nanstd(np.abs(np.array(it_prev)),axis=0)/np.sum(~np.isnan(np.array(it_prev)),axis=0)\n",
    "        std_train = np.nanstd(np.abs(np.array(it_train)),axis=0)/np.sum(~np.isnan(np.array(it_train)),axis=0)\n",
    "        std_times = np.nanstd(np.abs(np.array(it_times)),axis=0)/np.sum(~np.isnan(np.array(it_times)),axis=0)\n",
    "        std_recal = np.nanstd(np.array(it_recal),axis=0)/np.sum(~np.isnan(np.array(it_recal)),axis=0)\n",
    "        std_fail = np.nanstd(np.array(it_fail),axis=0)/np.sum(~np.isnan(np.array(it_fail)),axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "        for mod in plot_mod:\n",
    "            plot_ind = mod_tot.index(mod)\n",
    "            # ax.plot(ave_acc[2:,plot_ind],'.-',label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            x = ~np.isnan(ave_val[:,plot_ind]) \n",
    "            # ave_acc2 = cp.deepcopy(ave_acc)\n",
    "            # ave_acc[x,plot_ind] = ave_val[x,plot_ind]\n",
    "\n",
    "            ax[0].plot(ave_acc[:,plot_ind],'.-',ms=8,label= mod + ' = '+ str(ave_recal[plot_ind]) + ' +/- ' + \"{:.2f}\".format(std_recal[plot_ind]) + ', ' + str(ave_fail[plot_ind]) +' failed')#str(std_recal[plot_ind,0]))\n",
    "            ax[0].plot(np.squeeze(np.where(x)),ave_acc[x,plot_ind],'kx',ms=12)\n",
    "            ax[1].plot(ave_acc2[:,plot_ind],'.-',ms=8,label= mod + ' = '+ str(ave_recal[plot_ind]) + ' +/- ' + \"{:.2f}\".format(std_recal[plot_ind])+ ', ' + str(ave_fail[plot_ind]) + ' failed')#+ str(std_recal[plot_ind,0]))\n",
    "            # ax.plot(np.squeeze(np.where(x)), ave_val[~np.isnan(ave_val[:,plot_ind]),plot_ind],'.-',ms=8,label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            # plt.fill_between(np.arange(ave_acc[2:,plot_ind].shape[0]),ave_acc[2:,plot_ind]-std_acc[2:,plot_ind],ave_acc[2:,plot_ind]+std_acc[2:,plot_ind],alpha=.3)\n",
    "        ax[1].legend()\n",
    "        for i in range(2):\n",
    "            ax[i].axhline(75, ls='--', color='grey')\n",
    "            ax[i].set_ylim([0,100])\n",
    "        # ax[1].axhline(75, ls='--', color='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(ave_val < 75,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 128\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn','acnn03','acnn30','acewc00','acewc30', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn03', 'avcnn15', 'acnnl03','crvcnn','acewclm','xtra','xtra1','xtra2']\n",
    "ft = 'feat'\n",
    "iter = 1\n",
    "\n",
    "for sub in range(4,5):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    mod_all = ['vcnn']\n",
    "\n",
    "    # load or initialize cnn weights\n",
    "    if load_mod:\n",
    "        with open(subs[sub] + '_' + str(0) + '_r_accs.p','rb') as f:\n",
    "            all_acc, all_recal, all_val, all_prev, all_train, all_times, _, _, c_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "    else:\n",
    "        c_weights = None\n",
    "        v_weights = None\n",
    "        v_wc = None\n",
    "        cl_wc = None\n",
    "        all_recal = np.empty((len(mod_tot),1))\n",
    "        all_recal[:] = np.nan\n",
    "        all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "    mod_i = 0\n",
    "    for mod in mod_all:\n",
    "        acc = np.zeros((len(all_files),5))\n",
    "        acc_val = np.zeros((len(all_files),5))\n",
    "        acc_prev = np.zeros((len(all_files),5))\n",
    "        acc_train = np.zeros((len(all_files),5))\n",
    "\n",
    "        if 'cnn' in mod:\n",
    "            acc_i = 2\n",
    "        elif 'cewc' in mod:\n",
    "            acc_i = 4\n",
    "        elif 'lda' in mod:\n",
    "            acc_i = 0\n",
    "\n",
    "        cnn = None\n",
    "        ewc = None\n",
    "\n",
    "        ep = 50\n",
    "        recal = 0\n",
    "        skip = False\n",
    "\n",
    "        # Loop through files\n",
    "        for i in range(1,2):#len(all_files)-1):\n",
    "            # load training file\n",
    "            train_file = all_files[i]\n",
    "            train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "            train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "            val_data = train_data\n",
    "            val_params = train_params\n",
    "\n",
    "            train_dof = np.unique(train_params[:,-1])\n",
    "            key = np.empty(train_dof.shape)\n",
    "            for key_i in range(len(train_dof)):\n",
    "                key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "            n_dof = int(np.max(key))\n",
    "            \n",
    "            train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "            val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key)\n",
    "\n",
    "            _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "\n",
    "            _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "            del train_data, train_params, val_data, val_params\n",
    "\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, print_b=True)\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=[cnn], n_dof=n_dof, ep=30, dec=False,print_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_clean_cnn)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    x_out[y_clean[:,cl]==1,...] = np.random.normal(np.mean(x_clean_cnn[y_clean[:,cl]==1,...],axis=0), np.std(x_clean_cnn[y_clean[:,cl]==1,...],axis=0),x_clean_cnn[y_clean[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_train_cnn)\n",
    "for cl in range(y_train.shape[1]):\n",
    "    x_out[y_train[:,cl]==1,...] = np.random.normal(np.mean(x_train_cnn[y_train[:,cl]==1,...],axis=0), np.std(x_train_cnn[y_train[:,cl]==1,...],axis=0),x_train_cnn[y_train[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = np.ones((1*y_clean.shape[0],8))\n",
    "x_out,_,_ = cnn.dec(samp1,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),samp=True)\n",
    "# _,x_out,_,_ = cnn(x_clean_cnn,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),dec=True)\n",
    "x_out = x_out.numpy()\n",
    "# for i in range(y_clean.shape[1]):\n",
    "#     adjust1 = np.std(x_clean_cnn[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     rescale = np.mean(adjust1)/np.mean(np.std(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0))\n",
    "#     gmean = np.mean(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     x_out[np.argmax(y_clean,axis=1)==i,...] = (x_out[np.argmax(y_clean,axis=1)==i,...] - gmean)*rescale + gmean\n",
    "# x_out = np.maximum(np.minimum(x_out,1),0)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    ind = np.tile(np.argmax(y_clean,axis=1)==cl,[1])\n",
    "    ind2 = np.argmax(y_clean,axis=1)==cl\n",
    "    x_temp = x_out[ind,...].reshape((np.sum(ind),-1))\n",
    "    x_true = x_clean_cnn[ind2,...].reshape((np.sum(ind2),-1))\n",
    "    # for i in range()\n",
    "    plt.figure()\n",
    "    # for i in range(x_true.shape[0]):\n",
    "    #     plt.plot(x_true[i,...],'k-')\n",
    "        \n",
    "    for i in range(x_temp.shape[0]):\n",
    "        plt.plot(x_temp[i,...],'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lda = x_clean_cnn.reshape(x_clean_cnn.shape[0],-1)\n",
    "y_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "y_train_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_lda,y_lda)\n",
    "y_out = dlda.predict(x_lda, w, c)\n",
    "print(dlda.eval_lda(w, c, x_lda, y_lda))\n",
    "x_out_lda = x_out.reshape(x_out.shape[0],-1)\n",
    "print(dlda.eval_lda(w,c, x_out_lda,np.tile(y_train_lda,[1,1])))\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_out_lda,np.tile(y_train_lda,[1,1]))\n",
    "print(dlda.eval_lda(w, c, x_out_lda,y_train_lda))\n",
    "print(dlda.eval_lda(w, c, x_lda, np.argmax(y_clean,axis=1)[...,np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn1,test_accuracy)\n",
    "print(lp.test_models(x_out, y_clean, None, None, cnn=cnn1, test_mod=test_mod, test_accuracy=test_accuracy))\n",
    "\n",
    "cnn2, _ = lp.train_models(traincnn=x_out,y_train=y_clean, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn2,test_accuracy)\n",
    "print(lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn2, test_mod=test_mod, test_accuracy=test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "fig,ax = plt.subplots(1,5,figsize=(30,4))\n",
    "for sub in range(2,3):#,5):\n",
    "    with open(subs[sub] + '_0_r_accs.p','rb') as f:\n",
    "        acc_all, recal_all, cur_all, prev_all, val_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "    # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "    colors =  cm.get_cmap('tab20c')\n",
    "    c = np.empty((20,4))\n",
    "    for i in range(20):\n",
    "        c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "    nn_c[0,-1] = 1\n",
    "    all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "    pt_m = ['ko-','o-','o-','o-','s','s','s','s','D']\n",
    "    nn_c = np.vstack((np.array([0,0,0,1]), c[0,:],c[1,:],c[2,:],c[3,:],c[4,:],c[5,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "    # nn_c[0,-1] = 1\n",
    "\n",
    "    labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "    labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "    # labels = mod_tot\n",
    "\n",
    "    ax_ind = sub\n",
    "    it = 0\n",
    "    for v in [1,2]: \n",
    "        i = mod_tot.index(mod_all[v])\n",
    "        acc_temp = acc_all[1:-1,i]\n",
    "        if not np.isnan(acc_temp).all():\n",
    "            x = np.arange(len(acc_temp))\n",
    "            recal_i = (acc_temp < 0)\n",
    "            ax[ax_ind].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v],color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "            it+=1\n",
    "\n",
    "    for i in range(5):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        \n",
    "        ax[i].set_ylim([0,100])\n",
    "        ax[i].set_title('TR' + str(i+1))\n",
    "    ax[0].legend()\n",
    "    ax[2].set_xlabel('Calibration Set')\n",
    "    ax[0].set_ylabel('Accuracy (%)')\n",
    "    plt.rc('font', size=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "for sub in range(2,3):#,5):\n",
    "    fig,ax = plt.subplots(1,4,figsize=(20,4))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, val_all,mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [1,0,0,1,2,2,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[it]+': ' + str(int(recal_all[i,0])),color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "                it+=1\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "cv_iter = 1\n",
    "for sub in range(0,5):\n",
    "    fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','o','*','o','s','D','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','f-cnn-5','f-cnn-3','f-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in [0, 3, 5, 4, 6, 7]: #range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v]+': ' + str(int(recal_all[i,0])),color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                it+=1\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "        \n",
    "\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e4d54467b05e62951c9fd7929782b99429e3b62c1a3b146d4f3dbf79f907e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('adapt_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
