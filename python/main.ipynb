{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from gpu import set_gpu\n",
    "import numpy as np\n",
    "import os\n",
    "import adapt.utils.data_utils as prd\n",
    "import adapt.loop as lp\n",
    "import adapt.ml.lda as dlda\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import adapt.ml.dl_subclass as dl\n",
    "import copy as cp\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import time\n",
    "from IPython import display\n",
    "import gc as gc\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR62\n",
      "train dof: [ 1  6 16 19 48 90], key: [0 1 2 3 4 5]\n",
      "training nn\n",
      "time: 34.897677421569824\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180524_161811, Accuracy: 99.74 , Val: 99.32 , Prev: 0.00 , Train: 99.32\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180525_084201, Accuracy: 82.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180531_073149, Accuracy: 89.48 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180602_105936, Accuracy: 80.53 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180604_090437, Accuracy: 74.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20180604_090437\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.3294105529785156\n",
      "WARNING:tensorflow:5 out of the last 17 calls to <function get_test.<locals>.test_step at 0x000002B7D72E2DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 18 calls to <function get_test.<locals>.test_step at 0x000002B7D72E2DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180604_214053, Accuracy: 81.26 , Val: 80.65 , Prev: 79.00 , Train: 100.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180605_080547, Accuracy: 80.95 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180606_174322, Accuracy: 62.94 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20180606_174322\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2733068466186523\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180606_174322, Test: 20180610_081839, Accuracy: 64.65 , Val: 88.45 , Prev: 72.84 , Train: 100.00\n",
      "recal: 3 20180610_081839\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.4180331230163574\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180612_085047, Accuracy: 78.50 , Val: 87.41 , Prev: 85.85 , Train: 100.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180614_081624, Accuracy: 77.98 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180610_081839, Test: 20180616_182930, Accuracy: 70.48 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 4 20180616_182930\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.3320984840393066\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180616_182930, Test: 20180617_184248, Accuracy: 60.85 , Val: 89.07 , Prev: 78.04 , Train: 99.90\n",
      "recal: 5 20180617_184248\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.4376368522644043\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180619_193541, Accuracy: 82.77 , Val: 52.70 , Prev: 84.29 , Train: 99.58\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073139, Accuracy: 82.30 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073838, Accuracy: 73.61 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 6 20180620_073838\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.290897846221924\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_073838, Test: 20180620_083903, Accuracy: 68.19 , Val: 85.74 , Prev: 69.51 , Train: 100.00\n",
      "recal: 7 20180620_083903\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.3355863094329834\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180621_094013, Accuracy: 80.79 , Val: 85.95 , Prev: 86.89 , Train: 98.44\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180623_200107, Accuracy: 69.96 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 8 20180623_200107\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 4.770328521728516\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180703_072425, Accuracy: 78.66 , Val: 82.31 , Prev: 70.86 , Train: 99.48\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180711_074144, Accuracy: 74.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 9 20180711_074144\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.0050909519195557\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_074144, Test: 20180711_113445, Accuracy: 72.41 , Val: 77.42 , Prev: 83.04 , Train: 100.00\n",
      "recal: 10 20180711_113445\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.4977328777313232\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180713_102029, Accuracy: 76.16 , Val: 84.50 , Prev: 62.43 , Train: 99.27\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180717_073851, Accuracy: 71.00 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 11 20180717_073851\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.5639102458953857\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180717_073851, Test: 20180717_112511, Accuracy: 84.64 , Val: 85.76 , Prev: 49.95 , Train: 99.37\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180717_073851, Test: 20190321_073718, Accuracy: 42.99 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 12 20190321_073718\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.5171878337860107\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_073718, Test: 20190321_095657, Accuracy: 65.91 , Val: 77.09 , Prev: 77.34 , Train: 98.70\n",
      "recal: 13 20190321_095657\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.396090507507324\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_095657, Test: 20190325_084612, Accuracy: 56.12 , Val: 70.76 , Prev: 77.00 , Train: 99.53\n",
      "recal: 14 20190325_084612\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.5334150791168213\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190325_084612, Test: 20190326_073702, Accuracy: 50.21 , Val: 64.90 , Prev: 44.60 , Train: 99.81\n",
      "recal: 15 20190326_073702\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.9505486488342285\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190328_125619, Accuracy: 73.15 , Val: 82.06 , Prev: 83.33 , Train: 99.07\n",
      "recal: 16 20190328_125619\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.6483609676361084\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190328_125619, Test: 20190331_112350, Accuracy: 81.97 , Val: 76.42 , Prev: 76.67 , Train: 99.81\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190328_125619, Test: 20190402_073436, Accuracy: 65.16 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 17 20190402_073436\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.949024200439453\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_073436, Test: 20190402_142249, Accuracy: 66.19 , Val: 81.19 , Prev: 54.97 , Train: 99.81\n",
      "recal: 18 20190402_142249\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.7716121673583984\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190403_050414, Accuracy: 77.50 , Val: 82.03 , Prev: 72.81 , Train: 98.79\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190403_052115, Accuracy: 86.17 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190404_075938, Accuracy: 74.80 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 19 20190404_075938\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.4684910774230957\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190404_085916, Accuracy: 77.07 , Val: 74.30 , Prev: 78.68 , Train: 99.81\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190406_091945, Accuracy: 68.94 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 20 20190406_091945\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.925771951675415\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190410_102236, Accuracy: 81.97 , Val: 85.32 , Prev: 80.54 , Train: 99.64\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190410_102641, Accuracy: 79.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190415_073441, Accuracy: 88.68 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190415_102702, Accuracy: 78.03 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190423_063337, Accuracy: 79.09 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190424_045927, Accuracy: 81.14 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190502_102725, Accuracy: 83.15 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190505_082637, Accuracy: 80.76 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190406_091945, Test: 20190505_083227, Accuracy: 72.57 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 21 20190505_083227\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.787590742111206\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190505_083227, Test: 20190518_142954, Accuracy: 70.47 , Val: 77.47 , Prev: 84.95 , Train: 99.25\n",
      "------------------------acnn05 17 - 4 -- 0-------------\n",
      "train dof: [ 1  6 16 19 48 90], key: [0 1 2 3 4 5]\n",
      "setting CNN weights\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180524_161811, Accuracy: 99.74 , Val: 99.32 , Prev: 0.00 , Train: 99.04\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180525_084201, Accuracy: 82.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180531_073149, Accuracy: 89.48 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180602_105936, Accuracy: 80.53 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180521_090336, Test: 20180604_090437, Accuracy: 74.65 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 1 20180604_090437\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.3149633407592773\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180604_214053, Accuracy: 81.73 , Val: 84.29 , Prev: 81.03 , Train: 99.69\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180605_080547, Accuracy: 80.79 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180604_090437, Test: 20180606_174322, Accuracy: 65.17 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 2 20180606_174322\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.0144498348236084\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180606_174322, Test: 20180610_081839, Accuracy: 77.82 , Val: 86.16 , Prev: 75.03 , Train: 100.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180606_174322, Test: 20180612_085047, Accuracy: 72.77 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 3 20180612_085047\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.7010388374328613\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180612_085047, Test: 20180614_081624, Accuracy: 79.18 , Val: 84.60 , Prev: 81.58 , Train: 100.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180612_085047, Test: 20180616_182930, Accuracy: 72.83 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 4 20180616_182930\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.207425832748413\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180616_182930, Test: 20180617_184248, Accuracy: 59.50 , Val: 87.30 , Prev: 83.45 , Train: 99.48\n",
      "recal: 5 20180617_184248\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.5222086906433105\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180619_193541, Accuracy: 78.08 , Val: 51.98 , Prev: 80.85 , Train: 99.37\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073139, Accuracy: 79.18 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180617_184248, Test: 20180620_073838, Accuracy: 73.09 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 6 20180620_073838\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.4015450477600098\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_073838, Test: 20180620_083903, Accuracy: 73.09 , Val: 85.85 , Prev: 74.40 , Train: 100.00\n",
      "recal: 7 20180620_083903\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.2972700595855713\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180621_094013, Accuracy: 82.56 , Val: 86.47 , Prev: 89.07 , Train: 98.54\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180620_083903, Test: 20180623_200107, Accuracy: 68.97 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 8 20180623_200107\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.7789924144744873\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180703_072425, Accuracy: 79.07 , Val: 83.56 , Prev: 74.40 , Train: 99.69\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180711_074144, Accuracy: 77.67 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180623_200107, Test: 20180711_113445, Accuracy: 55.34 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 9 20180711_113445\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.38486909866333\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180713_102029, Accuracy: 79.70 , Val: 84.60 , Prev: 75.75 , Train: 99.69\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180717_073851, Accuracy: 75.74 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 19 48 90]\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180711_113445, Test: 20180717_112511, Accuracy: 67.15 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 10 20180717_112511\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 19 48 90]\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.5467193126678467\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20180717_112511, Test: 20190321_073718, Accuracy: 60.69 , Val: 86.37 , Prev: 67.12 , Train: 98.96\n",
      "recal: 11 20190321_073718\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.9813907146453857\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_073718, Test: 20190321_095657, Accuracy: 66.98 , Val: 76.72 , Prev: 82.73 , Train: 99.44\n",
      "recal: 12 20190321_095657\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.6972014904022217\n",
      "bad recal\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190321_095657, Test: 20190325_084612, Accuracy: 54.91 , Val: 64.99 , Prev: 73.84 , Train: 99.35\n",
      "recal: 13 20190325_084612\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.8345563411712646\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190325_084612, Test: 20190326_073702, Accuracy: 74.38 , Val: 78.58 , Prev: 78.31 , Train: 99.53\n",
      "recal: 14 20190326_073702\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.924971103668213\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190326_073702, Test: 20190328_125619, Accuracy: 72.04 , Val: 79.09 , Prev: 73.84 , Train: 98.88\n",
      "recal: 15 20190328_125619\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.6622767448425293\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190328_125619, Test: 20190331_112350, Accuracy: 81.04 , Val: 77.81 , Prev: 76.58 , Train: 98.88\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190328_125619, Test: 20190402_073436, Accuracy: 74.01 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 16 20190402_073436\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.7970685958862305\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_073436, Test: 20190402_142249, Accuracy: 69.73 , Val: 80.07 , Prev: 66.57 , Train: 99.81\n",
      "recal: 17 20190402_142249\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.369290351867676\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190402_142249, Test: 20190403_050414, Accuracy: 73.78 , Val: 82.12 , Prev: 73.28 , Train: 98.23\n",
      "recal: 18 20190403_050414\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.840291738510132\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190403_050414, Test: 20190403_052115, Accuracy: 86.49 , Val: 82.40 , Prev: 75.14 , Train: 100.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190403_050414, Test: 20190404_075938, Accuracy: 73.96 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 19 20190404_075938\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 5.144836664199829\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190404_085916, Accuracy: 83.42 , Val: 77.84 , Prev: 81.75 , Train: 100.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190406_091945, Accuracy: 77.59 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190410_102236, Accuracy: 80.72 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190404_075938, Test: 20190410_102641, Accuracy: 74.76 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 20 20190410_102641\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.702694892883301\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190410_102641, Test: 20190415_073441, Accuracy: 86.54 , Val: 88.08 , Prev: 79.61 , Train: 99.44\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190410_102641, Test: 20190415_102702, Accuracy: 75.00 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190410_102641, Test: 20190423_063337, Accuracy: 80.72 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190410_102641, Test: 20190424_045927, Accuracy: 74.38 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 21 20190424_045927\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.7753918170928955\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190424_045927, Test: 20190502_102725, Accuracy: 72.02 , Val: 85.20 , Prev: 71.88 , Train: 99.53\n",
      "recal: 22 20190502_102725\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 2.514349937438965\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190502_102725, Test: 20190505_082637, Accuracy: 80.25 , Val: 91.35 , Prev: 76.07 , Train: 98.51\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190502_102725, Test: 20190505_083227, Accuracy: 73.64 , Val: 0.00 , Prev: 0.00 , Train: 0.00\n",
      "recal: 23 20190505_083227\n",
      "prev: [ 1  6 16 19 48 90]\n",
      "cur: [ 1  6 16 17 19 48 90]\n",
      "removing train 17\n",
      "train dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "training nn\n",
      "time: 3.056654453277588\n",
      "init test dof: [ 1  6 16 17 19 48 90]\n",
      "removing extra test DOF 17\n",
      "test_dof: [ 1  6 16 19 48 90], key: [0. 1. 2. 3. 4. 5.]\n",
      "Set: 20190505_083227, Test: 20190518_142954, Accuracy: 69.12 , Val: 79.61 , Prev: 78.98 , Train: 99.53\n",
      "------------------------avcnn 21 - 2 -- 0-------------\n"
     ]
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn2','acnn05','acnn30','acewc30','acewc15', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn', 'avcnn15', 'acnnl03','crvcnn','acewclm','crcnn','acewc00','xtra2']\n",
    "ft = 'tdar'\n",
    "iter = 10\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_mod = 0\n",
    "\n",
    "for it in range(1):#iter):\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    if it == 0:\n",
    "        mod_all = ['blda','lda','alda','cnn','acnn05','avcnn']#,'crcnn']\n",
    "        mod_all = ['lda','alda','acnn05','avcnn']\n",
    "        mod_all = ['acnn05','avcnn']\n",
    "    else:\n",
    "        mod_all = ['bcnn','cnn','acnn05','avcnn']\n",
    "\n",
    "    for sub in range(4,5):\n",
    "        print(subs[sub])\n",
    "        sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "        all_files = os.listdir(sub_path)\n",
    "        if 'skip' in all_files:\n",
    "            all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "        # load or initialize cnn weights\n",
    "        if load_mod:\n",
    "            with open(subs[sub] + '_' + str(it) + '_r_accs1.p','rb') as f:\n",
    "                all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof,_, _, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        else:\n",
    "            c_weights = None\n",
    "            v_weights = None\n",
    "            v_wc = None\n",
    "            cl_wc = None\n",
    "            scaler_0 = None\n",
    "            all_recal = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "            all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "            rows, cols = (len(all_files), len(mod_tot))\n",
    "            all_dof = [[0]*cols]*rows\n",
    "\n",
    "        mod_i = 0\n",
    "        for mod in mod_all:\n",
    "            acc = np.zeros((len(all_files),2))\n",
    "            acc_val = np.zeros((len(all_files),2))\n",
    "            acc_prev = np.zeros((len(all_files),2))\n",
    "            acc_train = np.zeros((len(all_files),2))\n",
    "            mod_recal = np.zeros((len(all_files),))\n",
    "\n",
    "            if 'lda' in mod:\n",
    "                acc_i = 0\n",
    "            else:\n",
    "                acc_i = 1\n",
    "\n",
    "            cnn = None\n",
    "            ewc = None\n",
    "            clda = None\n",
    "\n",
    "            ep = 30\n",
    "            recal = 0\n",
    "            skip_recal = 0\n",
    "            skip = False\n",
    "\n",
    "            # Loop through files\n",
    "            for i in range(0,len(all_files)-1):\n",
    "                # Check if need to recalibrate\n",
    "                if i > 0:\n",
    "                    if acc[i,acc_i] < 75:\n",
    "                        skip = False\n",
    "                        recal += 1\n",
    "                        print('recal: ' + str(recal) + ' ' + all_files[i])\n",
    "                        acc[i,acc_i] *= -1\n",
    "                        mod_recal[i] = 1\n",
    "                    else:\n",
    "                        skip = True\n",
    "                    \n",
    "                    if 'b' in mod:\n",
    "                        skip = True\n",
    "                        \n",
    "                if not skip:\n",
    "                    # load training file\n",
    "                    train_file = all_files[i]\n",
    "                    train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "                    # if first train, use two train files\n",
    "                    if i == 0:\n",
    "                        # load training file\n",
    "                        train_file2 = all_files[i+1]\n",
    "                        train_data2, train_params2 = prd.load_caps_train(sub_path + train_file2 + '/traindata.mat')\n",
    "                        train_data = np.vstack((train_data,train_data2))\n",
    "                        train_params = np.vstack((train_params,train_params2))\n",
    "                        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "                        val_data, val_params = train_data, train_params\n",
    "                    else:\n",
    "                        train_data, train_params, _ = prd.threshold(train_data, train_params,th)\n",
    "                    \n",
    "                        tr_i = np.zeros((train_params.shape[0],))\n",
    "                        te_i = np.zeros((train_params.shape[0],))\n",
    "                        for cls in np.unique(train_params[:,-1]):\n",
    "                            dof = np.array(np.where(train_params[:,-1] == cls))\n",
    "                            tr_i[dof[0,:dof.shape[1]//2]] = 1\n",
    "                            te_i[dof[0,dof.shape[1]//2:]] = 1\n",
    "\n",
    "                        train_temp = train_data[tr_i.astype(bool),...]\n",
    "                        params_temp = train_params[tr_i.astype(bool),...]\n",
    "                        val_data = train_data[te_i.astype(bool),...]\n",
    "                        val_params = train_params[te_i.astype(bool),...]\n",
    "                        \n",
    "                        train_data, train_params = train_temp, params_temp\n",
    "                        del train_temp, params_temp, tr_i, te_i\n",
    "                        \n",
    "                    # if combining, save current training data\n",
    "                    # if 'cr' in mod:\n",
    "                    #     # combine old and new training data\n",
    "                    #     if i > 1:\n",
    "                    #         train_data = np.vstack((train_data_0,train_data))\n",
    "                    #         train_params = np.vstack((train_params_0,train_params))\n",
    "\n",
    "                    #     train_data_0 = cp.deepcopy(train_data)\n",
    "                    #     train_params_0 = cp.deepcopy(train_params)\n",
    "\n",
    "                    # if (i == 1 and mod[0] == 'a') or (i == 1 and mod[:2] == 'cr') or (mod[0] != 'a' and mod[:2] != 'cr'):\n",
    "                        \n",
    "                    if i > 0:\n",
    "                        # get previous dofs\n",
    "                        prev_ndof = [n_dof, key, train_dof]\n",
    "                        if mod[0] == 'a':\n",
    "                            print('prev: ' + str(train_dof))\n",
    "                            # get current dofs and create key\n",
    "                            train_dof = np.unique(train_params[:,-1])\n",
    "                            print('cur: ' + str(train_dof))\n",
    "                            key = np.zeros((len(train_dof),))\n",
    "                            # check if current dofs are all in old dof list\n",
    "                            dof_ovlp = np.isin(train_dof,prev_ndof[2],assume_unique=True)\n",
    "                            temp_dof = cp.deepcopy(train_dof)\n",
    "                            # loop through dofs that are in previous dofs, set the keys\n",
    "                            for dof in train_dof[dof_ovlp]:\n",
    "                                key[train_dof==dof] = prev_ndof[1][prev_ndof[2]==dof]\n",
    "                                temp_dof[train_dof==dof] = prev_ndof[2][prev_ndof[2]==dof]\n",
    "\n",
    "                            # check if previous dofs has classes not in this set\n",
    "                            dof_xtra = ~np.isin(prev_ndof[2],temp_dof,assume_unique=True)\n",
    "                            temp_dof = np.hstack((temp_dof,prev_ndof[2][dof_xtra]))\n",
    "                            key = np.hstack((key,prev_ndof[1][dof_xtra]))\n",
    "\n",
    "                            # loop through dofs that are not in previous dofs (ie new classes), add keys\n",
    "                            key_i = 1\n",
    "                            xtra = False\n",
    "                            for dof in train_dof[~dof_ovlp]:\n",
    "                                # if 'lda' in mod:\n",
    "                                # xtra = True\n",
    "                                # key[temp_dof==dof] = np.max(key) + 1\n",
    "                                    # key_i += 1\n",
    "                                # else:\n",
    "                                    # remove extras\n",
    "                                print('removing train ' + str(dof))\n",
    "                                ind = train_params[:,-1] == dof\n",
    "                                train_params = train_params[~ind,...]\n",
    "                                train_data = train_data[~ind,...]\n",
    "                                ind = val_params[:,-1] == dof\n",
    "                                val_params = val_params[~ind,...]\n",
    "                                val_data = val_data[~ind,...]\n",
    "                                key = np.delete(key,temp_dof==dof)\n",
    "                                temp_dof = np.delete(temp_dof,temp_dof==dof)\n",
    "\n",
    "                            train_dof = cp.deepcopy(temp_dof)\n",
    "                        else:\n",
    "                            # get current dofs and create key\n",
    "                            train_dof = np.unique(train_params[:,-1])\n",
    "                            key = np.arange(len(train_dof))\n",
    "                    else:\n",
    "                        # get current dofs and create key\n",
    "                        train_dof = np.unique(train_params[:,-1])\n",
    "                        key = np.arange(len(train_dof))\n",
    "\n",
    "                    n_dof = len(train_dof)\n",
    "                    all_dof[i][mod_tot.index(mod)] = train_dof\n",
    "\n",
    "                    train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key,False)\n",
    "                    val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key,False)\n",
    "\n",
    "                    print('train dof: ' + str(train_dof) + ', key: ' + str(key))\n",
    "\n",
    "                    if (mod[0] == 'a' and i > 0) or ('cr' in mod and i > 0) or (mod == 'vcnn' and i > 0):\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, _, _, _, _ = prd.prep_train_caps(train_data, train_params, emg_scale=emg_scale, scaler=scaler, num_classes=n_dof, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False)\n",
    "                    else:\n",
    "                        _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "                        if ((i == 0) and (c_weights is not None)) or ((i == 0) and (v_weights is not None)):\n",
    "                            scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                    _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "                    if 'cr' in mod:\n",
    "                        # combine old and new training data\n",
    "                        if i > 0:\n",
    "                            x_clean_cnn = np.vstack((clean_data_0,x_clean_cnn))\n",
    "                            y_clean = np.vstack((clean_params_0,y_clean))\n",
    "                            x_train_cnn = np.vstack((x_clean_cnn,x_train_cnn))\n",
    "                            y_train = np.vstack((y_clean,y_train))\n",
    "\n",
    "                    del train_data, train_params, val_data, val_params\n",
    "\n",
    "                    if 'lda' not in mod:\n",
    "                        cnnlda = 'l' in mod\n",
    "                        if i == 0:\n",
    "                            if c_weights is None:\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda, bn_training=True, prog_train=True)\n",
    "                                c_weights = cp.deepcopy([cnn.enc.get_weights(),cnn.clf.get_weights()])\n",
    "                                scaler_0 = cp.deepcopy(scaler)    \n",
    "                            else:\n",
    "                                print('setting CNN weights')\n",
    "                                cnn = dl.CNN(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                                if cnnlda:\n",
    "                                    print('setting LDA weights')\n",
    "                                    w_c = cp.deepcopy(cl_wc[0].astype('float32'))\n",
    "                                    c_c = cp.deepcopy(cl_wc[1].astype('float32'))\n",
    "                            if 'ewc' in mod:\n",
    "                                cnn = dl.EWC(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                            if 'ad' in mod:\n",
    "                                cnn = dl.CNN(n_class=n_dof,adapt=True)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                cnn.enc.set_weights(c_weights[0])\n",
    "                                cnn.clf.set_weights(c_weights[1])\n",
    "                            if 'av' in mod:\n",
    "                                mu_class, std_class, N = prd.set_mean(x_clean_cnn,y_clean)\n",
    "\n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                            \n",
    "                            if scaler_0 is None:\n",
    "                                scaler_0 = cp.deepcopy(scaler)\n",
    "                            else:\n",
    "                                scaler = cp.deepcopy(scaler_0)\n",
    "\n",
    "                        else:\n",
    "                            prev_w = cnn.get_weights()\n",
    "                            if xtra and mod[0] == 'a':\n",
    "                                print('add neuron')\n",
    "                                cnn = dl.CNN(n_class=n_dof)\n",
    "                                cnn(x_train_cnn[:1,...])\n",
    "                                w_temp = cnn.get_weights()\n",
    "                                w_temp[:-2] = prev_w[:-2]\n",
    "                                w_temp[-2][:,:-1] = prev_w[-2]\n",
    "                                w_temp[-1][:-1] = prev_w[-1]\n",
    "                                cnn.set_weights(w_temp)\n",
    "                            if 'adcnn' in mod: # adapt first layer only\n",
    "                                # cnn.base.trainable=False\n",
    "                                cnn.clf.trainable=False\n",
    "                                ep = int(mod[-2:])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], _, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], adapt=True, cnnlda=cnnlda, lr=0.00001)\n",
    "                            elif 'avcnn' in mod:\n",
    "                                # prev_w = cnn.get_weights()\n",
    "                                # prev_w = cnn.get_weights()\n",
    "                                prev_mu = [mu_class, std_class, N]\n",
    "                                # generate old training data, same size as clean data\n",
    "                                # x_out = cp.deepcopy(x_clean_cnn)\n",
    "                                # num_y = x_out.shape[0]//prev_ndof[0]\n",
    "                                # y_gen = np.zeros((x_out.shape[0],n_dof))\n",
    "                                # for cl in prev_ndof[1]:\n",
    "                                #     cl_i = prev_ndof[1]==cl\n",
    "                                #     y_gen[int(cl*num_y):int((cl+1)*num_y),np.where(prev_ndof[1]==cl)] = 1\n",
    "                                #     x_out[int(cl*num_y):int((cl+1)*num_y),...] = np.random.normal(mu_class[cl_i], std_class[cl_i],x_clean_cnn[:num_y,...].shape)\n",
    "                                \n",
    "                                x_out = cp.deepcopy(x_clean_cnn)\n",
    "                                num_y = x_out.shape[0]//prev_ndof[0]\n",
    "                                y_gen = np.zeros((x_out.shape[0],n_dof))\n",
    "                                y_xtra = 0\n",
    "                                x_xtra = 0\n",
    "                                for cl in prev_ndof[1]:\n",
    "                                    cl_i = np.where(prev_ndof[1]==cl)\n",
    "                                    x_ind = np.squeeze(y_clean[:,cl_i]==1)\n",
    "                                    if np.sum(x_ind) > 0:\n",
    "                                        y_gen[x_ind,np.where(prev_ndof[1]==cl)] = 1\n",
    "                                        x_out[x_ind,...] = np.random.normal(mu_class[cl_i], std_class[cl_i],x_clean_cnn[x_ind,...].shape)\n",
    "                                    else:\n",
    "                                        temp = np.zeros((num_y,n_dof))\n",
    "                                        temp[:,cl_i] = 1\n",
    "                                        if isinstance(y_xtra,np.ndarray):\n",
    "                                            y_xtra = np.vstack((y_xtra,temp))\n",
    "                                            x_xtra = np.vstack((x_xtra,np.random.normal(mu_class[cl_i], std_class[cl_i],x_clean_cnn[:num_y,...].shape)))\n",
    "                                        else:\n",
    "                                            y_xtra = cp.deepcopy(temp)\n",
    "                                            x_xtra = np.random.normal(mu_class[cl_i], std_class[cl_i],x_clean_cnn[:num_y,...].shape)\n",
    "                                \n",
    "                                # for cl in prev_ndof[1].astype(int):\n",
    "                                #     cl_i = np.where(prev_ndof[1]==cl)\n",
    "                                #     x_ind = np.squeeze(y_clean[:,cl]==1)\n",
    "                                #     if np.sum(x_ind) > 0:\n",
    "                                #         y_gen[x_ind,np.where(prev_ndof[1]==cl)] = 1\n",
    "                                #         x_out[x_ind,...] = np.random.normal(mu_class[cl], std_class[cl],x_clean_cnn[x_ind,...].shape)\n",
    "                                #     else:\n",
    "                                #         temp = np.zeros((num_y,n_dof))\n",
    "                                #         temp[:,cl] = 1\n",
    "                                #         if isinstance(y_xtra,np.ndarray):\n",
    "                                #             y_xtra = np.vstack((y_xtra,temp))\n",
    "                                #             x_xtra = np.vstack((x_xtra,np.random.normal(mu_class[cl], std_class[cl],x_clean_cnn[:num_y,...].shape)))\n",
    "                                #         else:\n",
    "                                #             y_xtra = cp.deepcopy(temp)\n",
    "                                #             x_xtra = np.random.normal(mu_class[cl_i], std_class[cl],x_clean_cnn[:num_y,...].shape)\n",
    "\n",
    "                                if isinstance(y_xtra,np.ndarray):\n",
    "                                    x_out = np.vstack((x_out,x_xtra))\n",
    "                                    y_gen = np.vstack((y_gen,y_xtra))\n",
    "\n",
    "                                x_train_aug = np.vstack((x_out,x_train_cnn))\n",
    "                                y_train_aug = np.vstack((y_gen,y_train))\n",
    "\n",
    "                                # x_train_m = np.vstack((x_out,x_clean_cnn))\n",
    "                                # y_train_m = np.vstack((y_gen,y_clean))\n",
    "\n",
    "                                # for cl in key.astype(int):\n",
    "                                #     cl_i = np.where(key==cl)\n",
    "                                #     x_ind = np.squeeze(y_train_m[:,cl_i]==1)\n",
    "                                #     mu_class[cl_i] = np.nanmean(x_train_m[x_ind,...])\n",
    "                                #     std_class[cl_i] = np.nanstd(x_train_m[x_ind,...])\n",
    "                                # mu_class, std_class, N = prd.set_mean(x_clean_cnn,y_clean)\n",
    "\n",
    "                                mu_class, std_class, N = prd.update_mean(x_clean_cnn,y_clean,N,mu_class,std_class,key,prev_ndof[1])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)],_,_ = lp.train_models(traincnn=x_train_aug,y_train=y_train_aug, mod=[cnn], n_dof=n_dof, ep=5, dec=False, lr=0.00001, bn_training=False, bn_trainable=False, prog_train=xtra)\n",
    "                            elif 'acnn' in mod: # update whole CNN and lda weights\n",
    "                                ep = int(mod[-2:])\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, n_dof=n_dof, ep=ep, mod=[cnn], cnnlda=cnnlda, lr=0.00001, bn_training=False, bn_trainable=False, prog_train=xtra)\n",
    "                            elif 'cnn' in mod: # recalibrate cnnlda\n",
    "                                cnn, all_times[i,mod_tot.index(mod)], w_c, c_c = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['cnn'], n_dof=n_dof, ep=ep, cnnlda=cnnlda, bn_training=True, prog_train=True)\n",
    "                            elif 'acewc' in mod:\n",
    "                                w_c, c_c, all_times[i,mod_tot.index(mod)] = lp.train_task(cnn, 15, 1, x_train_cnn, y_train, [prev_x, x_val_cnn],[prev_y, y_val], lams=[int(mod[-2:])], bat=bat, cnnlda=cnnlda)\n",
    "                            \n",
    "                            if 'l' in mod:\n",
    "                                clda = [w_c, c_c]\n",
    "                        del test_mod\n",
    "                        test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                        if i > 0:\n",
    "                            acc_prev[i,:] = lp.test_models(prev_x, prev_y, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_val[i,:] = lp.test_models(x_val_cnn, y_val, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        acc_train[i,:] = lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "                        \n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            mod_recal[i] = -1\n",
    "                            print('bad recal')\n",
    "                            # if acc_val[i,acc_i] < 75:\n",
    "                            #     if i > 0:\n",
    "                            #         n_dof, key, train_dof = prev_ndof\n",
    "                            #         if 'vcnn' in mod:\n",
    "                            #             mu_class, std_class, N = prev_mu\n",
    "                            #         cnn = dl.CNN(n_class = n_dof)\n",
    "                            #         cnn(x_train_cnn[:1,...])\n",
    "                            #         cnn.set_weights(prev_w)\n",
    "                            #         del test_mod\n",
    "                            #         test_mod = dl.get_test(cnn, test_accuracy)\n",
    "                            # else:\n",
    "                            #     print('keeping new model')\n",
    "                            #     mod_recal[i] = -2\n",
    "                        elif 'cr' in mod:\n",
    "                            clean_data_0 = cp.deepcopy(x_clean_cnn)\n",
    "                            clean_params_0 = cp.deepcopy(y_clean)\n",
    "                        if 'ewc' in mod: \n",
    "                            cnn.compute_fisher(x_train_cnn, y_train, num_samples=200, plot_diffs=False) \n",
    "                            cnn.star()\n",
    "                    else:\n",
    "                        if i == 0:\n",
    "                            N = np.zeros((n_dof),)\n",
    "                            cov_class = np.zeros([x_train_lda.shape[1],x_train_lda.shape[1]])\n",
    "                            mu_class = np.zeros([n_dof,x_train_lda.shape[1]])\n",
    "                        prev_lda = [mu_class,cov_class,N]\n",
    "                        start_time = time.time()\n",
    "                        if mod[0] != 'a' or (i == 0 and mod[0] == 'a'):\n",
    "                            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda, key)\n",
    "                        else:\n",
    "                            w, c, mu_class, cov_class, N = dlda.update_lda(x_train_lda, y_train_lda, N, mu_class, cov_class, key, prev_ndof[1])\n",
    "                        all_times[i,mod_tot.index(mod)] = time.time() - start_time\n",
    "\n",
    "                        acc_val[i,:],out = lp.test_models(None, None, x_val_lda, y_val_lda, lda=[w,c])\n",
    "                        acc_train[i,:],out = lp.test_models(None, None, x_train_lda, y_train_lda, lda=[w,c])\n",
    "                        if i > 0:\n",
    "                            acc_prev[i,:],out = lp.test_models(None, None, prev_x_lda, prev_y_lda, lda=[w,c])\n",
    "                        if acc_val[i,acc_i] < 75:\n",
    "                            mod_recal[i] = -1\n",
    "                            print('bad recal')\n",
    "                            # if acc_val[i,acc_i] < 75:\n",
    "                            #     if i > 0:\n",
    "                            #         mu_class, cov_class, N = prev_lda\n",
    "                            #         n_dof, key, train_dof = prev_ndof\n",
    "                            # else:\n",
    "                            #     print('keeping new model')\n",
    "                            #     mod_recal[i] = -2\n",
    "                        del x_train_lda, y_train_lda\n",
    "                    \n",
    "                    if mod_recal[i] != -1 or i == 0 :\n",
    "                        prev_x = cp.deepcopy(x_val_cnn)\n",
    "                        prev_y = cp.deepcopy(y_val)\n",
    "                        prev_x_lda = cp.deepcopy(x_val_lda)\n",
    "                        prev_y_lda = cp.deepcopy(y_val_lda)\n",
    "                    \n",
    "                    del x_train_cnn, y_train, x_val_cnn, y_val, x_val_lda, y_val_lda, x_clean_cnn, y_clean\n",
    "                \n",
    "                # load data\n",
    "                test_file = all_files[i+1]\n",
    "                test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "                \n",
    "                # check class labels\n",
    "                test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "                test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key,True)\n",
    "\n",
    "                # test \n",
    "                y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "                \n",
    "                # test \n",
    "                if 'lda' in mod:\n",
    "                    acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "                else:\n",
    "                    acc[i+1,:] = lp.test_models(x_test_cnn, y_test, x_test_lda, y_test_lda, cnn=cnn, clda=clda, test_mod=test_mod, test_accuracy=test_accuracy)\n",
    "\n",
    "                print ('Set: ' + train_file + ', Test: ' + test_file + ',', f'Accuracy: {acc[i+1,acc_i]:.2f}', f', Val: {acc_val[i,acc_i]:.2f}', f', Prev: {acc_prev[i,acc_i]:.2f}', f', Train: {acc_train[i,acc_i]:.2f}')\n",
    "                del y_test, x_test_cnn, x_test_lda, y_test_lda#, test_data, test_params\n",
    "\n",
    "            all_acc[:,mod_tot.index(mod)] = acc[:,acc_i]\n",
    "            all_val[:,mod_tot.index(mod)] = acc_val[:,acc_i]\n",
    "            all_prev[:,mod_tot.index(mod)] = acc_prev[:,acc_i]\n",
    "            all_train[:,mod_tot.index(mod)] = acc_train[:,acc_i]\n",
    "            all_recal[:,mod_tot.index(mod)] = mod_recal\n",
    "\n",
    "            print('------------------------' + mod + ' ' + str(np.sum(mod_recal==1)) + ' - ' + str(np.sum(mod_recal==-1)) + ' -- ' + str(np.sum(mod_recal==-2)) + '-------------')\n",
    "            mod_i += 1\n",
    "\n",
    "            # if 'cr' in mod:\n",
    "            #     del train_data_0, train_params_0\n",
    "\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','wb') as f:\n",
    "            pickle.dump([all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof, mod_all, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale],f)\n",
    "        \n",
    "        gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:69: RuntimeWarning: Mean of empty slice\n",
      "  ave_acc2 = np.nanmean(np.abs(np.array(it_acc2)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:70: RuntimeWarning: Mean of empty slice\n",
      "  ave_acc = np.nanmean(np.abs(np.array(it_acc)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:71: RuntimeWarning: Mean of empty slice\n",
      "  ave_val = np.nanmean(np.abs(np.array(it_val)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:72: RuntimeWarning: Mean of empty slice\n",
      "  ave_prev = np.nanmean(np.abs(np.array(it_prev)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:73: RuntimeWarning: Mean of empty slice\n",
      "  ave_train = np.nanmean(np.abs(np.array(it_train)),axis=0)\n",
      "C:\\Users\\yteh\\AppData\\Local\\Temp\\ipykernel_162248\\2889983251.py:74: RuntimeWarning: Mean of empty slice\n",
      "  ave_times = np.nanmean(np.abs(np.array(it_times)),axis=0)\n",
      "C:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\lib\\nanfunctions.py:1670: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  var = nanvar(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "ft = 'tdar'\n",
    "iter = 6\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "plot_mod = ['lda','alda','cnn','acnn03','avcnn','crcnn']\n",
    "plot_mod = ['lda','avcnn']\n",
    "plot_mod = ['acnn05','avcnn']\n",
    "for plot_i in range(1):\n",
    "    for sub in range(4,5):    \n",
    "        it_acc = []\n",
    "        it_recal = []\n",
    "        it_fail = []\n",
    "        it_val = []\n",
    "        it_prev = []\n",
    "        it_train = []\n",
    "        it_times = []\n",
    "        it_replaced = []\n",
    "        for it in range(1):#iter):\n",
    "        \n",
    "            # load or initialize cnn weights\n",
    "            if plot_i == 1:\n",
    "                with open('0323 full run pre and post/' + subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            else:\n",
    "                with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "                    all_acc, all_recal, all_val, all_prev, all_train, all_times, all_dof, _, mod_tot, c_weights, v_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "            \n",
    "            lda_ind = mod_tot.index('alda') + 1\n",
    "            all_acc[all_acc==0] = np.nan\n",
    "            all_val[all_val==0] = np.nan\n",
    "            all_prev[all_prev==0] = np.nan\n",
    "            all_train[all_train==0] = np.nan\n",
    "            all_times[all_times==0] = np.nan\n",
    "\n",
    "            it_acc.append(all_acc)\n",
    "            it_recal.append(np.sum(all_recal==1,axis=0))\n",
    "            it_fail.append(np.sum(all_recal==-1,axis=0))\n",
    "            it_replaced.append(np.sum(all_recal==-2,axis=0))\n",
    "            it_val.append(all_val)\n",
    "            it_prev.append(all_prev)\n",
    "            it_train.append(all_train)\n",
    "            it_times.append(all_times)\n",
    "\n",
    "            it_acc[it][:,:lda_ind] = it_acc[0][:,:lda_ind]\n",
    "            it_recal[it][:lda_ind] = it_recal[0][:lda_ind]\n",
    "            it_fail[it][:lda_ind] = it_fail[0][:lda_ind]\n",
    "            it_replaced[it][:lda_ind] = it_replaced[0][:lda_ind]\n",
    "            it_val[it][:,:lda_ind] = it_val[0][:,:lda_ind]\n",
    "            it_prev[it][:,:lda_ind] = it_prev[0][:,:lda_ind]\n",
    "            it_train[it][:,:lda_ind] = it_train[0][:,:lda_ind]\n",
    "            it_times[it][:,:lda_ind] = it_times[0][:,:lda_ind]\n",
    "\n",
    "\n",
    "        it_acc2 = cp.deepcopy(it_acc)\n",
    "        for i in range(len(it_acc2)):\n",
    "            x = it_val[i] < 0\n",
    "            # print(x.type)\n",
    "            # print(it_acc2[i].shape)\n",
    "            # print(ave_val.shape)\n",
    "            it_acc2[i][(it_acc[i]< 0) & (it_val[i] > 0)]= it_val[i][(it_acc[i]< 0)& (it_val[i] > 0)]\n",
    "            # it_acc2[i][(it_acc[i]< 0)]= it_val[i][(it_acc[i]< 0)]\n",
    "\n",
    "\n",
    "        ave_acc2 = np.nanmean(np.abs(np.array(it_acc2)),axis=0)\n",
    "        ave_acc = np.nanmean(np.abs(np.array(it_acc)),axis=0)\n",
    "        ave_val = np.nanmean(np.abs(np.array(it_val)),axis=0)\n",
    "        ave_prev = np.nanmean(np.abs(np.array(it_prev)),axis=0)\n",
    "        ave_train = np.nanmean(np.abs(np.array(it_train)),axis=0)\n",
    "        ave_times = np.nanmean(np.abs(np.array(it_times)),axis=0)\n",
    "        ave_recal = np.nanmean(np.array(it_recal),axis=0)\n",
    "        ave_fail = np.nanmean(np.array(it_fail),axis=0)\n",
    "        ave_replaced = np.nanmean(np.array(it_replaced),axis=0)\n",
    "\n",
    "        std_acc2 = np.nanstd(np.abs(np.array(it_acc2)),axis=0)/np.sum(~np.isnan(np.array(it_acc2)),axis=0)\n",
    "        std_acc = np.nanstd(np.abs(np.array(it_acc)),axis=0)/np.sum(~np.isnan(np.array(it_acc)),axis=0)\n",
    "        std_val = np.nanstd(np.abs(np.array(it_val)),axis=0)/np.sum(~np.isnan(np.array(it_val)),axis=0)\n",
    "        std_prev = np.nanstd(np.abs(np.array(it_prev)),axis=0)/np.sum(~np.isnan(np.array(it_prev)),axis=0)\n",
    "        std_train = np.nanstd(np.abs(np.array(it_train)),axis=0)/np.sum(~np.isnan(np.array(it_train)),axis=0)\n",
    "        std_times = np.nanstd(np.abs(np.array(it_times)),axis=0)/np.sum(~np.isnan(np.array(it_times)),axis=0)\n",
    "        std_recal = np.nanstd(np.array(it_recal),axis=0)/np.sum(~np.isnan(np.array(it_recal)),axis=0)\n",
    "        std_fail = np.nanstd(np.array(it_fail),axis=0)/np.sum(~np.isnan(np.array(it_fail)),axis=0)\n",
    "        std_replaced = np.nanstd(np.array(it_replaced),axis=0)/np.sum(~np.isnan(np.array(it_replaced)),axis=0)\n",
    "\n",
    "        fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "        for mod in plot_mod:\n",
    "            plot_ind = mod_tot.index(mod)\n",
    "            # ax.plot(ave_acc[2:,plot_ind],'.-',label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            x = ~np.isnan(ave_val[:,plot_ind]) \n",
    "            # ave_acc2 = cp.deepcopy(ave_acc)\n",
    "            # ave_acc[x,plot_ind] = ave_val[x,plot_ind]\n",
    "\n",
    "            ax[0].plot(ave_acc[:,plot_ind],'.-',ms=8,label= mod + ' = '+ \"{:.2f}\".format(ave_recal[plot_ind]) + ' +/- ' + \"{:.2f}\".format(std_recal[plot_ind]) + ', ' + \"{:.2f}\".format(ave_fail[plot_ind]) +' failed'+ ', ' + \"{:.2f}\".format(ave_replaced[plot_ind]) +' replaced')#str(std_recal[plot_ind,0]))\n",
    "            # ax[0].plot(np.squeeze(np.where(x)),ave_acc[x,plot_ind],'kx',ms=12)\n",
    "            ax[1].plot(ave_acc2[:,plot_ind],'.-',ms=8,label= mod + ' = '+ \"{:.2f}\".format(ave_recal[plot_ind]) + ' +/- ' + \"{:.2f}\".format(std_recal[plot_ind])+ ', ' + \"{:.2f}\".format(ave_fail[plot_ind]) + ' failed'+ ', ' + \"{:.2f}\".format(ave_replaced[plot_ind]) +' replaced')#+ str(std_recal[plot_ind,0]))\n",
    "            # ax.plot(np.squeeze(np.where(x)), ave_val[~np.isnan(ave_val[:,plot_ind]),plot_ind],'.-',ms=8,label= mod + ' = '+ str(ave_recal[plot_ind,0]) + ' +/- ' + str(std_recal[plot_ind,0]))\n",
    "            # plt.fill_between(np.arange(ave_acc[2:,plot_ind].shape[0]),ave_acc[2:,plot_ind]-std_acc[2:,plot_ind],ave_acc[2:,plot_ind]+std_acc[2:,plot_ind],alpha=.3)\n",
    "        ax[1].legend()\n",
    "        for i in range(2):\n",
    "            ax[i].axhline(75, ls='--', color='grey')\n",
    "            ax[i].set_ylim([0,100])\n",
    "        # ax[1].axhline(75, ls='--', color='grey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TR58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\utils\\data_utils.py:638: RuntimeWarning: invalid value encountered in sqrt\n",
      "  m = np.sqrt((m1-m2).T*(cov_inv)*(m1-m2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init test dof: [ 1  6 16 17 19 48 53]\n",
      "test_dof: [ 1  6 16 17 19 48 53], key: [0. 1. 2. 3. 4. 5. 6.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\main.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=91'>92</a>\u001b[0m test_data, test_params \u001b[39m=\u001b[39m lp\u001b[39m.\u001b[39mcheck_labels(test_data,test_params,train_dof,key,\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=93'>94</a>\u001b[0m \u001b[39m# test \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=94'>95</a>\u001b[0m y_test, _, x_test_cnn, x_test_lda, y_test_lda \u001b[39m=\u001b[39m prd\u001b[39m.\u001b[39;49mprep_test_caps(test_data, test_params, scaler, emg_scale, num_classes\u001b[39m=\u001b[39;49mn_dof, ft\u001b[39m=\u001b[39;49mft, split\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=96'>97</a>\u001b[0m \u001b[39m# test \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/main.ipynb#ch0000002?line=97'>98</a>\u001b[0m acc[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m,:] \u001b[39m=\u001b[39m lp\u001b[39m.\u001b[39mtest_models(\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m,  x_test_lda, y_test_lda, lda\u001b[39m=\u001b[39m[w,c])\n",
      "File \u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\utils\\data_utils.py:157\u001b[0m, in \u001b[0;36mprep_test_caps\u001b[1;34m(x, params, scaler, emg_scale, num_classes, ft, split)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=154'>155</a>\u001b[0m \u001b[39m# LDA data\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=155'>156</a>\u001b[0m y_lda \u001b[39m=\u001b[39m params[:,[\u001b[39m0\u001b[39m]]\n\u001b[1;32m--> <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=156'>157</a>\u001b[0m x_lda \u001b[39m=\u001b[39m extract_feats_caps(x_orig,ft\u001b[39m=\u001b[39;49mft)\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=157'>158</a>\u001b[0m \u001b[39m# y_lda = np.argmax(y_train_noise,axis=1)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=158'>159</a>\u001b[0m \u001b[39m# x_lda = extract_feats_caps(x_train_noise,ft=ft)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=160'>161</a>\u001b[0m \u001b[39mreturn\u001b[39;00m y_test, x_test_mlp, x_test_cnn, x_lda, y_lda\n",
      "File \u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\adaptive\\python\\adapt\\utils\\data_utils.py:280\u001b[0m, in \u001b[0;36mextract_feats_caps\u001b[1;34m(raw, ft, uint, order)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=276'>277</a>\u001b[0m     z_th \u001b[39m=\u001b[39m \u001b[39m0.025\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=277'>278</a>\u001b[0m     s_th \u001b[39m=\u001b[39m \u001b[39m0.015\u001b[39m\n\u001b[1;32m--> <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=279'>280</a>\u001b[0m mean_mav \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile(np\u001b[39m.\u001b[39;49mmean(raw,axis\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m,np\u001b[39m.\u001b[39mnewaxis],(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,N))\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=280'>281</a>\u001b[0m raw_demean \u001b[39m=\u001b[39m raw\u001b[39m-\u001b[39mmean_mav\n\u001b[0;32m    <a href='file:///c%3A/Users/yteh/Documents/work/git/projects/adaptive/python/adapt/utils/data_utils.py?line=282'>283</a>\u001b[0m mav\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mabs(raw_demean),axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3440\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3436'>3437</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3437'>3438</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m mean(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3439'>3440</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39m_mean(a, axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype,\n\u001b[0;32m   <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/fromnumeric.py?line=3440'>3441</a>\u001b[0m                       out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\adapt_env_2\\lib\\site-packages\\numpy\\core\\_methods.py:179\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=175'>176</a>\u001b[0m         dtype \u001b[39m=\u001b[39m mu\u001b[39m.\u001b[39mdtype(\u001b[39m'\u001b[39m\u001b[39mf4\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=176'>177</a>\u001b[0m         is_float16_result \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=178'>179</a>\u001b[0m ret \u001b[39m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=179'>180</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, mu\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=180'>181</a>\u001b[0m     ret \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mtrue_divide(\n\u001b[0;32m    <a href='file:///c%3A/ProgramData/Anaconda3/envs/adapt_env_2/lib/site-packages/numpy/core/_methods.py?line=181'>182</a>\u001b[0m             ret, rcount, out\u001b[39m=\u001b[39mret, casting\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m'\u001b[39m, subok\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 32\n",
    "load_mod = False\n",
    "ft = 'tdar'\n",
    "iter = 10\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "test_mod = 0\n",
    "\n",
    "sub_b = []\n",
    "sub_s = []\n",
    "sub_sep=[]\n",
    "for sub in range(2,7):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    # load or initialize cnn weights\n",
    "    acc = np.zeros((len(all_files),2))\n",
    "    acc_val = np.zeros((len(all_files),2))\n",
    "    acc_prev = np.zeros((len(all_files),2))\n",
    "    acc_train = np.zeros((len(all_files),2))\n",
    "    \n",
    "\n",
    "    acc_i = 0\n",
    "\n",
    "    # Loop through files\n",
    "    for i in range(len(all_files)-1):              \n",
    "        # load training file\n",
    "        train_file = all_files[i]\n",
    "        train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "        # load training file\n",
    "        train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "        val_data, val_params = train_data, train_params\n",
    "            \n",
    "        # get current dofs and create key\n",
    "        if i == 0:\n",
    "            train_dof = np.unique(train_params[:,-1])\n",
    "            key = np.arange(len(train_dof))\n",
    "\n",
    "            n_dof = len(train_dof)\n",
    "\n",
    "            h = np.ones((len(all_files),n_dof))\n",
    "            b = np.ones((len(all_files),n_dof))\n",
    "            sep = np.zeros((len(all_files),n_dof))\n",
    "            tot_b = np.ones((len(all_files,)))\n",
    "            h[:] = np.nan\n",
    "            b[:] = np.nan\n",
    "\n",
    "            train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key,False)\n",
    "            val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key,False)\n",
    "\n",
    "            _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=False, split=False,num_classes=n_dof)\n",
    "\n",
    "            w, c, mu_class, _, _, N, cov_class = dlda.train_lda(x_train_lda, y_train_lda, key)\n",
    "            \n",
    "            cl_s = np.zeros((n_dof,))\n",
    "            # cl_s[:] = np.nan\n",
    "            m_cl = np.zeros((n_dof,x_train_lda.shape[1]))\n",
    "            s_cl = np.zeros((n_dof,x_train_lda.shape[1],x_train_lda.shape[1]))\n",
    "            for cl in train_dof:\n",
    "                train_ind = np.squeeze(y_train_lda == key[train_dof==cl])\n",
    "                temp_x = x_train_lda[train_ind,...]\n",
    "                m1 = np.nanmean(temp_x,axis=0)\n",
    "                s1 = np.cov(temp_x.T)\n",
    "                m_cl[key[train_dof==cl],...] = np.nanmean(temp_x,axis=0)\n",
    "                s_cl[key[train_dof==cl],...] = np.cov(temp_x.T)\n",
    "                ct = 0\n",
    "                for cl_i in train_dof:\n",
    "                    temp_ind = np.squeeze(y_train_lda == key[train_dof==cl_i])\n",
    "                    temp_x2 = x_train_lda[temp_ind,...]\n",
    "                    m2 = np.nanmean(temp_x2,axis=0)\n",
    "                    s2 = np.cov(temp_x2.T)\n",
    "                    cl_s[key[train_dof==cl]] += np.nanmean(prd.mahal(m1,s1,m2,s2))\n",
    "                    ct+=1\n",
    "                cl_s[key[train_dof==cl]] /= ct\n",
    "\n",
    "        #del x_train_lda, y_train_lda, x_train_cnn, y_train, x_clean_cnn, y_clean\n",
    "        \n",
    "        # load data\n",
    "        test_file = all_files[i+1]\n",
    "        test_data, test_params = prd.load_caps_train(sub_path + test_file + '/traindata.mat')\n",
    "        \n",
    "        # check class labels\n",
    "        test_data, test_params, _ = prd.threshold(test_data, test_params, th)\n",
    "        test_data, test_params = lp.check_labels(test_data,test_params,train_dof,key,True)\n",
    "\n",
    "        # test \n",
    "        y_test, _, x_test_cnn, x_test_lda, y_test_lda = prd.prep_test_caps(test_data, test_params, scaler, emg_scale, num_classes=n_dof, ft=ft, split=False)\n",
    "        \n",
    "        # test \n",
    "        acc[i+1,:] = lp.test_models(None, None,  x_test_lda, y_test_lda, lda=[w,c])\n",
    "\n",
    "        for cl in train_dof:\n",
    "            test_ind = np.squeeze(y_test_lda == key[train_dof==cl])\n",
    "            if i == 0:\n",
    "                train_ind = np.squeeze(y_train_lda == key[train_dof==cl])\n",
    "                m1 = np.nanmean(x_train_lda[train_ind,:],axis=0)\n",
    "                s1 = np.cov(x_train_lda[train_ind,:].T)\n",
    "            if np.sum(test_ind) > 50:\n",
    "                # print(np.sum(test_ind))\n",
    "                m2 = np.nanmean(x_test_lda[test_ind,:],axis=0)\n",
    "                s2 = np.cov(x_test_lda[test_ind,:].T)\n",
    "                h[i+1,key[train_dof==cl]] = prd.hellinger(m1,s1,m2,s2)\n",
    "                b[i+1,key[train_dof==cl]] = np.nanmean(prd.mahal(m1,s1,m2,s2))\n",
    "                ct = 0\n",
    "                for cl_i in train_dof:\n",
    "                    cl_sep = np.nanmean(prd.mahal(np.squeeze(m_cl[key[train_dof==cl_i],...]),np.squeeze(s_cl[key[train_dof==cl_i],...]),m2,s2))\n",
    "                    if np.isnan(cl_sep):\n",
    "                        print('oops')\n",
    "                    else:\n",
    "                        ct += 1\n",
    "                        sep[i+1,key[train_dof==cl]] += cl_sep\n",
    "                sep[i+1,key[train_dof==cl]] /= ct\n",
    "                # b[i+1,key[train_dof==cl]] = prd.bhatta(m1,s1,m2,s2)\n",
    "        # print(h[i+1,:])\n",
    "        # m1 = np.nanmean(x_train_lda,axis=0)\n",
    "        # s1 = np.cov(x_train_lda.T)\n",
    "        # m2 = np.nanmean(x_test_lda,axis=0)\n",
    "        # s2 = np.cov(x_test_lda.T)\n",
    "        # b[i+1,0] = np.min(h[i+1,:])\n",
    "\n",
    "        print(\"{:.2f}\".format(acc[i+1,0]))\n",
    "        del y_test, x_test_cnn#, x_test_lda, y_test_lda, test_data, test_params\n",
    "\n",
    "    print(np.median(acc,axis=0))\n",
    "    with open(subs[sub] + '_hell.p','wb') as f:\n",
    "        pickle.dump([acc,h,b,cl_s,sep],f)\n",
    "    sub_b.append(b)\n",
    "    sub_s.append(cl_s)\n",
    "    sub_sep.append(sep)\n",
    "    \n",
    "    gc.collect(generation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 4\n",
    "h[h==0] = np.nan\n",
    "\n",
    "\n",
    "\n",
    "for sub in range(7):\n",
    "    with open(subs[sub] + '_hell.p','rb') as f:\n",
    "        acc,h,b,cl_s,sep = pickle.load(f)\n",
    "    min_h = np.nanmax(h[1:,:],axis=1)\n",
    "    norm_m = b/np.nanmean(cl_s)\n",
    "    acc_p = acc[1:,0]\n",
    "    fig,ax = plt.subplots()\n",
    "    fig1,ax1 = plt.subplots(1,2,figsize=(15,3))\n",
    "    # ax.plot(-np.nanmean(b[1:,:],axis=1),acc_p, 'x')\n",
    "    ax.plot(np.nanmax(b[1:,:],axis=1),acc_p, 'x')\n",
    "    ax1[0].plot(np.nanmax(b[1:,:],axis=1), 'x')\n",
    "    ax1[1].plot(acc_p, 'x')\n",
    "    ax1[1].set_ylim([0,100])\n",
    "    # ax1[0].set_ylim([0,20])\n",
    "    # ax.set_xlim([0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for sub in range(7):\n",
    "    with open(subs[sub] + '_lda_accs.p','rb') as f:\n",
    "        acc, _ = pickle.load(f)\n",
    "    temp.append(acc[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count recalibrations\n",
    "path = 'C:/Users/yteh/Documents/work/necal/home data/'\n",
    "subs = os.listdir(path)\n",
    "if 'skip' in subs:\n",
    "    subs = np.delete(subs,subs.index('skip'))\n",
    "bat = 128\n",
    "load_mod = False\n",
    "mod_tot = ['blda','lda','crlda','alda','bcnn','cnn', 'crcnn','acnn03','acnn30','acewc00','acewc30', 'adcnn30', 'vcnn', 'bvcnn', 'avcnn03', 'avcnn15', 'acnnl03','crvcnn','acewclm','xtra','xtra1','xtra2']\n",
    "ft = 'feat'\n",
    "iter = 1\n",
    "\n",
    "for sub in range(4,5):\n",
    "    print(subs[sub])\n",
    "    sub_path = path + subs[sub] + '/DATA/MAT/'\n",
    "    all_files = os.listdir(sub_path)\n",
    "    if 'skip' in all_files:\n",
    "        all_files = np.delete(all_files,all_files.index('skip'))\n",
    "\n",
    "    # first iteration, includes LDA; others exclude LDA\n",
    "    mod_all = ['vcnn']\n",
    "\n",
    "    # load or initialize cnn weights\n",
    "    if load_mod:\n",
    "        with open(subs[sub] + '_' + str(0) + '_r_accs.p','rb') as f:\n",
    "            all_acc, all_recal, all_val, all_prev, all_train, all_times, _, _, c_weights, cl_wc, scaler_0, emg_scale = pickle.load(f)\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "    else:\n",
    "        c_weights = None\n",
    "        v_weights = None\n",
    "        v_wc = None\n",
    "        cl_wc = None\n",
    "        all_recal = np.empty((len(mod_tot),1))\n",
    "        all_recal[:] = np.nan\n",
    "        all_acc = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_val = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_prev = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_train = np.zeros((len(all_files),len(mod_tot)))\n",
    "        all_times = np.zeros((len(all_files),len(mod_tot)))\n",
    "\n",
    "    mod_i = 0\n",
    "    for mod in mod_all:\n",
    "        acc = np.zeros((len(all_files),5))\n",
    "        acc_val = np.zeros((len(all_files),5))\n",
    "        acc_prev = np.zeros((len(all_files),5))\n",
    "        acc_train = np.zeros((len(all_files),5))\n",
    "\n",
    "        if 'cnn' in mod:\n",
    "            acc_i = 2\n",
    "        elif 'cewc' in mod:\n",
    "            acc_i = 4\n",
    "        elif 'lda' in mod:\n",
    "            acc_i = 0\n",
    "\n",
    "        cnn = None\n",
    "        ewc = None\n",
    "\n",
    "        ep = 50\n",
    "        recal = 0\n",
    "        skip = False\n",
    "\n",
    "        # Loop through files\n",
    "        for i in range(1,2):#len(all_files)-1):\n",
    "            # load training file\n",
    "            train_file = all_files[i]\n",
    "            train_data, train_params = prd.load_caps_train(sub_path + train_file + '/traindata.mat')\n",
    "\n",
    "            train_data, train_params, th = prd.threshold(train_data, train_params)\n",
    "            val_data = train_data\n",
    "            val_params = train_params\n",
    "\n",
    "            train_dof = np.unique(train_params[:,-1])\n",
    "            key = np.empty(train_dof.shape)\n",
    "            for key_i in range(len(train_dof)):\n",
    "                key[key_i] = cp.deepcopy(train_params[np.argmax(train_params[:,2] == train_dof[key_i]),0])\n",
    "            n_dof = int(np.max(key))\n",
    "            \n",
    "            train_data, train_params = lp.check_labels(train_data,train_params,train_dof,key)\n",
    "            val_data, val_params = lp.check_labels(val_data,val_params,train_dof,key)\n",
    "\n",
    "            _, x_clean_cnn, y_clean, _, x_train_cnn, y_train, x_train_lda, y_train_lda, emg_scale, scaler, _, _, _ = prd.prep_train_caps(train_data, train_params, prop_b=False, batch_size=bat, ft=ft, noise=True, split=False,num_classes=n_dof)\n",
    "\n",
    "            _, _, _, _, x_val_cnn, y_val, x_val_lda, y_val_lda, _, _, _, _, _ = prd.prep_train_caps(val_data, val_params, emg_scale=emg_scale,scaler=scaler, prop_b=False, batch_size=bat, ft=ft, num_classes=n_dof, noise=False, split=False)\n",
    "\n",
    "            del train_data, train_params, val_data, val_params\n",
    "\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=30, dec=True, print_b=True)\n",
    "            cnn, all_times[i,mod_tot.index(mod)] = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=[cnn], n_dof=n_dof, ep=30, dec=False,print_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_clean_cnn)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    x_out[y_clean[:,cl]==1,...] = np.random.normal(np.mean(x_clean_cnn[y_clean[:,cl]==1,...],axis=0), np.std(x_clean_cnn[y_clean[:,cl]==1,...],axis=0),x_clean_cnn[y_clean[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = cp.deepcopy(x_train_cnn)\n",
    "for cl in range(y_train.shape[1]):\n",
    "    x_out[y_train[:,cl]==1,...] = np.random.normal(np.mean(x_train_cnn[y_train[:,cl]==1,...],axis=0), np.std(x_train_cnn[y_train[:,cl]==1,...],axis=0),x_train_cnn[y_train[:,cl]==1,...].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp1 = np.ones((1*y_clean.shape[0],8))\n",
    "x_out,_,_ = cnn.dec(samp1,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),samp=True)\n",
    "# _,x_out,_,_ = cnn(x_clean_cnn,np.tile(np.argmax(y_clean,axis=1),[1]).astype('float32'),dec=True)\n",
    "x_out = x_out.numpy()\n",
    "# for i in range(y_clean.shape[1]):\n",
    "#     adjust1 = np.std(x_clean_cnn[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     rescale = np.mean(adjust1)/np.mean(np.std(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0))\n",
    "#     gmean = np.mean(x_out[np.argmax(y_clean,axis=1)==i,...],axis=0)\n",
    "#     x_out[np.argmax(y_clean,axis=1)==i,...] = (x_out[np.argmax(y_clean,axis=1)==i,...] - gmean)*rescale + gmean\n",
    "# x_out = np.maximum(np.minimum(x_out,1),0)\n",
    "for cl in range(y_clean.shape[1]):\n",
    "    ind = np.tile(np.argmax(y_clean,axis=1)==cl,[1])\n",
    "    ind2 = np.argmax(y_clean,axis=1)==cl\n",
    "    x_temp = x_out[ind,...].reshape((np.sum(ind),-1))\n",
    "    x_true = x_clean_cnn[ind2,...].reshape((np.sum(ind2),-1))\n",
    "    # for i in range()\n",
    "    plt.figure()\n",
    "    # for i in range(x_true.shape[0]):\n",
    "    #     plt.plot(x_true[i,...],'k-')\n",
    "        \n",
    "    for i in range(x_temp.shape[0]):\n",
    "        plt.plot(x_temp[i,...],'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lda = x_clean_cnn.reshape(x_clean_cnn.shape[0],-1)\n",
    "y_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "y_train_lda = np.argmax(y_clean,axis=1)[...,np.newaxis]\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_lda,y_lda)\n",
    "y_out = dlda.predict(x_lda, w, c)\n",
    "print(dlda.eval_lda(w, c, x_lda, y_lda))\n",
    "x_out_lda = x_out.reshape(x_out.shape[0],-1)\n",
    "print(dlda.eval_lda(w,c, x_out_lda,np.tile(y_train_lda,[1,1])))\n",
    "w,c, _, _, _, _, _ = dlda.train_lda(x_out_lda,np.tile(y_train_lda,[1,1]))\n",
    "print(dlda.eval_lda(w, c, x_out_lda,y_train_lda))\n",
    "print(dlda.eval_lda(w, c, x_lda, np.argmax(y_clean,axis=1)[...,np.newaxis]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn1, _ = lp.train_models(traincnn=x_train_cnn,y_train=y_train, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn1,test_accuracy)\n",
    "print(lp.test_models(x_out, y_clean, None, None, cnn=cnn1, test_mod=test_mod, test_accuracy=test_accuracy))\n",
    "\n",
    "cnn2, _ = lp.train_models(traincnn=x_out,y_train=y_clean, mod=['vcnn'], n_dof=n_dof, ep=15, dec=False,print_b=True)\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name=\"test_accuracy\")\n",
    "test_mod = dl.get_test(cnn2,test_accuracy)\n",
    "print(lp.test_models(x_clean_cnn, y_clean, None, None, cnn=cnn2, test_mod=test_mod, test_accuracy=test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "fig,ax = plt.subplots(1,5,figsize=(30,4))\n",
    "for sub in range(2,3):#,5):\n",
    "    with open(subs[sub] + '_0_r_accs.p','rb') as f:\n",
    "        acc_all, recal_all, cur_all, prev_all, val_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "    # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "    colors =  cm.get_cmap('tab20c')\n",
    "    c = np.empty((20,4))\n",
    "    for i in range(20):\n",
    "        c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "    nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "    nn_c[0,-1] = 1\n",
    "    all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "    pt_m = ['ko-','o-','o-','o-','s','s','s','s','D']\n",
    "    nn_c = np.vstack((np.array([0,0,0,1]), c[0,:],c[1,:],c[2,:],c[3,:],c[4,:],c[5,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "    # nn_c[0,-1] = 1\n",
    "\n",
    "    labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "    labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "    # labels = mod_tot\n",
    "\n",
    "    ax_ind = sub\n",
    "    it = 0\n",
    "    for v in [1,2]: \n",
    "        i = mod_tot.index(mod_all[v])\n",
    "        acc_temp = acc_all[1:-1,i]\n",
    "        if not np.isnan(acc_temp).all():\n",
    "            x = np.arange(len(acc_temp))\n",
    "            recal_i = (acc_temp < 0)\n",
    "            ax[ax_ind].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v],color=nn_c[it,:])\n",
    "            ax[ax_ind].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "            it+=1\n",
    "\n",
    "    for i in range(5):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        \n",
    "        ax[i].set_ylim([0,100])\n",
    "        ax[i].set_title('TR' + str(i+1))\n",
    "    ax[0].legend()\n",
    "    ax[2].set_xlabel('Calibration Set')\n",
    "    ax[0].set_ylabel('Accuracy (%)')\n",
    "    plt.rc('font', size=15) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_iter = 1\n",
    "for sub in range(2,3):#,5):\n",
    "    fig,ax = plt.subplots(1,4,figsize=(20,4))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, val_all,mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        pt_m = ['ko','*','*','o','s','s','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [1,0,0,1,2,2,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[it]+': ' + str(int(recal_all[i,0])),color=nn_c[it,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[it,:])\n",
    "                it+=1\n",
    "\n",
    "    for i in range(4):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "cv_iter = 1\n",
    "for sub in range(0,5):\n",
    "    fig,ax = plt.subplots(1,2,figsize=(15,5))\n",
    "    for it in range(0,cv_iter):\n",
    "        with open(subs[sub] + '_' + str(it) + '_r_accs.p','rb') as f:\n",
    "            # acc_all, recal_all = pickle.load(f)\n",
    "            acc_all, recal_all, cur_all, prev_all, mod_all, mod_tot, c_weights, cl_wc, scaler_0, emg_scale= pickle.load(f)\n",
    "\n",
    "        # mod_all = ['ld','bld','bcnnl','cnnl','acnnl','acnnl3','acnnl30','acewcl']\n",
    "        colors =  cm.get_cmap('tab20c')\n",
    "        c = np.empty((20,4))\n",
    "        for i in range(20):\n",
    "            c[i,:] = colors(i*1/20)\n",
    "\n",
    "\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[9,:],c[1,:],c[10,:],c[2,:]))\n",
    "        nn_c = np.vstack((np.zeros((1,4)),c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:],c[8,:],c[0,:]))\n",
    "        nn_c[0,-1] = 1\n",
    "        all_m = ['ko-','o-','o-','s-','s-','v-','v-']\n",
    "        pt_m = ['ko','o','*','o','s','D','s','s','D']\n",
    "        nn_c = np.vstack((np.array([0,0,0,1]),np.array([0,0,0,1]), c[0,:],c[1,:],c[4,:],c[5,:],c[6,:],c[8,:],c[6,:],c[0,:],c[8,:],c[0,:]))\n",
    "\n",
    "        labels = ['lda','mlp','cnn','a-mlp','a-cnn','ewc-mlp','ewc-cnn','c-mlp','c-cnn','c-ld']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','a-cnn-5','a-cnn-3','a-cnn-30','ewc-cnn']\n",
    "        labels = ['r-lda','lda','cnn','r-cnn','f-cnn-5','f-cnn-3','f-cnn-30','ewc-cnn']\n",
    "        # labels = mod_tot\n",
    "\n",
    "        ax_ind = [0,0,1,1,1,1,2,2,2,2,2,2,3,3,3,3]\n",
    "        it = 0\n",
    "        for v in [0, 3, 5, 4, 6, 7]: #range(len(mod_all)):\n",
    "            i = mod_tot.index(mod_all[v])\n",
    "            acc_temp = acc_all[1:-1,i]\n",
    "            if not np.isnan(acc_temp).all():\n",
    "                x = np.arange(len(acc_temp))\n",
    "                recal_i = (acc_temp < 0)\n",
    "                ax[ax_ind[it]].plot(np.abs(acc_temp),'-',color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[~recal_i],np.abs(acc_temp[~recal_i]),pt_m[it],label=labels[v]+': ' + str(int(recal_all[i,0])),color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].plot(x[recal_i],np.abs(acc_temp[recal_i]),'x',ms=10,color=nn_c[v,:])\n",
    "                ax[ax_ind[it]].xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                it+=1\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i].axhline(70, ls='--', color='grey')\n",
    "        ax[i].legend()\n",
    "        ax[i].set_ylim([0,100])\n",
    "        \n",
    "\n",
    "    ax[0].set_ylabel('Accuracy (%)')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96e4d54467b05e62951c9fd7929782b99429e3b62c1a3b146d4f3dbf79f907e6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('adapt_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
